\documentclass[12pt,notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel} 

\usepackage{fullpage}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[left=15mm, right=15mm, top=20mm, bottom=20mm]{geometry}

\usepackage{hyperref}

\usepackage{graphicx}

\theoremstyle{plain}
\newtheorem{thm}{Теорема}[section]
\newtheorem{lemma}[thm]{Лемма}
\newtheorem{corr}[thm]{Следствие}
\theoremstyle{definition}
\newtheorem{exc}[thm]{Упражнение}
\newtheorem{exm}[thm]{Пример}
\newtheorem{rem}[thm]{Замечание}
\theoremstyle{plain}

%\renewcommand{\thesection}{}
\renewcommand{\thethm}{\arabic{section}.\arabic{thm}}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\sbs}{\subseteq}
\renewcommand{\setminus}{\smallsetminus}
\newcommand{\void}{\varnothing}
\newcommand{\mP}{\mathcal{P}}
\renewcommand{\phi}{\varphi}
\newcommand{\eps}{\varepsilon}
\newcommand{\dom}{\mathop{\mathrm{dom}}}
\newcommand{\rng}{\mathop{\mathrm{rng}}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\pto}{\stackrel{p}{\to}}
\newcommand{\rst}{\mathop{\upharpoonright}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\0}{\mathbf{0}}

\newcommand{\dvd}{\mathop{\mid}}
\newcommand{\ndvd}{\mathop{\nmid}}
\newcommand{\pth}[1]{\overset{\raisebox{0.25em}{$ #1 $}}{\relbar\hspace{-.8mm}\relbar}}


\newcommand{\ply}{\Longrightarrow}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\sz}{\mathrm size}
\DeclareMathOperator{\dpt}{\matrhm depth}
\DeclareMathOperator{\mj}{\matrhm maj}

\newcommand{\mF}{\mathcal{F}}
\renewcommand{\P}{\mathsf{P}}

% For square bracket in cases:
\makeatletter
\newenvironment{sqcases}{%
	\matrix@check\sqcases\env@sqcases
}{%
	\endarray\right.%
}
\def\env@sqcases{%
	\let\@ifnextchar\new@ifnextchar
	\left\lbrack
	\def\arraystretch{1.2}%
	\array{@{}l@{\quad}l@{}}%
}
\makeatother

% for methodical comments
\newcommand{\mcomm}[1]{}
% Евгений Владимирович сказал закомментировать это
%\medskip\noindent\begin{tabular}{| l}
	%\parbox{0.99\textwidth}{{\small
			%#1 }}\end{tabular}
%\smallskip}




\title{Элементы дискретной математики}
\author{Е. В. Дашков}



\begin{document}
\maketitle

\renewcommand\contentsname{Содержание}
\tableofcontents
\newpage

\section{Истинность, ложность и высказывания}\label{sect:1}

\mcomm{}

Первая наша цель – выделить наиболее важные логические конструкции, которые можно найти в естественном языке при рассмотрении как повседневных явлений, так и математических. Читатель хорошо знаком с тем фактом, что основу нашей речи составляют \emph{высказывания} (также известные как \emph{повествовательные предложения}). Они и вправду повествуют о чем-то или утверждают что-то, в пику вопросительным предложениям, задающим вопрос, или повелительным, выражающим повеление или просьбу. Например,
\begin{quote}
	Роза упала на лапу Азора
\end{quote}
это пример высказывания. Другим примером будет
\begin{quote}
	Сегодня четверг и $2 + 3$ равно $5$.
\end{quote}

Упрощая, некоторые высказывания \emph{истинны} и остальные \emph{ложны}. Вы можете превратить любое высказывание в \emph{общий вопрос} (\emph{Упала ли роза на лапу Азора?}) – такой, что ответ на него определит высказывание как истинное (\emph{Да, упала}) или ложное (\emph{Нет, не упала}). Разница между истиной и ложью легко уловима интуитивно, хотя с трудом поддается формальному определению.
\begin{exm}
	Истинно, что \emph{$2 + 3$ equals $5$}, постольку, поскольку арифметика и наше понимание числа корректны. То, \emph{четверг ли сегодня}, зависит от настоящей даты. Науке до сих пор неизвестно, действительно ли \emph{любое совершенное\footnote{Натуральное число называют \emph{совершенным} если оно равняется сумме своих делителей (не включая само число), например, $6 = 1 + 2 + 3$ и $28 = 1 + 2 + 4 + 7 + 14$.} число четно}.
	
	Высказывание \emph{данное высказывание истинно} может быть истинным или ложным без какого-либо противоречия, пусть маловероятно, что существует способ удостовериться в каком-либо из этих двух вариантов. С другой стороны, высказывание \emph{данное высказывание ложно} может быть ни истинным, ни ложным, ведь любая посылка ведет к противоречию: если высказывание истинно, то оно должно быть ложно по своему содержимому, и т.д.
\end{exm}

\mcomm{}

Таким образом, наш предварительный анализ нельзя назвать исчерпывающим. Тем не менее, ни нерешенные математические задачи, ни искусственные самореферентные конструкции не смогут остановить нас от завладения инструментом логики!

Причиной тому послужит то, что мы будем изучать только лишь определенным образом составленные \emph{модели} «настоящих» повседневных высказываний. Мы закрепляем имя «высказывание» для таких моделей и повторяем, что \emph{любое высказывание либо истинно, либо ложно, но никогда не является и тем, и другим}.

\medskip
Некоторые высказывания мы называем \emph{составными}, то есть они составлены из более чем одного простого высказывания. Например, \emph{$2$ равно $3$ или $1$ равно $1$}; \emph{неправда, что $2$ равно $1$}; \emph{кто-то верит, что $2$ равно $3$}.

Рассмотрим конструкции, которые позволяют нам рождать означенные составные высказывания, например: \emph{\dots или \dots}; \emph{\dots, но \dots}; \emph{если \dots, то \dots}; \emph{неправда, что \dots}; \emph{кто-то верит, что \dots} и т.д. Всякую подобную конструкцию мы называем \emph{(логической) связкой}, если истинность высказывания, составленного с помощью нее зависит исключительно от истинности более простых высказываний, что она связывает.
\begin{exm}\label{L1:exm_pi}
	В каком случае верно, что \emph{$A$ или $B$} истинно (где $A$ и $B$ означают произвольные высказывания)? Тогда и только тогда, когда хотя бы одно из высказываний  $A$ и $B$ истинно. Т.о., \emph{\dots или \dots} – это логическая связка. Однако \emph{любой студент знает, что \dots} таковой не является. Действительно, рассмотрим высказывания $0 = 0$ и $\pi < 3.14159265358979323847$. Оба они истинны, хотя \emph{любой студент знает, что $0 = 0$} скорее всего истинно, а \emph{любой студент знает, что $\pi < 3.14159265358979323847$} скорее всего ложно. То есть, истинности $A$ недостаточно для истинности высказывания \emph{любой студент знает, что $A$}.
\end{exm}

Мы называем высказывание \emph{(логически) атомарным}, если оно не может быть разумным образом разделено на более простые. Так, \emph{$2$ равно $3$ или $1$ равно $1$} не атомарно, а $2 = 3$ и \emph{кто-то верит, что $2$ равно $3$} – атомарны.

Теперь мы исправим проблемы, связанные с бытовой интерпретацией логических связок, наиболее часто встречаемых в математике. Отсюда и дальше, произвольные высказывания будут обозначены буквами латинского алфавита: например, \emph{$A$ и $B$}; \emph{если $A$, то $B$}. Мы используем $1$ для обозначения \emph{истины} и $0$ для обозначения \emph{лжи}. Так, мы получаем следующую \emph{таблицу истинности}.
\begin{center}
	\begin{tabular}{| c c | c | c | c | c | c |}
		\hline
		$A$ & $B$ & не $A$  &$A$ и $B$&$A$ или $B$&если $A$ то $B$ &$A$ тогда и только тогда, когда $B$\\
		\hline
		$0$&$0$&$1$&$0$&$0$&$1$&$1$\\
		$0$&$1$&$1$&$0$&$1$&$1$&$0$\\
		$1$&$0$&$0$&$0$&$1$&$0$&$0$\\
		$1$&$1$&$0$&$1$&$1$&$1$&$1$\\
		\hline
	\end{tabular}
\end{center}
\begin{exc}
	Постройте таблицу истинности для составного высказывания \emph{если не ($A$ или $B$), то $C$ и $A$}.
\end{exc}

В естественном языке мы можем найти множество конструкций, которые возможно использовать в качестве логических связок. В таких случаях, как правило, жертвуется часть их изначального смысла. Следующие таблицы представляют три условно равнозначных способа выражения логических связок: в русско-язычной логической литературе, с помощью символов и повседневные аналоги в русском языке.

\begin{center}
	\begin{tabular}{|c | c | c | c |}
		\hline
		{\bf Связка} & \it отрицание  & \it конъюнкция & \it дизъюнкиця \\
		\hline
		\bf В логике & не $A$  &$A$ и $B$&$A$ или $B$\\
		\hline
		\bf Символьно & $\neg A$, $\bar A\phantom{\dfrac{1}{2}}$  &$A \wedge B$, $A \mathop{\&} B$&$A \vee B$\\
		\hline
		\bf В русском языке & неправда, что $A$; &и $A$, и $B$; $A$, но $B$; & либо $A$, либо $B$; $A$ и/или $B$; \\
		&   $A$ неверно &  $A$ несмотря на $B$ & $A$, $B$ или и то, и то\\
		\hline
	\end{tabular}
	
	\vspace{10mm}
	
	\begin{tabular}{|c | c | c |}
		\hline
		\bf Связка & \it импликация & \it эквивалентность \\
		\hline
		\bf В логике & если $A$ то $B$ &$A$ тогда и только тогда, когда $B$\\
		\hline
		\bf Символьно & $A \to B$, $A \Rightarrow B$ &$A \leftrightarrow B$, $A \Leftrightarrow B$\\
		\hline
		\bf В русском языке & $B$ если $A$; $B$ когда $A$; $A$ только когда $B$;   & $A$ равнозначно $B$; \\
		&$A$ только если $B$; из $A$ следует $B$; & если $A$, то $B$, и наоборот;  \\
		& $B$ при условии, что $A$; в случае $A$, $B$;  & $A$ необходимо и достаточно для $B$ \\
		& $A$ достаточно для $B$; $B$ необходимо для $A$ &  \\
		\hline
	\end{tabular}
\end{center}
\mcomm{}
\begin{exm}
	Располагая некоторыми высказываниями $A, B, C,\dots$, мы можем без труда составлять более сложные логические конструкции с помощью логических связок. Например, 
	$$\bigl((A \to \neg(B \to C)) \vee (B \wedge \neg A)\bigr) \to \bigl(((\neg A) \wedge ((\neg C) \to A)) \wedge B \bigr)$$.
	Подобные выражения мы будем иногда называть \emph{формулами}.
	
	Мы также можем опускать некоторые скобки по определенным правилам, чтобы сделать текст более читаемым. Согласно одному из таких правил, конъюнкция и дизъюнкция имеют \emph{меньший приоритет}, чем отрицание, но \emph{больший}, чем импликация. Таким образом, $\neg X \wedge Y$ означает $(\neg X) \wedge Y$ (но не $\neg (X \wedge Y)$), в то время как $X \wedge Y \to Z \vee W$ равносильно $(X \wedge Y) \to (Z \vee W)$. Другое подобное правило говорит о том, что $X \wedge Y \wedge Z$ означает $(X \wedge Y) \wedge Z$ --- как мы вскоре увидим, формула $X \wedge (Y \wedge Z)$  \emph{не идентична}, но тем не менее \emph{эквивалентна} формуле $(X \wedge Y) \wedge Z$, так что выбор в данном случае произволен (однако важен с формальной точки зрения). Похожее правило работает с ${\vee}$. Взяв эти правила на вооружение, мы можем переписать наш пример:
	$$(A \to \neg(B \to C)) \vee (B \wedge \neg A) \to \neg A \wedge (\neg C \to A) \wedge B.$$
\end{exm}

\paragraph{Эквивалентности, тавтологии и корректные рассуждения.} Предположим два высказывания, $F$ и $G$, составленных из более простых высказываний $A_1,\ldots,A_n$ с помощью логических связок. (В таких случаях будем писать $F(A_1,\ldots,A_n)$.) Высказывания $F$ и $G$ \emph{(логически) эквивалентны} тогда (и только тогда)\footnote{Эта часть нередко подразумевается в математическом определении, но опускается.}, когда они оба истинны либо оба ложны, в зависимости от истинностных значений высказываний $A_1,\ldots, A_n$. Мы будем обозначать данное отношение как $F \equiv G$.
\begin{exm}
	Высказывания $A \wedge B$ и $B \wedge A$ эквивалентны. Это очевидно из их истинностных таблиц. Высказывания $\neg \neg A$ и $A$ также эквивалентны, так как первое истинно, если (и только если) $\neg A$ ложно, то есть если $A$ истинно. 
	
	Высказывания $A \to B$ и $\neg A \vee B$ эквивалентны. Действительно, каждое из них ложно тогда (и только тогда), когда $A$ истинно и $B$ ложно. В противном случае оба они истинны. Аналогично $\neg(A \to B)$ и $A \wedge \neg B$ эквивалентны: каждое истинно тогда и только тогда, когда $A$ истинно и $B$ ложно; иначе, оба они ложны.
	
	Высказывания $A \to \neg B$ и $B \to \neg A$ тоже эквивалентны (этот факт также известен, как \emph{закон контрапозиции}, закон \emph{о контрапозиции} или правило вывода \emph{modus tollens}). Действительно, в каком случае $A \to \neg B$ ложно? Тогда и только тогда, когда $A$ истинно и $\neg B$ ложно. То есть когда и $A$, и $B$ истинны. Однако высказывание $B \to \neg A$ также ложно тогда и только тогда, когда и $B$, и $A$ истинны. Таким образом, рассматриваемые высказывания эквивалентны, потому как невозможно, чтобы одно было истинно, когда ложно другое.
	
	
	
	Напротив, высказывания $A \to B$ и $B \to A$ не эквивалентны, так как их истинностные значения различны при истинном $A$ и ложном $B$.
\end{exm}
\mcomm{}

Высказывание $F$, состоящее из высказываний $A_1,\ldots, A_n$ мы называем \emph{тавтологией} если оно истинно при любых значениях высказываний $A_1,\ldots,A_n$. Тавтологии можно рассматривать как некоторые \emph{законы} логики.
\begin{exm}
	Чтобы проверить, является ли данная формула тавтологией, достаточно построить ее истинностную таблицу. Например, $A \to A$ --- очевидная тавтология. Однако построение истинностных таблиц становится затратной процедурой при большом количестве атомов. Иногда мы можем сэкономить время следующим образом.
	
	Рассмотрим высказывание $F = (A \to B) \to ((B \to C) \to (A \to C))$. Допустим $F$ не является тавтологией, то есть, ложно для каких-то значений высказываний $A, B, C$. Какими могут быть эти значения? Очевидно, когда $A \to B$ истинно, а $(B \to C) \to (A \to C)$ ложно. Последнее подразумевает, что $B \to C$ истинно и $A \to C$ ложно. Тогда $A$ истинно, $C$ --- ложно. В силу $A \to B$, $B$ истинно. В силу же $B \to C$, $C$ истинно, однако это противоречит предыдущему утверждению. Таким образом, перед нами \emph{противоречие}. Потому как рассуждения наши корректны, единственным слабым местом будет утверждение о ложности $F$. Итак, $F$ не может быть ложным.
\end{exm}
\mcomm{}


\paragraph{Рассуждения.} Во время всякого рассуждения (логического аргумента) мы принимаем на веру некоторые \emph{допущения}, исходя из которых затем делаем \emph{вывод}. Мы надеемся, что вывод будет истинным \emph{в любом случае}, покуда допущения (посылки) наши верны (отдельные высказывания в нашем рассуждении могут принимать и истинное, и ложное значения, отсюда и "в любом случае"). В таком случае рассуждение называется \emph{корректным} (общезначимым). (Мы могли бы выразить эту идею иначе: \emph{невозможно, чтобы вывод был ложен, если все посылки истинны}.) Рассмотрим следующий аргумент:
\begin{quote}
	Сегодня четверг. Каждый четверг идет дождь. Значит $3 = 1 + 2$.
\end{quote}
Выглядит не слишком убедительно, несмотря на то, что вывод \emph{всегда} истинен (в то время как истинность первого высказывания зависит от настоящего дня недели). Более того, в силу того, что вывод \emph{гарантированно} истинен \emph{вне зависимости} от истинности посылок, это рассуждение бесполезно, бессмысленно. Слегка переиначим его:
\begin{quote}
	Сегодня четверг. Каждый четверг идет дождь. Значит сегодня идет дождь.
\end{quote}
Гораздо лучше, хотя рассуждение все еще имеет сомнительную логическую силу, так как истинность и посылок, и вывода зависит от настоящего дня недели. Вывод теперь \emph{гарантированно} истинен, покуда посылки истинны. Если посылки все-таки ложны --- что ж, логик умывает руки; об истинности вывода такого аргумента он ничего сказать не может. В случае \emph{общезначимого} же аргумента, истинность вывода при истинности посылок гарантируется \emph{всегда}, в любой день, в любой ситуации. Нам не нужно знать \emph{ничего} об истинностных значениях высказываний аргумента в той или иной ситуации, чтобы быть уверенными в том, что вывод истинен в степени не меньшей, чем истинность посылок.

\mcomm{}

Мы можем переложить это рассуждение в символы:
$$(T \wedge (T \to R)) \to R,$$
где $T$ означает `сегодня четверг', а $R$ --- `сегодня идет дождь'. (Обратите внимание на принцип: если посылки --- это $A_1, \ldots, A_n$ и вывод --- $C$, мы составляем аргумент в виде формулы $(A_1 \wedge  \ldots \wedge A_n) \to C$). Нетрудно увидеть, что это выражение является тавтологией. В общем случае, рассуждение корректно, если соответствующая ему формула --- тавтология (однако обратное не всегда верно).

\begin{exm}
	Рассмотрим пример:
	\begin{quote}
		Сегодня идет дождь. Каждый четверг идет дождь. Значит сегодня четверг.
	\end{quote}
	Можно записать это рассуждение как $(R \wedge (T \to R)) \to T$, где атомы $T$ и $R$ имеют те же значения, что и выше. Получившееся выражение не является тавтологией, так как оно ложно, когда $R$ истинно и $T$ ложно. Рассуждение действительно не корректно: он неверен, если, например, дождь идет каждый день, но сегодня суббота.
\end{exm}

Занимаясь научными изысканиями, мы надеемся делать неизвестные нам ранее, но необходимо истинные заключения из имеющихся посылок (данных, допущений, фактов). Поэтому ученый должен следовать логически корректным рассуждениям везде, где это возможно. В частности, любое математическое доказательство должно быть подобным рассуждением (общезначимым аргументом).

\section{Язык математики}
\mcomm{}

Сейчас мы попробуем исследовать внутреннюю структуру атомов. Разделим их! Каждый атом --- утвердительное предложение, а поверхностный лингвистический анализ говорит нам, что каждое такое предложение должно иметь \emph{подлежащее} (субъект) и \emph{сказуемое} (предикат). Можно сказать, что сказуемое --- то, \emph{что говорят}, а подлежащее --- то, \emph{о ком/чем говорят}.\footnote{Иногда выявить эти две составляющие не так просто: например, что является подлежащим в предложении \emph{сегодня пасмурно}? Что "пасмурно"?} Например, в предложении
\begin{quote}
	Роза упала на лапу Азора,
\end{quote}
`розу' можно рассматривать в качестве подлежащего, а `упала на лапу Азора' --- как сказуемое. Предложение можно слегка поменять:
\begin{quote}
	Фиалка упала на лапу Азора.
\end{quote}
То же сказуемое (предикат) применяется к другому подлежащему. Можно заметить, что выражение
\begin{quote}
	$x$ упал на лапу Азора
\end{quote}
превращается в высказывание, когда мы заменяем $x$ (который называют \emph{переменной}, потому как он меняется) на название более содержательного объекта. Игнорируя грамматическую терминологию, выражение
\begin{quote}
	$x$ упал на лапу $y$,
\end{quote}
которое превращается в высказывание после подстановки имен на место \emph{переменных} $x$ и $y$, можно также назвать \emph{предикатом}. Первый рассмотренный нами предикат содержит одну переменную, и потому называется \emph{унарным}, а второй содержит две различные переменные, и называется \emph{бинарным}. Развивая эту мысль, можно рассматривать \emph{$n$-арные} предикаты с переменными $x_1,\ldots,x_n$.

Обобщая, \emph{предикат} (или \emph{свойство}) --- это выражение, которое превращается в логическое высказывание (истинное либо ложное) после замены всех переменных на названия более конкретных объектов. Также, любое высказывание можно рассматривать как $0$-арный (\emph{нульарный}) предикат.

\begin{exm}
	Следующие выражения --- типичные предикаты: \emph{$x$ четно}, \emph{$x$ больше, чем $y$}. Они превращаются в высказывания (например, \emph{$4$ четно}, \emph{$3$ больше, чем $7$}) если мы подставим вместо переменных числовые значения. С другой стороны, предикат \emph{$x$ --- сын $y$} может стать высказыванием при подстановке вместо переменных человеческих имен.
\end{exm}
Весьма естественно, интуитивно соотносить с предикатом некоторый \emph{домен} (область значений), откуда берутся подставляемые объекты-значения. Например, областью значений предиката \emph{$x$ четно} могут быть натуральные или целые числа, в то время как эти же области значений не будут иметь смысла для предиката \emph{$x$ --- сын $y$}.

\mcomm{}

Мы без труда можем составлять новые предикаты из имеющихся с помощью логических связок. Например, \emph{$x$ четно, и неправда, что $y$ больше, чем $x$}. Но для этого есть и более интересный способ. А именно, можно составить суждение о всей области значений: \emph{существует некоторый $x$ такой, что $x$ четен}; \emph{все $x$ четны}; \emph{существует много четных $x$-ов}; \emph{существует единственный четный $x$}; \emph{большинство $x$-ов четны}; \emph{существует бесконечно много четных $x$-ов}, etc.

Подобные конструкции называются \emph{кванторами}. Квантор \emph{связывает} некоторую переменную в предикате так, что она более не доступна для подстановки вместо нее значения (такая переменная зовется \emph{связанной}; в противном случае она зовется \emph{свободной}). И в самом деле, предикат \emph{либо $x$ четен, либо $x$ нечетен} можно превратить в высказывание \emph{либо $7$ четно, либо $7$ нечетно}, в то время как высказывание \emph{для каждой $7$, либо $7$ четно, либо $7$ нечетно} не имеет большого смысла.

Из многих известных математикам кванторов, есть два наиболее важных. Эти кванторы --- квантор \emph{существования} (\emph{существует $x$ такой, что \dots}), и квантор \emph{всеобщности} (\emph{для каждого $x$ верно, что \dots}). Иногда мы будем писать $A(x)$ для обозначения предиката $A$ с переменной $x$ (например, \emph{$x$ --- \dots}, \emph{$x$ является \dots}, etc.). Так, можно записать предикат \emph{$x$ четно} как $\text{четный}(x)$; здесь $A$ можно в свою очередь выразить словами, \emph{быть четным}.

\begin{center}
	\begin{tabular}{|c | c | c |}
		\hline
		\bf Квантор & \it существования & \it всеобщности \\
		\hline
		\bf В логике & существует некоторый $x$ такой, что $A$  & для любого $x$ верно $A$ \\
		\hline
		\bf В символах & $\exists x\, A(x)$; $\exists x\, A$   & $\forall x\, A(x)$; $\forall x\, A$ \\
		\hline
		\bf На русском 
		& для некоторого $x$, $A(x)$; & для всех $x$, $A(x)$; \\
		& для какого-то $x$, $A(x)$;  & для любого $x$, $A(x)$; \\
		& что-то $A$; кто-то $A$; &  для произвольного $x$, $A(x)$; \\
		& хотя бы один $A$;  & каким бы $x$ ни был, $A(x)$; \\
		& для хотя бы одного $x$, $A(x)$; & $A(x)$ всегда верно; \\
		& есть $x$ такой, что $A(x)$ & всё $A$; все $A$  \\
		\hline
	\end{tabular}
\end{center}
\mcomm{}

\begin{exm}
	Давай переведем несколько фраз на русском на символический язык логики. \emph{Все розы красные}. Для перевода этого предложения, нам нужны предикаты, выражающие степень того, является ли объект красным и розой. Так $Рз(x)$ будет означать \emph{$x$ является розой}, а $\text{Кр}(x)$ --- \emph{$x$ красный}. Домены для этих предикатов должны быть выбраны разумно --- «все цветы» или даже «все существующие объекты». Тогда наша фраза будет выглядеть как $\forall x\, (\text{Рз}(x) \to \text{Кр}(x))$. Обратите внимание на наличие импликации здесь: мы правда \emph{удостоверяемся}, что $x$ красный --- но при условии, что $x$ это роза. Мы обходимся без утверждений о том, что какая-либо не-роза является красной, а также не утверждаем какой бы то ни было объект в качестве розы\footnote{Если бы первое было неверным, то предикат $\text{Кр}(x)$ стоял бы вне импликации, либо вместе с другой посылкой. Если же второе было бы неверным, предикат $\text{Рз}(x)$ находился бы по правую сторону импликация или вне ее; в данном же случае это допущение, посылка.}.
	
	\emph{Никакая роза не красная}. Данная фраза может звучать как отрицание предыдущей, но это совсем не так. Можно переложить ее в язык символов несколькими способами: $\forall x\, (\text{Рз}(x) \to \neg \text{Кр}(x))$ (\emph{все, что является розой, не красное}), $\neg \exists x\, (\text{Рз}(x) \wedge \text{Кр}(x))$ (\emph{неправда, что существует нечто и красное, и являющееся розой}), $\neg \exists x\, (\text{Кр}(x) \wedge \text{Рз}(x))$, $\forall x\, (\text{Кр}(x) \to \neg \text{Рз}(x))$ (\emph{все красное не является розой}).
	
	Заметьте, что на далекой планете, где не существует роз, и $\forall x\, (\text{Рз}(x) \to \text{Кр}(x))$, и $\forall x\, (\text{Рз}(x) \to \neg \text{Кр}(x))$ будут истинны, так как посылки ложны, ведь $\text{Рз}(x)$ даст «ложь» для любого $x$ на той планете, откуда импликации станут истинными. Таким образом, эти две формулы не являются отрицанием друг друга. Подробнее, см. Пример~\ref{exm_vacuous} ниже.
	
	Но как тогда выразить отрицание первой фразы? $\neg \forall x\, (\text{Рз}(x) \to \text{Кр}(x))$, само собой, или, эквивалентно, $\exists x\, (\text{Рз}(x) \wedge \neg \text{Кр}(x))$ (\emph{существует нечто, являющееся розой, но при этом не красное}). Это превращение импликации с квантором под действием отрицания не так запутанно, как может показаться на первый взгляд: здесь применяются несколько простых законов логики (см. ниже).
\end{exm}

Некоторые кванторы могут быть выражены через другие. Например, выражение $\exists x\, A(x)$ обычно рассматривается как эквивалентное выражению $\neg \forall x\, \neg A(x)$ («некоторый $x$ удовлетворяет свойству $A$ тогда и только тогда, когда неверно, что свойство $A$ дает «ложь» при применении его к любому $x$»), и аналогично, $\forall x\, A(x)$ можно рассматривать как эквивалентное $\neg \exists x\, \neg A(x)$. Если мы вооружены корректно определенным равенством, можно выразить \emph{существует единственный $x$ такой, что $A(x)$} как $\exists x\, (A(x) \wedge \forall y\, (A(y) \to y = x))$.

В общем случае, два предиката зовутся (логически) эквивалентными, если они истинны (или ложны) «одновременно»\footnote{Сделать это «определение» более строгим --- слишком большая задача для нас сейчас.}. Таким образом, $\exists x\, A(x) \equiv \neg \forall x\, \neg A(x)$ и $\forall x\, A(x) \equiv \neg \exists x\, \neg A(x)$.

\begin{exm}
	Продолжая предыдущий пример, имеем 
	\begin{multline*}
		\neg \forall x\, (\text{Рз}(x) \to \text{Кр}(x)) \equiv \neg \neg \exists x \neg\, (\text{Рз}(x) \to \text{Кр}(x)) \equiv\\
		\exists x \neg\, (\text{Рз}(x) \to \text{Кр}(x)) \equiv\exists x\, (\text{Рз}(x) \wedge \neg \text{Кр}(x)),
	\end{multline*}
	как $\neg (A \to B) \equiv A \wedge \neg B$.
\end{exm}

\begin{rem}
	Когда квантор используется в какой-либо формуле, он связывает все вхождения своей переменной в какой-то части формулы. Например, формула $\forall x\, A(x) \to \forall x\, B(x)$ обыкновенно читается как $\forall x\, ( A(x) ) \to \forall x\, ( B(x) )$, то есть \emph{область действия} первого квантора ограничена подформулой $A(x)$, а второго --- подформулой $B(x)$. Связывание квантора сильнее (имеет больший приоритет), чем любая логическая связка, так что как правило область действия квантора заканчивается на первой бинарной логической связке справа от него. Чтобы включить связку в область действия квантора, необходимы скобки: например, в $\forall x\, (A (x) \to B(x))$ квантор связывает вхождения $x$ и в $A$, и в $B$. С другой стороны, в формуле $\forall x\, A(x) \to B(x)$, никакое вхождение $x$ в $B(x)$ не связано первым квантором. То есть любое такое вхождение свободно для подстановки вместо него значения (если не связано каким-либо квантором внутри $B(x)$).
	
	Если два вхождения $x$ связаны одним и тем же квантором, они \emph{гарантированно} указывают на один объект. Скажем, в $\exists x\, (A(x) \wedge B(x))$, мы имеем в виду \emph{один и тот же} $x$, удовлетворяющий и $A$, и $B$. Если два вхождения переменной принадлежат разным областям действия, они могут указывать на разные объекты. Например, $\exists x\, A(x) \wedge \exists x\, B(x)$ утверждает, что что-то удовлетворяет свойству $A$, и что-то (возможно другое) удовлетворяет $B$ --- тождество этих двух "что-то" ни утверждается, ни опровергается. Таким образом, мы можем легко менять названия связанных переменных: $\exists x\, A(x) \wedge \exists x\, B(x) \equiv \exists x\, A(x) \wedge \exists y\, B(y)$.
	
	Говоря языком программистов, связанная переменная \emph{локальна} и сохраняет свое значение в пределах своей области видимости (scope), но не за пределами последней.
\end{rem}

Используя квантор с переменной $x$, мы обязаны зафиксировать ее \emph{домен}, откуда позволяется брать объекты для подстановки вместо $x$. Например, высказывание $\forall x\, ( \text{четный}(x) \vee \text{нечетный}(x))$ имеет смысл для целочисленных доменов, но вряд ли для случая, когда $x$ может обозначать, скажем, минерал. Истинностное значение высказывания, использующего кванторы, сильно зависит от выбираемых доменов для их переменных.

\begin{exm}
	Рассмотрим высказывание $\exists x\, (2 x + 3 = 1)$. Это, очевидно, означает, что уравнение $2x + 3 = 1$ имеет решение. Но в каком домене, в каких числах? Есть целочисленное (а значит рациональное и вещественное) решение $x = -2$, но в натуральных числах это уравнение решить нельзя. Таким образом, высказывание истинно для домена целых чисел, но ложно для домена натуральных чисел.
	
	Еще один пример --- высказывание $\exists x \forall y\, x \leq y$ ложно для целочисленного домена, но истинно для натурального (например, при $x = 0$).
\end{exm}

\begin{exm}\label{exm_vacuous}
	В математических рассуждениях нередко встречается вырожденный случай, воспринимаемый иногда как парадоксальный. Рассмотрим высказывание:
	\begin{quote}
		Сегодняшний король Франции лысый.
	\end{quote}
	Если мы истрактуем это как 
	\begin{quote}
		Для любого $x$, если $x$ --- сегодняшний король Франции, тогда $x$ лысый,
	\end{quote}
	и выберем множество всех живущих и когда-либо живших людей в качестве домена, то увидим, что \emph{$x$ --- сегодняшний король Франции} ложно для всех $x$ (на момент $2023$ г. н. э.), то есть вся импликация \emph{истинна}. Очевидно, высказывание «сегодняшний король Франции волосат» не менее истинно. Подобные ситуации (и сами «парадоксально» истинные высказывания) называют \emph{пустыми истинами}, которые действительно нередки в математике.
	
	Например, стандартный алгоритм доказательства утверждения вида \emph{не существует такого $x$, что $A(x)$} следующий: рассмотрим произвольный $x$ такой, что $A(x)$ истинно, и докажем, что какое-то известно ложное высказывание \emph{логически} следует из $A(x)$. Следовательно, если бы $x$ с истинным $A(x)$ существовал, то заведомо ложное высказывание было бы истинным, что невозможно. Таким образом, такого $x$ не существует. Заметьте, что все это время мы рассуждали об $x$, оказавшимся \emph{несуществующим} к концу нашего рассуждения. То есть наша \emph{логика} должна быть корректна в таких \emph{противоречащих фактам} случаях, когда мы действительно доказываем пустую истину: «для любого $x$ с истинным $A(x)$, ложность истинна». 
\end{exm}
\mcomm{}

Давайте проверим на корректность несколько рассуждений. Как мы писали выше, рассуждение корректно тогда и только тогда, когда вывод истинен при истинности посылок.

\begin{exm}
	Начнем с классики. Рассмотрим рассуждение:
	\begin{quote}
		Все люди смертны. Сократ человек. Значит Сократ смертен.
	\end{quote}
	Так как мы знаем, кем был Сократ, мы уверены в его смертности. Но что если бы мы не знали этого? (Стандартная ситуация на практике.) Что если значение имени «Сократ» переменно?
	
	Можно переложить это высказывание в символы как $\forall x\, (\text{Ч}(x) \to \text{С}(x)) \wedge \text{Ч}(\text{Сократ}) \to \text{С}(\text{Сократ})$, где $\text{Ч}$ --- «быть человеком», а $\text{С}$ --- «быть смертным», и выбрав в качестве домена любой, включающий множество всех людей. По гипотезе, $\text{Ч}(x) \to \text{С}(x)$ должно быть истинным для всех $x$ из домена. Если мы \emph{инстанцируем} $x$ как (подставим на его место) имя «Сократ», получим тавтологию: $(\text{Ч}(\text{Сократ}) \to \text{С}(\text{Сократ})) \wedge \text{Ч}(\text{Сократ}) \to \text{С}(\text{Сократ})$. Значит рассуждение корректно. Однако обратите внимание, что изначальная формула не является тавтологией, так как первая ее часть --- $\forall x\, (\text{Ч}(x) \to \text{С}(x))$ --- это атом, чья внутрення структура не может быть выражена с помощью одних лишь логических связок (то есть с появлением кванторов разрешимость логических формул значительно затрудняется).
\end{exm}
\mcomm{}

\begin{exm}
	Вот то же высказывание, но слегка измененное:
\end{exm}
\begin{quote}
	Любое (натуральное) число, которое делится на $4$, четно. Все числа делятся на $4$. Значит все числа четные.
\end{quote}
Здесь мы снова имеем заведомо ложную посылку. Однако знание «заведомой ложности» требует некоторого знания арифметики, в то время как логика позволяет нам судить о корректности рассуждения без всякого априорного знания.

Итак, мы можем формализовать высказывание так: $\forall x\, (\text{Дел}(x) \to \text{Чет}(x)) \wedge \forall x\, \text{Дел}(x) \to \forall x\, \text{Чет}(x)$ на домене $\N = \{0,1,2,\ldots\}$ натуральных чисел, где $\text{Дел}(x)$ означает «$x$ делится на $4$», а $\text{Чет}(x)$ --- «$x$ четно». Очевидно, первая посылка истинна, а вторая ложна. Однако нам не нужно знание о числах, чтобы судить о корректности суждений. Действительно, если наши посылки верны (или \emph{были бы} верны --- русская грамматика уже подразумевает нереальность, невозможность условия), мы могли бы \emph{доказать} истинность вывода.

В самом деле, исходя из посылок, нам необходимо $\forall x\, \text{Чет}(x)$. Рассмотрим \emph{произвольное} (т.е. неотличимое от других элементов домена) число $x'$. Так как любое число удовлетворяет предикату $\text{Дел}$, наше $x'$ тоже удовлетворяет ему --- значит $\text{Дел}(x')$ истинно. Мы можем при этом инстанцировать $x$ как $x'$ в первой посылке. Так как и $\text{Дел}(x') \to \text{Чет}(x')$, и $\text{Дел}(x')$ истинны, $\text{Чет}(x')$ тоже истинно. А потому как наш $x'$ был произвольным, мы можем получить тот же вывод с любым другим $x$ из домена, а затем \emph{обобщить} результат и получить $\forall x\, \text{Дел}(x)$.
\begin{exm}
	
	Следующий пример призван продемонстрировать большую выразительную силу кванторов:
	\begin{quote}
		Когда я голоден, я хочу пойти домой или в ресторан. Иногда я голоден, но не хочу идти домой. Значит иногда я хочу пойти в ресторан. 
	\end{quote}
	Главное здесь --- выбрать правильный домен. Как вариант --- течение, продолжительность времени (то есть все переменные означают некоторую точку во времени). Тогда мы получаем: $$\forall x\, (\text{Г}(x) \to \text{Дом}(x) \vee \text{Р}(x)) \wedge \exists x\, (\text{Г}(x) \wedge \neg \text{Дом}(x)) \to \exists x\, \text{Р}(x).$$ Это рассуждение тоже корректно. Действительно, пусть $x_0$ будет некоторым моментом во времени когда я голоден, но не хочу домой. Можно инстанцировать $x$ как $x_0$:
	$$(\text{Г}(x_0) \to \text{Дом}(x_0) \vee \text{Р}(x_0)) \wedge \text{Г}(x_0) \wedge \neg \text{Дом}(x_0) \to \text{Р}(x_0).$$
	Таким образом, из посылок следует истинность $\text{Р}(x_0)$, откуда следует истинность $\exists x\, \text{Р}(x)$.
\end{exm}

\begin{exm}
	Наконец, последний пример:
	\begin{quote}
		Каждый человек любит себя. Следовательно, всякий человек любим кем-то.
	\end{quote}
	Перекладывая в формулы, получим что-то вроде $\forall x\, \text{Л}(x,x) \to \exists y \exists x\, \text{Л}(x,y)$. Корректно ли это рассуждение? На первый взгляд --- да, ведь из посылки очевидно следует $\forall y \exists x\, \text{Л}(x,y)$ (можно, например, взять $y$ как $x$ здесь и инстанцировать $x$ как $y$ в посылке). Но следует ли из последнего $\exists y \exists x\, \text{Л}(x,y)$? В общем случае, следует ли $\exists z\, A(z)$ из $\forall z\, A(z)$?
	
	Ответ зависит от того, пуст ли домен (вспомните \emph{пустые истины} о \emph{несуществующем} «сегодняшнем короле Франции»). Обыкновенно требуется, чтобы домены были непустыми. Тогда рассуждение должно быть корректно.
\end{exm}

\begin{exc}
	Рассмотрим рассуждение:
	\begin{quote}
		Какой-то ключ может открыть любую дверь. Следовательно любая дверь может быть открыта каким-то ключом.
	\end{quote}
	Переложите это рассуждение в символы и проверьте его на корректность. Сделайте то же самое для «обратного» рассуждения:
	\begin{quote}
		Любая дверь может быть открыта каким-то ключом. Следовательно какой-то ключ может открыть любую дверь.
	\end{quote}
	
\end{exc}

Формальные законы для проверки подобных рассуждений на корректность определяются \emph{предикативной} логикой или логикой \emph{первого порядка} и выходят за рамки нашего курса.

\section{A Case Study: Strings}\label{sect:strings}

\mcomm{This section introduces an `inductive type' of strings without much `foundational' explanation. The main goal is to present the ideas of induction and recursion (which we looked upon as the heart of `discrete mathematics') without boring traditional examples of summing consecutive naturals, etc. We see this especially useful when the students have some (functional) programming experience. This section has few (if any) dependencies in the Course and may thus be freely omitted.}

Our next goal is to taste some \emph{discrete mathematics} while trying to stick to the logical formalism. We are not going to dig too deep for the `foundations' and will take some notions as familiar and statements as obvious. Our case study deals with \emph{strings} (or \emph{lists}, or \emph{words}), which are one of the most basic programming `data types'.

Intuitively, a \emph{string} is a finite sequence of \emph{symbols} of arbitrary nature. For example, $[x, y, x]$, $[4, 12, 2, 3]$, and $[\mbox{Socrates}, \mbox{Plato}, \mbox{Aristotle}]$ are strings where $x$, $y$, $3$, $12$, and $\mbox{Plato}$ are symbols. We use $[\ldots]$ and commas here to separate stings and symbols from each other. If we know that, say, $x$ and $y$ are treated as symbols (but not $xy$), we may safely omit the delimiters and render the first string as $xyx$. In general, we shall consider the strings over an arbitrary but fixed collection (or \emph{set}, in mathematical parlance) $A$ of symbols, which is called an \emph{alphabet}. E.\,g., $[4, 12, 2, 3]$ is a string over the set of all natural numbers $\N = \{0,1,2,\ldots\}$. Needless to say, an alphabet could be a set of strings over some other alphabet.

As it is usual in mathematics, we are interested in `borderline' degenerate objects. The empty string $[\ ]$ that contains no symbol is just such an object.

\paragraph{Inductive definition.} Our account of what a string is seems quite clear for a typical human (but not so much for the Mathematician who could ask what `finite sequence' means, etc.). But if we try to make this definition clear for a computer, that is, to describe it programmatically from `scratch', we may discover this task being quite hard. Typically, one has a certain `array' or `string' type already implemented in his favorite programming language. But what if you have not one?

One possible approach is based on the following observation: every string $s$ is either empty or is obtained from another string $s'$ by adding a certain symbol $x$ to it, or $s = x : s'$ is symbols. (Assuming that $x$ is added as the leftmost symbol, we obtain $[4,12, 2] = 4 : [12, 2] = 4 : (12 : [2]) = 4 : (12 : (2 : [\ ]))$. Let us call $x$ the \emph{head} of the string $s$, and call $s'$ the \emph{tail} thereof. Now, we may \emph{forget} about `finite sequences' and \emph{define} the set $S(A)$ of strings over an alphabet $A$ via the following generating rules:
\begin{quote}
	$[\ ]$ is a string over $A$;\\
	if $s'$ is a string over $A$ and $x$ is a symbol from $A$, then $x : s'$ is a string over $A$,
\end{quote}
which might be rendered symbolically as
$$[\ ] \in S(A);\quad \forall s'\; \forall x \left(\ (s' \in  S(A) \wedge  x \in A)\ \to\ x : s' \in S(A)\  \right).$$
We also tacitly suppose that \emph{every} sting is constructed according to these rules. Here, the symbols $[\ ]$ and $:$ \emph{lack any definition}. They are kind of `axioms' or, as the Programmer would say, \emph{constructors} (of the data type `string'). In a word, we replace the question \emph{what a string is} with the question \emph{how one can construct a string}. Along these lines, we can easily return to our `natural' symbolism by defining $[x_1, x_2, \ldots, x_n]$ as $x_1 : (x_2 : (\ldots : (x_n : [\ ])\ldots)))$.

Such `definitions by construction' are known as \emph{inductive definitions}.

\paragraph{Induction Principle.} If we want to prove anything about our redefined strings, we need to fix some basic properties which will be used as `axioms' in our arguments. Clearly, these properties should be as intuitively plausible as possible.

First, we assume that $[\ ] \neq x : s$ for all $x$ and $s$, as the empty string should differ from any other. Then we postulate that $x : s = y : t \iff (x = y \wedge s = t)$ for all $x, y \in A$ and $s, t \in S(A)$, that is two strings constructed via $:$ are equal iff their respective heads and tails are equal. Let us denote the first axiom by (A1) and the second one denote by (A2).

\begin{exm}
	The facts the principles (A1) and (A2) can prove are not too astonishing. We can show, say, that $[1,2] \neq [1,2,3]$. We need then to check that the equation $[1,2] = [1,2,3]$ is false. We will yet suppose that it is true, and then prove such a situation impossible. Hence, the equation must be false indeed. Such a technique is know as \emph{proof by contradiction} and is widely used.
	
	So, assume  $1:[2] = 1:[2,3]$. From (A2), it follows that $2 : [\ ] = [2] = [2,3] = 2 : [3]$. Yet another application of this principle results in $[\ ] = [3] = 3 : [\ ]$. On the other hand, $[\ ] \neq 3 : [\ ]$ by (A1). Thus, the equality $[\ ] = 3 : [\ ]$ is both true and false, which is not possible. A contradiction. Hence, our assumption does fail and, as a matter of fact, $[1,2] \neq [1,2,3]$.
	
	The Pedantic Reader might have noticed that we use a little more than just principles (A1) and (A2) in our argument. Namely, we have used `obvious' properties of equality, like \emph{transitivity} (if $x = y$ and $y = z$, then $x = z$), and the logic itself ($\neg P$ and $P$ cannot be both true). In the next exercise, you are also supposed to use the `obvious' properties of the familiar alphabet $\N$, like $0 \neq 1$.
\end{exm}

\begin{exc}
	Prove formally that $[1,2,3] \neq [1,3,4]$ and $[1 + 2, 1 + 1] = [5 - 2, 2]$ (all strings are over the alphabet $\N$; $1 + 2$ is just another name for the symbol $3$, so that $1 + 2 = 3$).
\end{exc}

To prove anything interesting, we need another principle. Surprisingly, one is enough. Being so versatile, this principle is necessary abstract.

Let $P(s)$ be a unary predicate with the domain $S(A)$, that is, a property of strings over $A$. Say, one might have $P = \mbox{``John likes $s$ better than the number $n$''}$ where $n$ is fixed. Assume that we want to prove that \emph{each string $s$ over $A$ satisfies the property $P$}, or $\forall s\, P(s)$ symbolically.

With this in view, we could consider an \emph{arbitrary} string $t$ and try to prove $P(t)$. As we know \emph{nothing special} about $t$, there is not much we can do here. But we know that each string has been constructed according to our inductive definition, that is, $t$ is either $[\ ]$ or $x : t'$ for some $x \in A$ and another string $t'$. For our goal $\forall s\, P(s)$, it is clearly necessary that $[\ ]$ satisfies $P$, or $P([\ ])$ in symbols. Assume that it is the case.

If $t = x : t'$, we get just another `arbitrary' string $t'$. It is yet clear intuitively that if $t'$ is not $[\ ]$, we would have  $t' = x' : t''$ etc.,---and get $[\ ]$ eventually after a finite number of steps. This gives us the following idea: if $P([\ ])$ and $P$ survives each step of string construction, i.\,e., from $P(t')$ it follows $P(x : t')$ for every $t'$ and $x$ (in symbols, $\forall t' \forall x\; (P(t') \to P(x:t'))$), then we obtain $P( t )$ for an arbitrary $t$ and, finally, $\forall s\, P(s)$.

To sum it up, we want to use the following \emph{Induction Principle} (IP) for strings:
\begin{quote}
	for every unary predicate $P$ over $S(A)$, if $P([\ ])$ and  $\forall s \forall x\; (P(s) \to P(x:s))$, then $\forall s\, P(s)$.
\end{quote}
The assumption $P([\ ])$ is called the \emph{base case} of induction, $\forall s \forall x\; (P(s) \to P(x:s))$ is called the \emph{inductive step}, and $P(s)$ is called \emph{inductive hypothesis} (which is usually abbreviated to IH) when used to infer $P(x:s)$ from it.

While we cannot prove (IP) in our current setting and have to take this principle as an axiom, we can support its intuitive validity by the following argument.
\begin{exm}\label{strings:ind_intuition}
	We let $A = \N$ and let both the assumptions $P([\ ])$ (ass.~1) and  $\forall s \forall x\; (P(s) \to P(x:s))$ (ass.~2) hold. Consider the string $t = [1,2,3] = 1 : (2 : (3 : [\ ]))$. From (IP), it follows that $\forall s\, P(s)$, whence $P(t)$. But one can prove this particular instance $P([1,2,3])$ directly from our assumptions \emph{without} (IP). Indeed, we obtain the following derivation:
	$$
	\begin{array}{rll}
		(1)&P([\ ])&\qquad\mbox{by (ass.~1)}\\
		(2)&P([\ ]) \to P(3 : [\ ])&\qquad\mbox{by (ass.~2)}\\
		(3)&P(3 : [\ ])&\qquad\mbox{from (1) and (2)}\\
		(4)&P(3 : [\ ]) \to P(2 : (3 : [\ ]))&\qquad\mbox{by (ass.~2)}\\
		(5)&P(2 : (3 : [\ ]))&\qquad\mbox{from (3) and (4)}\\
		(6)&P(2 : (3 : [\ ])) \to P(1 : (2 : (3 : [\ ])))&\qquad\mbox{by (ass.~2)}\\
		(7)&P(1 : (2 : (3 : [\ ])))&\qquad\mbox{from (5) and (6)}\\
	\end{array}
	$$
	It is clear that our $t$ is nothing special, so we can prove $P(s)$ for each particular $s$ this way, while the proof gets longer with $s$. Essentially, the Induction Principle `packs' this infinite multitude of proofs of unboundedly increasing length into one proof based on (IP).
\end{exm} 

\paragraph{Recursive functions.} So far, we have a mighty principle to prove things but not many statements to try it for. In programming practice, one defines a plethora of \emph{functions} over strings, whose properties are natural challenges to prove. As in the case of Induction Principle for proofs, we will take advantage of the the \emph{inductive} definition for strings when defining our functions.

Let us start with the string \emph{length}. This function takes a string $s$ over $A$ and returns a natural number $lh(s)$. Intuitively, we want the following equations to hold: $lh([\ ]) = 0$, $lh([8]) = 1$, $lh([3,11]) = 2$, and so on. We thus \emph{define} $lh$ by these two rules:
\begin{quote}
	$lh([\ ]) = 0$;\\
	$lh(x : s) = 1 + lh(s)$ for all $x$ and $s$.
\end{quote}
We assume such a function $lh$ that satisfies the two properties does indeed exist and is unique. They say that $lh$ is defined \emph{by recursion} on strings (since one \emph{recurs} to the `previous' value $lh(s)$ in order to get $lh(x: s)$). As in the case of (IP), the definition mimics somehow the structure of the inductive definition for strings.

\mcomm{It is easy to prove the uniqueness by IP application but the existence would likely require some `set-theoretic' considerations, which we want to avoid here.}

\begin{exm}
	Our definition for $lh$ is \emph{computationally effective}, i.\,e., it provides a recipe to compute the value $lh(s)$ for every particular string $s$. For example, $lh([3,11]) = lh (3:(11:[\ ])) = 1 + lh (11:[\ ]) = 1 + (1 + lh([\ ])) = 1 + (1 + 0) = 2$.
\end{exm}

\begin{exc}
	For strings over the English alphabet, prove that $lh(student) = 7$ formally.
\end{exc}

Another interesting function is \emph{append} (or \emph{concatenate}) which `glues' two strings together, so that $app([1,3,2],[3,2]) = [1,3,2,3,2]$ and $app([5,8], [\ ]) = app([\ ], [5,8]) = [5, 8]$. We can define $app$ recursively the following way:
\begin{quote}
	$app([\ ], t) = t$ for each $t$;\\
	$app(x : s, t) = x : app(s, t)$ for all $x$, $s$, and $t$.
\end{quote}
\begin{exc}
	Prove formally that $app([1,3,2],[3,2]) = [1,3,2,3,2]$.
\end{exc}

Now, we have got just enough to state and prove moderately interesting theorems about string functions. From the definition, we know that $app([\ ], s) = s$. But what can one say about $app (s, [\ ])$? Clearly, $app ([1, 2], [\ ]) = 1 : app ([2],  [\ ]) = 1 : (2 : [\ ]) = [1, 2]$. Unsurprisingly, in general we have
\begin{lemma}\label{cs:app_nil_r}
	For every $s \in S(A)$, $app (s, [\ ]) = s$.
\end{lemma}
\begin{proof}
	By induction on $s$. If we let $P$ be just the equation $app (s, [\ ]) = s$, it suffices to show that $P([\ ])$ and $\forall s \forall x\; (P(s) \to P(x:s))$ hold, that is, $app ([\ ], [\ ]) = [\ ]$ and $\forall s \forall x\; (app (s, [\ ]) = s \to app (x : s, [\ ]) = x : s)$. The first statement holds by the definition of $app$. Let us prove the second one. Consider arbitrary $s$ and $x$ and assume that $app (s, [\ ]) = s$ is true (otherwise, the implication is clearly true). By definition, we have $app (x : s, [\ ]) = x : app(s, [\ ])$, whence $app (x : s, [\ ]) = x : s$ as required. Generalizing $s$ and $x$, we obtain what we want.
\end{proof}

It appears that $[\ ]$ plays the same role for $app$ as $0$ does for ${+}$, where one has $n + 0 = n = 0 + n$. How far can we extend this analogy? For example, we know that $n + (m + l) = (n + m) + l$. What about $app$?

\begin{lemma}\label{cs:app_assoc}
	For every $s, t, r \in S(A)$, $app (s, app(t, r)) = app (app (s, t), r)$.
\end{lemma}
\begin{proof}
	As (IP) deals with unary predicates while the equation $app (s, app(t, r)) = app (app (s, t), r)$ has three parameters, we have to fix two of them. The right choice of one to base the induction on is crucial! We consider some arbitrary strings $t$, $r$ and apply induction on $s$ for those fixed with $P(s) = (app (s, app(t, r)) = app (app (s, t), r))$. 
	
	Clearly, $P([\ ]) = (app ([\ ], app(t, r)) = app (app ([\ ], t), r))$. Both sides of this equation equal $app(t,r)$ by the definition of $app$, hence the statement $P([\ ])$ holds. Now, let us assume $P(s) = (app (s, app(t, r)) = app (app (s, t), r))$ for an arbitrary $s$ and $x$, then try to get $P(x:s)$. Indeed,
	\begin{multline*}
		app (x:s, app(t, r)) = x : app (s, app(t, r)) = x : app (app (s, t), r)) =\\
		app (x : app (s, t), r)) = app (app (x : s, t), r)).
	\end{multline*}
	Here, we have applied the inductive hypothesis $P(s)$ and the definition of $app$. As both the base case and the inductive step are proved now, we may conclude $\forall s\, P(s)$ by virtue of $(IP)$. Generalizing our arbitrary $t$ and $r$, we obtain the required statement.
\end{proof}
\begin{exc}
	Try to prove the statement above by induction on either $t$ or $r$. What obstacles do you face when doing so? Can you overcome them?
\end{exc}

In general, you should apply induction to the argument that is used in recursive definitions of the functions involved. The function $app$ is defined by recursion on its \emph{first} argument, whence our choice of induction on $s$.

\medskip
So far so good, but we also have $n + m = m + n$ for any numbers $n, m$. Does a similar property hold for $app$?
\begin{exc}
	Prove formally that it does not, i.\,e., it is not the case that $app(s,t) = app(t,s)$ for every $s$ and $t$. Notice that you do not need (IP) here but rather (A1) and (A2).
\end{exc}

The operations $app$ on strings and $+$ on natural numbers are not only somewhat similar but `interrelated' via the function $lh$. In fact, one has $lh([\ ]) = 0$ and $lh(app(s, t)) = lh(s) + lh(t)$ for every $s, t$. Relations of this form are called \emph{homomorphisms} in mathematics.
\begin{exc}
	Prove these statements formally.
\end{exc}

For natural numbers, from $n + m = n + l$, it follows that $m = l$. Does a similar property hold for $app$?
\begin{lemma}\label{cs:app_inv_head}
	For every $s, t, r \in S(A)$, if $app(s,t) = app(s,r)$, then $t = r$.
\end{lemma}
\begin{proof}
	Let us apply induction on $s$ to the predicate $P(s) = (app(s,t) = app(s,r) \to t = r)$. The base case is to infer $t = r$ from $app([\ ], t) = app([\ ], r)$, which is clear. For the inductive step, we assume (as the IH) that $app(s,t) = app(s,r)$ implies $t = r$ and try to prove $t = r$ from $app(x:s, t) = app(x:s, r)$. Simplifying the latter equation, we get $x : app(s,t) = x : app(s,t)$, whence $app(s,t) = app(s,r)$ by (A2). Applying the IH, obtain $t = r$.
\end{proof}
\begin{exc}
	Try to prove that from $app(t,s) = app(r,s)$, it follows that $t = r$. Probably, you will need to state and prove a few auxiliary lemmas to complete the task. Please read on for an easier but indirect approach.
\end{exc}

Another interesting string function is $rev$, which reverses a string, so that $rev([1,2,3]) = [3,2,1]$. Let us define it formally:
\begin{quote}
	$rev([\ ]) = [\ ]$;\\
	$rev(x : s) = app(rev(s), [x])$ for all $x$ and $s$.
\end{quote}
The function $rev$ relates to $app$ much like matrix transposition (or inversion) relates to matrix multiplication, where one has $(AB)^T = B^T A^T$ and $(AB)^{-1} = B^{-1} A^{-1}$ when everything is well-defined. E.\,g., $rev(app([1,2], [3,4])) = [4,3,2,1] = app(rev[3,4], rev[1,2])$. In fact, the same pattern might be seen in the usual rational number inversion and multiplication, as $\frac{1}{pq} = \frac{1}{q} \cdot \frac{1}{p}$ when $p, q \neq 0$. 

\begin{exc}\label{cs:rev_app_distr}
	Prove that $rev(app(s,t)) = app(rev(t), rev(s))$ for every $s, t \in S(A)$. Probably, you will need Lemmas~\ref{cs:app_nil_r} and~\ref{cs:app_assoc} at some stage.
\end{exc}

Clearly, $(p^{-1})^{-1}$ for both rationals and matrices, and $(A^T)^T$ holds for the latter as well. A similar statement is true for strings.
\begin{exc}\label{cs:rev_involutive}
	Prove that $rev(rev(s)) = s$ for each $s \in S(A)$.
\end{exc}

\begin{exc}
	Prove that $app(t,s) = app(r,s)$ implies $t = r$ for every $s, t, r \in S(A)$. Try to find a simple proof using Lemmas~\ref{cs:rev_involutive},~\ref{cs:rev_app_distr}, and~\ref{cs:app_inv_head}, yet no explicit induction.
\end{exc}

Many simple proofs employ inductive definitions but not induction itself.
\begin{lemma}\label{cs:app_eq_nil}
	For every $s,t \in S(A)$, if $app(s,t) = [\ ]$, then $s = [\ ]$ and $t = [\ ]$.
\end{lemma}
\begin{proof}
	According to the definition, each string is either $[\ ]$ or of the form $x : r$ for some $x, r$. Let us consider the two possible cases for the structure of $s$. (Such a procedure is known as a \emph{proof by cases}.)
	
	If $s = [\ ]$, then $[\ ] = app(s,t) = app([\ ], t) = t$, so $t = [\ ]$ as well. Assume now that $s = x : r$ for some $x$ and $r$. Then $[\ ] = app(x:r, t) = x : app(r, t)$, which is forbidden by (A1). Hence, this situation is impossible, we have got a contradiction (i.\,e., a false statement). Formally, from a falsity, it follows anything. In particular, we may infer $s = [\ ]$ and $t = [\ ]$.
	
	As each possible case results in $s = [\ ]$ and $t = [\ ]$, these statements do hold.
\end{proof}

\begin{exc}
	Prove that $lh(s) = 0$ is equivalent to  $s = [\ ]$ for any $s \in S(A)$.
\end{exc}

\section{Множества}
\mcomm{This section (besides its foundational and formalism-related contents) tries to introduce some `axiomatic thinking'. In this respect, it continues the line of the previous one (being totally independent thereof). In general, the axiomatic approach is painful for many students, since they are used to giving too much credit to their intuition and do not take axioms nor definitions seriously. In the case of sets, their intuition is most likely rigidly fixed to Euler diagrams etc., where one can clearly see two object types: `elements' and `sets'. This is, of course, inadequate for any interesting set-theoretic construction, like union or binary relation. As we want our presentation of `Discrete Mathematics' to be quite rigorous, this is unacceptable for us; therefore have we made this bitter `axiomatic pill' for our students. In practice, we however give it them \emph{after} more concrete and intuitive chapters on natural number induction and elementary arithmetic.
	\medskip\\
	Technically, we define set equality in terms of ${\in}$ with the Extensionality Axiom, give four set existence axioms (Pairing, Specification, Powerset, and Union) from ZF, and the Axiom of Foundation (as if we had the Axiom of Choice). We then use these to obtain all the `high school set theory' without much reference to intuition. In this Course, we neither give an explicit definition for the set $\N$ nor the Axiom of Infinity, nor we state any other axiom of ZFC. Throughout the Course, a few statements depend on the Axiom of Choice but we omit their proofs altogether.
}

Предположим, что у нас есть предикат $\phi(x)$ в предметной области $D$. Вполне естественно сгруппировать все $x$ из $D$, которые удовлетворяют $\phi$, и сформировать таким образом новый объект. Например, если $\phi(x)$ означает, что \emph{$x$ чётно}, а $D$ --- все натуральные числа, то таким объектом будут <<все чётные натуральные числа>>. Очевидно, что это <<часть>> предметной области $D$, выделенная свойством $\phi$.
%(также именуемая \emph{extension} предиката $\phi$)
Такая часть, которая может содержать много компонентов (например, чисел), но должна рассматриваться как единый объект, --- пример того, что в математике называют \emph{множеством}.

Интуитивно, \emph{множество} --- это набор (совокупность, коллекция) компонентов произвольной природы. Разумеется, это нельзя считать строгим определением, поскольку понятия \emph{набор} и \emph{компонент} не определены. Основная идея множества заключается в том, что это единообразный объект математики, обобщающий числа различных видов (целые, вещественные, комплексные и др.), функции, геометрические фигуры и многие другие понятия.

Ясно, что мы не можем определить столь базовое понятие через какое-либо другое. Однако мы можем \emph{не давать никакого определения}, сформулировав вместо этого некоторые правила, по которым <<ведут себя>> множества. Таким образом, \emph{множество} --- не более чем условное название фигуры в нашей игре, <<значение>> которого --- лишь роль этой фигуры в правилах игры. (Почти как \emph{слон} в шахматах не меняет своего <<значения>>, если мы назовём его \emph{офицером} или \emph{bishop --- епископом}.) В этом заключается суть \emph{аксиоматического метода}.

Однако какое же \emph{поведение} множеств мы определяем? В сущности, одно множество $A$ может \emph{быть элементом} другого множества $B$. Тогда мы говорим, что $A$ \emph{принадлежит} $B$, и пишем $A \in B$. Интуитивно, это означает, что <<компонент>> $A$ содержится в <<наборе>> $B$. Однако интуиция может быть обманчива: наши множества --- это лишь абстрактная \emph{модель} (математической или любой иной реальности), которая вправе идти вразрез с нашей интуицией.

Например, в нашей модели \emph{всё есть множество}, что плохо согласуется с интерпретацией <<компонент---совокупность>>, поскольку разницы между первым и вторым, в сущности, нет.

Таким образом, любой элемент любого множества сам является множеством и состоит из элементов, которые также являются множествами, и так далее. Мы увидим, что существует \emph{пустое множество}, которое не содержит элементов вовсе. Но, как бы то ни было, мы постулируем, что
\begin{quote}
	не существует бесконечных цепочек вида $x_0 \ni x_1 \ni \ldots \ni x_n \ni x_{n+1} \ni \ldots$.
\end{quote}
Утверждение, весьма близкое к данному (но всё же отличное от него) называется \emph{аксиомой основания}.

\mcomm{Clearly, the standard Axiom of Foundation implies this one. For the other direction, they usually apply the Axiom of (Dependent) Choice.}
\begin{exm}
	Утверждение $a \in a$ ложно для любого множества $a$. В противном случае существовала бы цепочка $a \ni a \ni a \ni\ldots$.
\end{exm}
\begin{exc}
	Докажите, что не существует таких множеств $a$ и $b$, что выполнено и $a \in b$, и $b \in a$.
\end{exc}

В многих разделах математики нас интересуют те или иные равенства. В нашей модели равенство множеств можно определить: $A = B$ тогда и только тогда, когда для любого множества $x$ выполнено: $x$ принадлежит $A$ тогда и только тогда, когда $x$ принадлежит $B$; иными словами, $A$ и $B$ состоят из одних и тех же элементов. Можно записать это в символьном виде:
$$A = B \iff \forall x\, (x \in A \iff x \in B).$$

Но что если $A = B$ и $B \in C$? Можно ли отсюда заключить, что $A \in C$? На самом деле нет. Поэтому мы постулируем это как \emph{аксиому равенства}:
\begin{quote}
	если $A = B$ и $B \in C$, то $A \in C$.
\end{quote}
Вместе с определением равенства = эта аксиома позволяет нам сказать, что равные множества ведут себя одинаково по обе стороны от знака ${\in}$. Таким образом, любое утверждение, определённое в терминах ${\in}$ (то есть любое утверждение в нашей модели), инвариантно относительно равенства =. Если $A = B$ и для $A$ что-то выполнено, то это же выполнено и для $B$. В символьной записи:
\begin{quote}
	для любого предиката $\phi$: если $A = B$, то $\phi(A) \iff \phi(B)$.
\end{quote}
Вполне естественно, не правда ли?

\mcomm{Clearly, one needs some form of induction over `predicates' (or, better, formulas) $\phi$ in order to prove this statement; some metatheory is thus needed. On the other hand, we can easily prove it for every \emph{particular} $\phi$ below. So, we have decided to believe it without a proof. }

Помимо равенства, существует естественное понятие сравнения множеств. А именно, мы говорим, что множество $A$ \emph{включено} в множество $B$ тогда и только тогда, когда для любого множества $x$, если $x$ принадлежит $A$, то $x$ принадлежит $B$; то есть любой элемент $A$ является элементом $B$. В символах:
$$A \sbs B \iff \forall x\, (x \in A \ply x \in B).$$
В данном случае мы можем также сказать, что $A$ --- \emph{подмножество} $B$, а $B$ --- \emph{надмножество} $A$. Если $A \sbs B$, но $A \neq B$, то множество $A$ называется \emph{строгим} подмножеством $B$.

\begin{lemma}\label{ch0:subset}
	Для любых множеств $A$, $B$ и $C$
	\begin{enumerate}
		\item $A \sbs A$;
		\item если $A \sbs B$ и $B \sbs C$, то $A \sbs C$;
		\item $A = B$ тогда и только тогда, когда $A \sbs B$ и $B \sbs A$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Докажем второй пункт. Необходимо показать, что $x \in C$ для любого $x \in A$. Рассмотрим произвольное множество $x$ и положим, что $x \in A$. Тогда $x \in B$, так как $A \sbs B$. Также, так как $B \sbs C$, то $x \in C$. Поскольку для \emph{произвольного} $x$ из $x \in A$ следует $x \in C$, это верно для \emph{всех} $x$.
\end{proof}

\begin{corr}\label{ch0:id}
	Для любых множеств $A$, $B$ и $C$
	\begin{enumerate}
		\item $A = A$;
		\item если $A = B$ и $B = C$, то $A = C$;
		\item если $A = B$, то $B = A$.
	\end{enumerate}
\end{corr}

А существуют ли вообще множества? Очевидно, что если бы их не существовало, то наша модель была бы бессмысленной. Обойдёмся без формального обоснования и просто примем, что существуют известные множества $\N = \{0,1,2,\ldots \}$ натуральных, $\Z$ целых, $\Q$ рациональных и $\R$ вещественных чисел. Разумеется, их элементы сами должны быть множествами. Так, $1$ и $2$ суть некоторые множества. \emph{Мы опускаем их определения}, хотя и принимаем некоторые элементарные свойства, например $1 \neq 2$. Иными словами, мы \emph{избегаем} использования внутренней структуры натуральных чисел (например, верно ли, что $1 \in 2$, или нет), но считаем известным, что $1 \neq 2$, $2 < 3$, $2 + 3 = 5$ и т.\,д.

\mcomm{I usually tell the students that from the standard definition of $\N$ it follows that $0 = \void$ and $n + 1 = \{0,1,\ldots,n\}$ for each $n$; then I ask them not to use these equations in their proofs since we currently have no means to make this \emph{recursive} `definition' rigorous. 
	\medskip\\
	When solving class problems, the Instructor should warn the students against making their examples dependent on whether $3 \neq \{ 4 \}$ etc. They had better look for \emph{simpler} examples based on sets whose elements are known for sure, like $\void$, $\{ \void \}$, etc.}

Если мы имеем какие-то множества (например, $\N$ и $\Z$), можем ли мы определить какие-то другие множества? В самом деле, важнейшая задача нашей модели --- обеспечить безопасные возможности сделать это. (Здесь \emph{безопасные} означает логически непротиворечивые; объяснение этому последует далее.)

\paragraph{``Из нескольких --- одно.''} Пусть $a_1,\ldots,a_n$ --- некоторые множества. Тогда существует такое множество $\{a_1,\ldots,a_n \}$, что
$$x \in \{a_1,\ldots,a_n\} \iff x = a_1 \vee x = a_2 \vee \ldots\ \vee x = a_n$$
для любого множества $x$.

\mcomm{Of course, there is no \emph{logical} necessity in turning the Axiom of Pairing into a schema for various $n \in \N$ as $\{a, b, c\} = \cup \{ \{a, b\}, \{ c, c \} \}$. Yet this observation uses the union essentially; so we have here preferred a freshman's comfort to logical elegance.}
\begin{exm}
	Мы можем определить множества $\{\N, \Z,\Q,\R\}$ и $\{ \N \}$. Очевидно, $\N \in \{\N\}$. Однако $\N \not \sbs \{\N\}$\footnote{Записи $A \not\sbs B$ и $A \notin B$ означают $\neg (A \sbs B)$ и $\neg (A \in B)$ соответственно.}, так как в противном случае из $0 \in \N$ следовало бы $0 \in \{\N\}$, откуда $0 = \N$ и $0 \in 0$. Последнее невозможно в силу аксиомы основания. С другой стороны, $\N \sbs \N$, но $\N \notin \N$ (опять же по аксиоме основания).
\end{exm}

Для любого множества $A$ рассмотрим множество $\{A\}$, которое носит название \emph{синглетон} множества $A$. По определению имеем
$$x \in \{ A \} \iff x = A$$
для любого $x$. То есть $A$ --- единственный элемент $\{ A \}$. Разница между $A$ и $\{A\}$ не так легко уловима нашей интуицией, поскольку мы обычно не отличаем группу из одного элемента от самого этого элемента. (Задумайтесь о точке на плоскости и о геометрической фигуре, образованной этой единственной точкой.) Однако различие это крайне важно для теоретико-множественной модели.

\begin{exm}
	$\{2,2,3\} = \{3,2\}$. В самом деле, 
	$$x \in \{2,2,3\} \iff x = 2 \vee x = 2 \vee x = 3 \iff x = 3 \vee x = 2 \iff x \in \{3,2\}.$$
	Таким образом, ни порядок элементов, ни количество вхождений каждого из них не охватываются понятием множества. Далее мы увидим, что есть способы выразить эти идеи косвенно.
\end{exm}

\paragraph{Выделение подмножества.} Вспомним, что мы пишем $\phi(x)$, если $x$ удовлетворяет свойству (предикату) $\phi$. Например, $\mbox{чётно}(2)$ означает, что число $2$ удовлетворяет свойству быть чётным, то есть $2$ чётно. Пусть $A$ --- множество, а $\phi$ --- некоторое свойство. Тогда существует множество $\{ x \in A \mid \phi(x) \}$ такое, что
$$y \in \{ x \in A \mid \phi(x) \} \iff y \in A \wedge \phi(y)$$
для любого $y$. Другими словами, мы берём и объединяем все элементы $A$, удовлетворяющие $\phi$.  Очевидно, что $\{ x \in A \mid \phi(x) \} \sbs A$.

Записи вида $\{x \in A \mid \phi(x)\}$ известны под названием \emph{set-builder notation}.

\begin{exm}
	Множество
	$\{ x \in \N \mid x\ \mbox{чётно} \}$
	есть множество всех чётных натуральных чисел.
\end{exm}

Но что если $\phi$ --- свойство, которым не обладает ни одно множество? А именно, рассмотрим множество
$$\void = \{ x \in \N \mid x \neq x \}.$$
По следствию~\ref{ch0:id} $x = x$ для любого $x$. По определению $x \neq x$ для всякого $x \in \void$. Значит, \emph{не существует такого $x$, что $x \in \void$}. Множество $E$ называется \emph{пустым}, если $x \notin E$ для любого $x$. Тогда множество $\void$ пустое. Существует ли какое-нибудь другое пустое множество?
\begin{lemma}\label{ch0:empty}\rule{1pt}{0pt}
	\begin{enumerate}
		\item Если $E$ --- пустое множество, то $E \sbs A$ для любого множества $A$.
		\item Если множества $E_1$ и $E_2$ пустые, то $E_1 = E_2$.
	\end{enumerate}
\end{lemma} 
\begin{proof}
	Второе утверждение следует из первого по лемме~\ref{ch0:subset}. Докажем первое утверждение. Для любого $x$ имеем $x \notin E$. Тогда если мы предположим, что $x \in E$, то мы непременно получим ложь, противоречие. Согласно законам логики, \emph{из лжи следует всё что угодно}. Поэтому мы имеем полное право заключить, что $x \in A$ (равно как и $1 = 2$ и т.\,д.), когда $x \in E$.
\end{proof}
\begin{corr}
	Множество $\void$ --- единственное пустое множество.
\end{corr}

\paragraph{Множества и предикаты.}
Как мы только что увидели, принцип выделения подмножества позволяет определить множество всех элементов некоторого данного множества $A$, которые удовлетворяют предикату $\phi$. В сущности, это одна из важнейших идей теории множеств. Зачастую в математических высказываниях мы будем заменять предикаты множествами.

Например, представьте себе, что мы хотим формализовать высказывание <<каждое чётное число равно сумме двух нечётных чисел>>. Отталкиваясь от формализации $$\forall x\, \bigl(x\ \mbox{чётно}\ \to \exists y \exists z\, (y\ \mbox{нечётно} \wedge z\ \mbox{нечётно}\ \wedge x = y + z)\bigr),$$
мы можем преобразовать её в
$$\forall x\, \bigl(x \in A \to \exists y \exists z\, (y \in B \wedge z \in B \wedge x = y + z)\bigr),$$
где $A$ и $B$ --- множества чётных и нечётных чисел соответственно. Обратите внимание на то, как <<соответствие предикату>> было здесь заменено на <<принадлежность множеству>>. Обычно в таком случае используют более компактную запись (с помощью так называемых \emph{ограниченных кванторов}):
$$\forall x \in A\, \exists y \in B\,  \exists z \in B\, (x = y + z)$$
(читать как <<для любого $x$ из $A$ существуют $y$ и $z$ из $B$ такие, что \dots>> --- что звучит вполне понятно и естественно, не так ли?).

Ограниченные кванторы имеют много разновидностей ($\forall x \in A$, $\forall x < 10$, $\exists x > 10$ и др.), но важно понимать, что \emph{ограниченный квантор всеобщности} подразумевает \emph{импликацию}, а \emph{ограниченный квантор существования} подразумевает \emph{конъюнкцию}. Формально,
$$\forall x \in A\ \phi \equiv \forall x\, (x \in A \to \phi)\qquad\mbox{и}\qquad\exists x \in A\ \phi \equiv \exists x\, (x \in A \wedge \phi).$$
\begin{exc}
	Докажите, что $\forall x \in A\ \phi$ истинно, а $\exists x \in A\ \phi$ ложно для $A = \void$ и любого утверждения $\phi$.
\end{exc}
\begin{exc}
	Докажите, что $\neg\forall x \in A\ \phi \equiv \exists x\in A\ \neg\phi$ и $\neg\exists x \in A\ \phi \equiv \forall x\in A\ \neg\phi$ для любых множества $A$ и утверждения $\phi$.
\end{exc}

\paragraph{Парадокс Рассела.} Очевидно, с помощью $\{x \in A \mid \phi(x) \}$ мы можем выделять лишь подмножества данного множества $A$. Но не могли бы мы объединить \emph{все} возможные $x$ (то есть не только из $A$) такие, что $x$ удовлетворяет $\phi$? Исторически, именно такая идея изначально стояла за понятием множества: любой набор объектов, имеющих что-то общее или удовлетворяющих какому-то общему свойству.

Как мы сейчас увидим, эта интуитивная идея оказывается логически противоречивой.

\begin{lemma}[<<Парадокс Рассела>>]
	Не существует такого множества $R$, что
	$$\forall x\, (x \in R \iff x \notin x).$$
\end{lemma}
\begin{proof}
	Допустим, что такое множество $R$ существует. Для начала предположим $R \notin R$. По определению $R$ из этого следует $R \in R$. В силу полученного противоречия неверно, что $R \notin R$, то есть мы доказали, что $R \in R$ (допустив существование $R$). Но в этом случае $R \notin R$ всё по тому же определению $R$. Таким образом, предположение о существовании $R$ приводит к противоречию. Значит, такого множества не существует.
\end{proof}

Таким образом, мы не можем определить множество $\{x \mid x \notin x \}$ как содержащее \emph{все} множества $x$, которые удовлетворяют вполне естественному условию $x \notin x$. Именно этот факт был сочтён парадоксальным и противоречащим интуиции.

\mcomm{In view of the Axiom of Foundation, $R$ is \emph{the set of all sets}, which does not thus exist. The Instructor might ask the students to prove the latter statement \emph{without} that axiom and might then explain why proofs from \emph{weaker} assumptions (even the ``axioms'') are usually worth looking for.}
\begin{exc}
	Не существует такого множества $V$, что
	$\forall x\, (x \in V).$
	Другими словами, не существует множества всех множеств. Постарайтесь не использовать аксиому основания.
\end{exc}
%\begin{proof}
%This is immediate by the Axiom of Foundation. Indeed, $x \notin x$ for any $x$ thereby. One the other hand, $V \in V$ must hold. Hence a contradiction.
%
%Moreover, this fact does not depend on the Axiom of Foundation. Again, suppose that such $V$ exists. Now specify a subset $R' = \{ x \in V \mid x \notin x \}$ thereof. Clearly, $R'$ is \emph{some} set, so $R' \in V$. If $R' \notin R'$, then $R' \in R'$ by the definition of $R'$. A contradiction. Thus, $R' \in R'$; hence, $R' \notin R'$ by the same definition. Finally, we see that such a set $V$ does not exist.
%\end{proof}

%\paragraph{Vacuous truth.} We have already used the symbol $\forall x$, which reads ``for any set $x$'' and is known as a \emph{universal quantifier}. This has a natural counterpart $\exists x$, which reads ``there exists some set $x$'' and is called an \emph{existential quantifier}.
%
%Usually, we are interested in so called \emph{bounded quantifiers} whose ``domain'' is explicitly restricted. For example, the equation $2x + 5 = 1$ has no \emph{natural} solutions but has an \emph{integer} solution $-2$.
%So, the statement $\exists x \in \Z\, (2x + 5 = 1)$ does hold, while $\exists x \in \N\, (2x + 5 = 1)$ does not.
%
%In general, $$\exists x \in A\ \phi(x) \iff \exists x\, (x \in A\ \mbox{and}\ \phi(x))$$
%and
%$$\forall x \in A\  \phi(x) \iff \forall x\, (\mbox{if}\ x \in A,\ \mbox{then}\ \phi(x)).$$
%What does that mean for an empty $A$? Clearly, from $\exists x \in \void\ \phi(x)$, it follows that $\exists x\, (x \in \void)$, which is not possible. So the statement $\exists x \in \void\ \phi(x)$ is false regardless of $\phi$.
%
%It is not so intuitive, but the statement $\forall x \in \void\ \phi(x)$ is true for any $\phi$. Indeed, this means that \emph{if $x \in \void$, then $x$ satisfies $\phi$} for any $x$. But the assumption $x \in \void$ is false for any $x$, so the whole implication is true (from a falsity it follows everything, as we have already seen).
%
%Such a true statement is called a \emph{vacuous truth}. These are omnipresent in mathematics. Here is a classical ``paradoxical'' illustration:
%\begin{quote}
%The present King of France is bald.
%\end{quote}
%If we interpret the statement this way: 
%\begin{quote}
%Anyone who is the present King of France is bald,
%\end{quote}
%we get a typical vacuous truth. Needless to say, it is no less true that \emph{the present King of France is hairy}. 
%
%Vacuous truths are closely connected with Lemma~\ref{ch0:empty}. The latter states that $\void \sbs A$ for any set $A$. This can be rephrased as $\forall x \in \void\, (x \in A)$, which is a vacuous truth.

\paragraph{Множество всех подмножеств.} Пусть $A$ --- некоторое множество. Тогда существует множество $\mP(A)$ такое, что
$$x \in \mP(A) \iff x \sbs A$$
для любого $x$. Другими словами, $\mP(A)$ --- множество всех \emph{подмножеств} множества $A$.

Поскольку $\void \sbs A$, $\void \in \mP(A)$ для любого множества $A$. Отметим, что $\mP(\void) = \{\void\}$, $\mP(1) = \{\void, \{1\} \}$ и $\mP(\{1,2\}) = \{\void, \{1\}, \{2\}, \{1,2\}\}$. Легко видеть, что $\{1,2\}$ состоит из $2$ элементов, а $\mP(\{1,2\})$ состоит из $4 = 2^2$ элементов; $\mP(\void)$ состоит из $1 = 2^0$ элементов; $\mP(\{1\})$ состоит из $2 = 2^1$ элементов. Такая аналогия между множествами всех подмножеств и степенями $2$ --- не совпадение.

\mcomm{Clearly, every formal proof that the set $\{1,2\}$ has just those subsets as specified above boils down to an exhaustive search argument (for one must at least write all the subsets down). It might be beneficial to draw the tree of all subsets for a small size example, yet we skip such formal proofs in general.}

\begin{exm}
	Если $\mP(X) = \mP(Y)$, то $X  = Y$, и наоборот.
	
	В самом деле, допустим $\mP(X) = \mP(Y)$. Так как $X \sbs X$, то $X \in \mP(X)$. По предположению $X \in \mP(Y)$, откуда $X \sbs Y$. В силу аналогичных рассуждений, $Y \sbs X$. Тогда получаем $X = Y$ по лемме~\ref{ch0:subset}.
	
	Теперь допустим $X = Y$. Рассмотрим произвольный $a \in \mP(X)$. Тогда $a \sbs X = Y$, откуда $a \sbs Y$ по лемме~\ref{ch0:subset}, откуда $a \in \mP(Y)$. Тогда $\mP(X) \sbs \mP(Y)$. Обратное включение аналогично.
\end{exm}

\paragraph{Объединение.} Пусть $A$ --- некоторое множество. Тогда существует множество $\cup A$ такое, что
$$x \in \cup A \iff \exists \alpha\ (x \in \alpha \wedge \alpha \in A)$$
для любого $x$. Таким образом, $\cup A$ есть множество всех элементов элементов множества $A$.

\mcomm{This concept is most likely unknown for most students; so they may need some time to grasp it. In our practice, drawing (potentially) infinite families of planar figures and highlighting their unions with color prove beneficial.
	\medskip\\
	Moreover, depending on the audience, it might be better to replace this general principle by a weaker version:
	\emph{For every sets $A$ and $B$, there exists a set $A \cup B$ such that
		$x \in A \cup B \iff x \in A \vee x \in B$ for all $x$}. This version is sufficient for most of this Course's developments. Countable unions pose a noticeable exception; they are yet neither frequent, nor crucial, nor hard to explain as most students will consider them ``natural'' and ``logical''.}

\begin{exm}\label{sets:fin_union}
	$\cup \void = \void$. В самом деле, если $x \in \cup \void$, то $x \in \alpha \in \void$ для некоторого множества $\alpha$; однако $\alpha \in \void$ невозможно, откуда получаем противоречие; таким образом, ни один $x$ не может быть элементом $\cup \void$; тогда $\cup \void$ пустое. По лемме~\ref{ch0:empty} $\void$ --- единственное пустое множество.
	
	Для любого $A$: $\cup \{ A \} = A$. Понятно, что $x \in \cup \{ A\} \iff \exists \alpha\ x \in \alpha \in \{ A \}$, а $\alpha \in \{A \}$ означает, что $\alpha = A$. Тогда $x \in \cup \{ A\} \iff \exists \alpha\ x \in \alpha = A \iff x \in A$. (Для последней эквивалентности можно положить $\alpha = A$.)
	
	Если $A = \{ \{1,2,3\}, \{1\}, \{2,4\}\}$, то для любого $x$
	$$
	\begin{array}{rcl}
		x \in \cup A &\iff& \exists \alpha\ (x \in \alpha \wedge \alpha \in A)\\
		&\iff& \exists \alpha\ (x \in \alpha \wedge (\alpha = \{1,2,3\} \vee \alpha = \{1\} \vee \alpha = \{2,4\}))\\
		&\iff& \exists \alpha\ ((x \in \alpha \wedge \alpha = \{1,2,3\}) \vee (x \in \alpha \wedge \alpha = \{1\}) \vee (x \in \alpha \wedge \alpha = \{2,4\}))\\
		&\iff& \exists \alpha\ (x \in \alpha \wedge \alpha = \{1,2,3\}) \vee \exists \alpha\ (x \in \alpha \wedge \alpha = \{1\}) \vee \exists \alpha\ (x \in \alpha \wedge \alpha = \{2,4\})\\
		&\iff& x \in \{1,2,3\}  \vee   x \in \{1\} \vee x \in \{2,4\} \\
		&\iff& x = 1 \vee x = 2 \vee x = 3 \vee x = 1 \vee x = 2 \vee x = 4\\
		&\iff& x = 1 \vee x = 2 \vee x = 3 \vee x = 4\\
		&\iff& x \in \{1,2,3,4\}.\\
	\end{array}
	$$
	Таким образом, $\cup A = \{1,2,3,4\}$.
	
	\mcomm{For the above argument, the Instructor might wish to explain a few logical equivalences: $\exists x\, (A \vee B) \equiv \exists x\, A \vee \exists x\, B$ and $\exists x\, (A(x) \wedge x = t) \equiv A(t)$.}
\end{exm}

\begin{exm}
	Пусть $S$ --- множество всех отрезков длины $1$ на прямой $\R$. (Вспомним, что \emph{отрезок}~--- множество вида $[a,b] = \{ x \in \R \mid a \leq x \leq b \}$ для некоторых чисел $a, b \in \R$. Очевидно, $[a,b] = \void$, если $b < a$. Пусть $a \leq b$. Тогда число $b - a \geq 0$ называется \emph{длиной} отрезка $[a,b]$. Понятно, что длина $[a,b]$ равна $1$ тогда и только тогда, когда $b = a + 1$.)
	
	Ясно, что $S = \{ X \in \mP(\R) \mid \exists a \in \R\ X = [a, a + 1] \}$. Менее строго множество $S$ может быть также записано как $\{ [a,a + 1] \mid a \in \R \}$. Что представляет собою множество $\cup S$? Покажем, что $\cup S = \R$.
	
	Если $x \in \cup S$, то $x \in X \in S$ для некоторого $X$, или, эквивалентно, $x \in [a, a + 1]$ для некоторого $a \in \R$. По определению отрезка получаем $x \in \R$.
	
	Для доказательства в другую сторону рассмотрим $x \in \R$. Существует отрезок из $S$, покрывающий (то есть содержащий) $x$ --- фактически, таких отрезков бесконечно много. В самом деле, $x \leq x \leq x + 1$,  $x - \frac{1}{2} \leq x \leq x + \frac{1}{2}$ и так далее. Таким образом, $x \in [x, x + 1] \in S$, откуда $x \in \cup S$.
\end{exm}

\begin{exm}
	Если $X \sbs Y$, то $\cup X \sbs \cup Y$.
	
	Рассмотрим произвольный $a \in \cup X$. По определению имеем $a \in A$ для некоторого $A \in X$. Из нашего условия следует, что $A \in Y$. Тогда $\exists a\ a \in A \in Y$, что означает $a \in \cup Y$. Таким образом, мы получили, что из $a \in \cup X$ следует $a \in \cup Y$ для \emph{любого} $a$, то есть $\cup X \sbs \cup Y$.
\end{exm}

\paragraph{Алгебра множеств.} Для произвольных множеств $A$ и $B$ обозначим $A \cup B$ множество $\cup\{A, B\}$, которое называется \emph{объединением множеств $A$ и $B$}. Рассуждая аналогично примеру~\ref{sets:fin_union}, легко доказать, что
$$x \in A \cup B \iff x \in A \vee x \in B$$
для любого $x$. В самом деле,
$$
\begin{array}{rcl}
	x \in A \cup B &\iff& x \in \cup\{ A, B \}\\
	&\iff&  \exists \alpha\ (x \in \alpha \wedge \alpha \in \{A, B\})\\
	&\iff& \exists \alpha\ (x \in \alpha \wedge (\alpha = A \vee \alpha = B))\\
	&\iff& \exists \alpha\ ((x \in \alpha \wedge \alpha = A) \vee (x \in \alpha \wedge \alpha = B) )\\
	&\iff& \exists \alpha\ (x \in \alpha \wedge \alpha = A) \vee \exists \alpha\ (x \in \alpha \wedge \alpha = B)\\
	&\iff& x \in A  \vee   x \in B.\\
\end{array}
$$
\noindent Также определим \emph{пересечение}
$$A \cap B = \{ x\in A \mid x \in B\}$$
и \emph{разность}
$$A \setminus B = \{x \in A \mid x \notin B\}$$
множеств $A$ и $B$. Очевидно,
$$x \in A \cap B \iff x \in A \wedge x \in B$$
для любого $x$. Говорят, что множество $A$ \emph{пересекается} с $B$, если $A \cap B \neq \void$. В противном случае множества называются \emph{непересекающимися}.

% Пришлось переместить эту картинку, чтобы определение не разбивалось ею на две части.
\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{union.pdf}
	\caption{Объединение на плоскости. Закрашенная область $\cup C$, представляющая собой множество \emph{точек} плоскости, является объединением множества \emph{кругов} $C$. В частности, $C$ конечно, в то время как $\cup C$ бесконечно.}
\end{figure}

\begin{lemma}\label{ch0:l3}
	Для любых множеств $A$ и $B$ имеем $A \cap B \sbs X \sbs A \cup B$, если $X \in \{A,B\}$. Также $A \setminus B \sbs A$ и $(A \setminus B) \cap B = \void$.
\end{lemma}
\begin{lemma}\label{ch0:lattice}
	Для любых множеств $A$ и $B$ следующие условия эквивалентны:
	\begin{enumerate}
		\item $A \sbs B$;
		\item $A \cap B = A$;
		\item $A \cup B = B$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Достаточно показать, что первое условие влечёт второе, второе влечёт третье, а третье влечёт первое.
	
	Допустим $A \sbs B$. По лемме~\ref{ch0:l3} имеем $A \cap B \sbs A$. Проверим, что $A \sbs A \cap B$. Рассмотрим произвольный $x \in A$. Тогда $x \in B$, так как $A \sbs B$. Отсюда $x \in A \cap B$. По лемме~\ref{ch0:subset} из $A \cap B \sbs A$ и $A \sbs A \cap B$ следует, что $A \cap B = A$.
	
	Теперь допустим $A \cap B = A$. По лемме~\ref{ch0:l3} $B \sbs A \cup B$. Осталось доказать, что $A \cup B \sbs B$. Если $x \in A \cup B$, то $x \in A$ или $x \in B$. В первом случае получим $x \in A \cap B$ из $A = A \cap B$, откуда $x \in B$. Во втором случае сразу же получим $x \in B$.
	
	Наконец, допустим $A \cup B = B$. По лемме~\ref{ch0:l3} имеем $A \sbs A \cup B$. По предположению получаем $A \cup B \sbs B$, откуда $A \sbs B$.
\end{proof}

Иногда все множества, которые мы рассматриваем, являются подмножествами некоторого множества $U$, которое называется \emph{универсальным} (для данного конкретного случая). Если универсальное множество $U$ известно (или понятно из контекста), то для любого $A \sbs U$ мы можем определить множество
$$\bar A = U \setminus A,$$
которое называется \emph{дополнением} $A$ (до $U$). Очевидно, $A \setminus B = A \cap \bar B$ для любых $A, B \sbs U$.

\mcomm{Some students mistake this `universe' for ``the set of all sets''. The Instructor should discourage this misconception.}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{set_algebra.pdf}
	\caption{\emph{Диаграммы Эйлера} для операций алгебры множеств. Множества отождествляются с фигурами на плоскости, а их элементы --- с точками плоскости. Как это ни наглядно, никакие диаграммы \emph{не являются доказательством} чего бы то ни было.}
\end{figure}

\begin{thm}[Тождества алгебры множеств]\label{ch0:boolean} Для любых множеств $U$ и $A, B, C \sbs U$
	\begin{enumerate}
		\item $A \cap B = B \cap A$; $A \cup B = B \cup A$;
		\item $(A \cap B) \cap C = A \cap (B \cap C)$; $(A \cup B) \cup C = A \cup (B \cup C)$;
		\item $A \cap A = A$; $A \cup A = A$;
		\item $A \cap (A \cup B) = A$; $A \cup (A \cap B) = A$;
		\item $\bar{\bar A} = A$;
		\item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$; $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$;
		\item $\overline{A \cap B} = \bar A \cup \bar B$; $\overline{A \cup B} = \bar A \cap \bar B$;
		\item $A \cap \void = \void$; $A \cup \void = A$; $A \cap U = A$; $A \cup U = U$; $\bar \void = U$; $\bar U = \void$;
		\item $A \cap \bar A = \void$; $A \cup \bar A = U$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Доказательство этой теоремы представляет собою переписывание логических эквивалентностей для конъюнкции, дизъюнкции и отрицания, которые следуют из соответствующих таблиц истинности.
	
	Проверим, например, что $\overline{A \cap B} = \bar A \cup \bar B$. Для любого $x$ имеем
	\begin{multline*}
		x \in \overline{A \cap B} \iff x \in U \wedge \neg (x \in A \cap B) \iff x \in U \wedge \neg (x \in A \wedge x \in B) \iff\\
		x \in U \wedge (\neg x \in A \vee \neg x \in B) \iff (x \in U \wedge \neg x \in A) \vee (x \in U \wedge \neg x \in B) \iff\\
		x \in \bar A \vee x \in \bar B \iff x \in \bar A \cup \bar B.
	\end{multline*}
	
	%The proof for the theorem is just a rephrasing of natural properties we expect from logical connectives \emph{and}, \emph{or}, \emph{not}, which we have already codified in the respective truth tables.
	%
	%Let us check, say, whether $\overline{A \cap B} = \bar A \cup \bar B$. If $x \in \overline{A \cap B}$, then $x \in U$ but not $x \in A \cap B$. So, it is not the case that $x \in A$ and $x \in B$. Hence, at least one of the following does not hold: $x \in A$ or $x \in B$. We get $x \in \bar A$ or $x \in \bar B$, that is, $x \in \bar A \cup \bar B$. Thus, $\overline{A \cap B} \sbs \bar A \cup \bar B$.
	%
	%Let us show that $\bar A \cup \bar B \sbs \overline{A \cap B}$. Suppose that $x \in \bar A \cup \bar B$; then $x \notin A$ or $x \notin B$. Assume $x \in A \cap B$. If $x \notin A$, we get $x \in A\cap B \sbs A$, whence a contradiction. If $x \notin B$, we get a contradiction in a similar way. Thus, our assumption is false, that is, $x \notin A \cap B$ and $x \in \overline{A \cap B}$.
\end{proof}

\begin{rem}
	Так как $(A \cap B) \cap C = A \cap (B \cap C)$, мы можем опускать скобки в выражениях вида $A_1 \cap A_2 \cap \ldots \cap A_n$. Разумеется, это же верно и для объединения.
\end{rem}

При рассмотрении тождеств алгебры множеств дополнения зачастую используются без всяких объяснений. Например, в нижеследующем примере можно положить $U = \cup\{A, B, C\}$ для \emph{любого} выбора множеств $A, B, C$.

\begin{exm}
	Покажем, что $A \setminus (B \setminus C) = (A \setminus B) \cup (A \cap C)$, используя уже известные тождества. В самом деле,
	\begin{multline*}
		A \setminus (B \setminus C) = A \cap \overline{B \cap \bar C} = A \cap (\bar B \cup \bar{\bar C}) =\\
		A \cap (\bar B \cup C) = (A \cap \bar B) \cup (A \cap C) = (A \setminus B) \cup (A \cap C).
	\end{multline*}
\end{exm}

\begin{exm}\label{L2:cup_sup}
	Для любых множеств $A, B, C$, если $A \sbs C$ и $B \sbs C$, то $A \cup B \sbs C$.
	
	По предположению и лемме~\ref{ch0:lattice}, получим $C = A \cup C$ и $C = B \cup C$. Теперь подставим $B \cup C$ вместо второго $C$ в первом равенстве: $C = A \cup (B \cup C)$. Но $A \cup (B \cup C) = (A \cup B) \cup C$ по теореме~\ref{ch0:boolean}, откуда $C = (A \cup B) \cup C$. Тогда получаем $A \cup B \sbs C$ в силу леммы~\ref{ch0:lattice}.
\end{exm}

\begin{exc}
	Докажите, что если $C \sbs A$ и $C \sbs B$, то $C \sbs A \cap B$.
\end{exc}

\paragraph{Декартово произведение.} Многие разделы математики оперируют такими понятиями, как упорядоченные <<пары>>, <<тройки>> и т.\,д. Например, точки плоскости обычно отождествляют с их координатами, порядок которых имеет значение. В самом деле, $(0,1)$ и $(1,0)$ суть \emph{различные} точки. Несмотря на то, что множества сами по себе не отражают никакого <<порядка элементов>> (например, $\{2,3\} = \{3,2\}$), упорядоченные пары легко смоделировать с помощью множеств.

Для произвольных множеств $a$ и $b$ рассмотрим множество
$$(a,b) = \{\{a\}, \{a, b\}\},$$
которое называется \emph{(упорядоченной) парой} множеств $a$ и $b$. Ключевое свойство таких пар есть следующее:
\begin{lemma}\label{ch0:pair}
	Для любых множеств $a, b, c, d$, 
	$$(a, b) = (c, d) \iff a = c \wedge b = d.$$
\end{lemma}
\begin{proof}
	Предположим, что $\{\{a\}, \{a, b\}\} = \{\{c\}, \{c, d\}\}$. Тогда $\{a\} \in \{\{c\}, \{c, d\}\}$, то есть $\{a\} = \{c\}$ или $\{a\} = \{c, d\}$. В первом случае $a \in \{c\}$, откуда $a = c$. Во втором случае $c \in \{a\}$ и опять $c = a$. Таким образом, $a = c$.
	
	Из предположения также следует, что $\{a, b\} = \{c\}$ или $\{a, b\} = \{c, d\}$. 
	
	В первом случае $b \in \{c\}$, откуда $b = c = a$. По предположению $\{c,d\} = \{a\}$ или $\{c,d\} = \{a,b\}$. Тогда $d = a$ или $d = b$. В любом случае $b = d$.
	
	Теперь рассмотрим случай $\{a, b\} = \{c, d\}$. Если $d = b$, доказывать нечего. В противном случае $d = a = c$, то есть $\{a, b\} = \{d\}$, откуда $b = d$.
	
	Для импликации в обратную сторону достаточно вспомнить, что равные множества полностью взаимозаменяемы; таким образом, из $(a, b) = (a, b)$, $a = c$ и $b = d$ следует, что $(a, b) = (c, d)$.
\end{proof}
\begin{corr}
	Для любых множеств $a$ и $b$: $(a, b) = (b, a)$ тогда и только тогда, когда $a = b$.
\end{corr}

\begin{rem}\label{L4:pair_collect}
	Если $a, b \in X$, то $(a, b) \in \mP (\mP (X))$.
	
	В самом деле, $\{a\}$ и $\{a,b\}$ являются подмножествами $X$ и, следовательно, элементами $\mP(X)$. Тогда $(a,b) = \{ \{a\}, \{ a,b\} \} \sbs \mP(X)$.
\end{rem}
\noindent\emph{Декартовым произведением} множеств $A$ и $B$ называется множество
$$A \times B = \{z \in \mP (\mP (A \cup B)) \mid \exists a \in A\, \exists b \in B\ z = (a, b) \}.$$
Существование такого множества гарантируется принципом выделения подмножества. Этот принцип требует указания множества $\mP (\mP (A \cup B))$ (откуда берутся $z$) в вышеприведённой формуле. Однако мы можем обойтись и без него: для любого множества $z$
$$z \in A \times B \iff \exists a \in A\, \exists b \in B\ z = (a, b).$$
В самом деле, если верна правая часть, то мы имеем $a, b \in A \cup B$ и $(a,b) \in \mP (\mP (A \cup B))$ по замечанию~\ref{L4:pair_collect}. Менее строгая, но более простая запись декартова произведения
$$A \times B = \{(a, b) \mid a \in A,\ b \in B\}$$
вдохновлена именно такими соображениями.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{cart_prod.pdf}
	\caption{Декартово произведение в пространстве. Множество $A \times (B_1 \cup B_2)$ закрашено.}
\end{figure}

\begin{exm}
	Очевидно, $A \times \void = \void$ для любого $A$. В самом деле, если $z \in A \times \void$, то $z = (a,b)$ и $b \in \void$ было бы выполнено для некоторых множеств $a, b$. Однако последнее неверно. Значит, $A \times \void$ пусто.
\end{exm}

\begin{exm}
	Поймём, почему $A \times B$ не обязательно равно $B \times A$. В самом деле, для $A = \{ x \}$ и $B = \{ y \}$ имеем $A \times B = \{(x,y)\}$ и $B \times A = \{(y,x)\}$. Положим $x = \void$ и $y = \{ \void \}$. Так как $\void \in y$, то $y$ непусто и $y \neq x$. Тогда $(x,y) \neq (y,x)$ по лемме~\ref{ch0:pair}, откуда $A \times B \neq B \times A$.
\end{exm}

Несмотря на то, что в общем случае $A \times (B \times C) \neq (A \times B) \times C$, мы можем опускать скобки в произведениях, руководствуясь правилом:
$$A_1 \times A_2 \times A_3 \times \ldots \times A_{n-1} \times A_n = (\ldots ((A_1 \times A_2) \times A_3) \times \ldots \times A_{n-1}) \times A_n.$$


\begin{rem}\label{ch0:pair_uniq}
	Если $(x,y) \in A \times B$, то $x \in A$ и $y \in B$ (и наоборот).
	
	В самом деле, пусть $(x,y) \in A \times B$. По определению произведения существуют $a \in A$ и $b \in B$ такие, что $(x,y) = (a,b)$. По лемме~\ref{ch0:pair} имеем $x = a$ и $y = b$, откуда $x \in A$ и $y \in B$.
	
	Это утверждение --- не такая уж и мелочь, какой оно может показаться. Чтобы понять это, рассмотрим <<сумму>> числовых множеств. Пусть $A, B \sbs \N$ и $A + B = \{ a + b \mid a \in A, b \in B \} = \{z \in \N \mid \exists a \in A\, \exists b \in B\  z = a + b\}$. Тогда $2 + 3 = 4 + 1 \in \{1,4,0\} + \{ 5,1 \}$, хотя $2 \notin \{1,4,0\}$ и $3 \notin \{5, 1\}$. 
\end{rem}

\begin{exm}\label{ch0:exm32}
	Для любых множеств $A, B, C, D$ 
	$$(A \times B) \cap (C \times D) = (A \cap C) \times (B \cap D).$$
	Пусть $z \in (A \times B) \cap (C \times D)$. Тогда $z \in A \times B$ и $z \in C \times D$.  Отсюда $z = (a,b)$ для некоторых $a \in A$ и $b\in B$. Из $(a,b) \in C \times D$ получаем $a \in C$ и $b \in D$ по замечанию~\ref{ch0:pair_uniq}. Имеем $a \in A \cap C$ и $b \in B \cap D$, откуда $z = (a, b) \in (A \cap C) \times (B \cap D)$.
	
	Для обратного включения допустим $z \in (A \cap C) \times (B \cap D)$. Тогда существуют $x \in A \cap C$ и $y \in B \cap D$ такие, что $z = (x, y)$. Так как $x \in A$ и $y \in B$, имеем $z \in A \times B$. Аналогично, $z \in C \times D$.
\end{exm}
\mcomm{We will allow more laxity (like in saying ``let $(x, y) \in (A \times B) \cap (C \times D)$'') in treating pairs when the students have clearly understood (hopefully) that one owes all such laxity solely to Lemma~\ref{ch0:pair}.}

\begin{exc}
	Докажите, что
	$(A \cup B) \times C = (A \times C) \cup (B \times C).$
\end{exc}

\begin{exm}\label{ch0:exm34}
	Пусть множество $C$ непусто. Тогда из $A \times C \sbs B \times C$ следует, что $A \sbs B$, и наоборот.
	
	Пусть $A \times C \sbs B \times C$ и $a \in A$. Так как $C \neq \void$, существует $c_0 \in C$, для которого $(a,c_0) \in A \times C$. Тогда $(a,c_0) \in B \times C$ и $a \in B$.
	
	Теперь допустим $A \sbs B$. По лемме~\ref{ch0:lattice} имеем $A = A \cap B$, откуда $A \times C = A \times (C \cap C) = (A \cap B) \times (C \cap C)$. Используя пример~\ref{ch0:exm32}, получаем
	$$A \times C = (A \times C) \cap (B \times C) \sbs B \times C.$$
\end{exm}
Обратите внимание на то, что предположение $C \neq \void$ необходимо для первой импликации. В противном случае: $\R \times \void = \void = \N \times \void$, но $\R \not\sbs \N$.

\mcomm{The clear resemblance of this statement to the cancellation laws for numerical multiplication is noteworthy. In this one and many examples that follow, the Instructor should try to persuade the students, that the Cartesian \emph{product} is a ``product'' indeed.}

\paragraph{Декартова степень.} Как и для умножения чисел, определим степень в смысле декартова произведения. Для произвольного множества $A$ и для всех натуральных $n \geq 2$ положим
$$
\begin{array}{rcl}
	A^0 &=& \{\void\};\\
	A^1 &=& A;\\
	A^n &=& \underbrace{A \times A \times \ldots \times A}_{n\ \mbox{\scriptsize копий}\ A}.
\end{array}
$$
\mcomm{We do not want any explicit recursion here since we have no Replacement axioms to make it rigorous. We prefer this schematic definition for $n$ running over $\N$.}
\begin{rem}\label{ch0:cart_pow}
	Отметим, что $A^{n+1} = A^n \times A$ для любого $n \geq 1$, а $A^0$ содержит ровно \emph{один} элемент. (Подобно тому, как $x^{n+1} = x^n \cdot x$ и $x^0 = 1$ для любых чисел $x$ и $n$.)
\end{rem}

Для $n \geq 2$ множество
$$(a_1,\ldots, a_{n-1}, a_{n}) = ((\ldots ((a_1,a_2), a_3), \ldots, a_{n-1}), a_{n})$$
называется \emph{упорядоченным набором (кортежем) длины $n$} множеств $a_1, \ldots, a_{n-1},$ $a_{n}$. Множества $a_i$ в таком случае называются \emph{компонентами} или \emph{координатами} кортежа $(a_1,\ldots, a_{n-1}, a_{n})$.
\begin{lemma}\label{L2:l_tuple_id}
	Для любого $n \geq 2$ и множеств $a_1, \ldots, a_{n}$, $b_1, \ldots, b_{n}$,
	$$(a_1,\ldots, a_{n}) = (b_1,\ldots, b_{n}) \iff a_i = b_i\ \mbox{для всех}\ i \in \{1,\ldots,n\}.$$
\end{lemma}
\begin{proof}
	Для любого $n$ достаточно применить лемму~\ref{ch0:pair} $(n-1)$ раз.
\end{proof}

\begin{rem}
	Очевидно, для любого $x$
	$$x \in A_1 \times A_2 \times \ldots \times A_n \iff \exists a_1 \in A_1 \ldots \exists a_n \in A_n\ x = (a_1,\ldots,a_n),$$
	и, в частности,
	$$x \in A^n \iff \exists a_1 \in A \ldots \exists a_n \in A\ x = (a_1,\ldots,a_n).$$
\end{rem}

\section{Индукция}
\mcomm{The main goal of this section is to introduce three popular forms of induction principle for $\N$: the most common one, the `Strong' (or `Well-founded' or `Noetherian') induction, and the Least Number principles. Then we prove that all three principles are equivalent to each other. The Instructor should encourage the students to apply these principles as formally as it is reasonably possible at their first steps.}

Давайте чуть глубже изучим множество $\N = \{0, 1, 2,\ldots \}$ \emph{натуральных чисел}. Фактически, мы не давали строго определения этого множества, и мы воздержимся от этого и впредь на протяжении этого курса.\footnote{Стандартная теоретико-множественная модель натуральных чисел такова: $0 = \void,\ 1 = \{0\},\ 2 = \{ 0, 1 \},\ \ldots,\ n + 1 = n \cup \{ n\} = \{0,1,\ldots, n \}$. Тогда можно определить $n < m$ как $n \in m$, а $n + m$ определить рекуррентно соотношениями $n + 0 = n$ и $n + (m + 1) = (n + m) + 1$. Разумеется, в этой модели многое ещё предстоит продумать.} Рассматривая множество $\N$, мы полагаем известными некоторые его <<базовые свойства>>, например $0 \neq 1$ или даже $2 < 3$ и $3 + 2 = 5$. Сейчас мы рассмотрим главное свойство $\N$ --- свойство \emph{индукции}.

В своей наиболее известной формулировке \emph{принцип математической индукции} гласит, что
\begin{quote}
	Для произвольного предиката $\phi$, если выполнено $\phi(0)$ и для каждого $n \in \N$ $\phi(n)$ влечёт $\phi(n+1)$, то $\phi(n)$ выполнено для всех $n \in \N$.
\end{quote}
Легко изложить это символьно ($\phi$ здесь фиксирован):
$$\bigl(\phi(0) \wedge \forall n\in \N \, (\phi(n) \to \phi(n+1))\bigr) \to \forall n \in \N\, \phi(n).$$
Утверждение $\phi(0)$ называется \emph{базой индукции}, $\forall n \in \N\, (\phi(n) \to \phi(n+1))$ называется \emph{шагом индукции}, а утверждение $\phi(n)$ для каждого $n$ называется \emph{предположением индукции}.

\mcomm{If the Instructor has skipped the chapter on strings, I suggest he proves `intuitive validity' of the Induction Principle similarly to Example~\ref{strings:ind_intuition}.}

\noindent Можно перефразировать этот принцип, используя множества вместо предикатов:
\begin{quote}
	Для произвольного множества $X \sbs \N$, если $0 \in X$ и для каждого $n \in \N$ $n \in X$ влечёт $n+1 \in X$, то $X = \N$.
\end{quote}
В самом деле, для каждого предиката $\phi$ мы можем рассмотреть подмножество $X = \{ n \in \N \mid \phi(n) \}$ множества $\N$ и, обратно, предикату $n \in X$ мы можем поставить в соответствие множество $X \sbs \N$. Значит, новая формулировка принципа эквивалентна изначальной.

\mcomm{In practice, the students usually ask me to elaborate on this point. Sometimes, we abandon the `set version' of induction (except for the Least Number Principle, of course) altogether.}

\begin{exm}
	Покажем, что для любого натурального $n \geq 3$ найдутся числа $a_1,\ldots,a_n \in \N_+$ такие, что
	$$1 = \frac{1}{a_1} + \ldots + \frac{1}{a_n},$$
	где $a_i \neq a_j$ при $i \neq j$.
	
	Дадим (возможно, излишне) подробное доказательство. Рассмотрим множество
	$$X = \{ n \in \N \mid\  n \geq 3 \to \mbox{существуют требуемые}\ a_1,\ldots, a_n\}.$$
	\mcomm{The Instructor might prefer a `predicate version' of this inductive proof instead.
		\medskip\\
		Many students justly see that the base case here is that of $n = 3$ essentially. Then they try to change \emph{the inductive principle} accordingly rather than the set or predicate in question. In my view, this practice is caused by their aversion to anything else than a simple equation in the inductive statement (like, say, an implication). This is counterproductive and the students should be discouraged from doing so at least in these easy early stage examples. They should be rather taught to adapt to the \emph{principle as it is} and to flexibly change the statement they want. Of course, these requirements may be finally lifted for harder problems and for students able to solve them.}
	Проверим, что $X$ удовлетворяет условию принципа индукции. Очевидно, $0 \in X$. Для произвольного $n \in X$ докажем, что $n + 1 \in X$. Если $n + 1 < 3$, то получаем это сразу. Если $n + 1 = 3$, то имеем $n+1 \in X$ в соответствии с равенством
	$$1 = \frac{1}{2} + \frac{1}{3} + \frac{1}{6}.$$
	Отметим, что индукционное предположение $2 \in X$ здесь бесполезно.\footnote{Очевидно, из $1 = \frac{1}{a} + \frac{1}{b}$ следует $a = b = 2$.} В сущности, базой данной индукции является утверждение $3 \in X$, но \emph{формально} мы можем обойтись и базой $0$.
	
	Наконец, допустим $n + 1 > 3$. Тогда $n \geq 3$. Поскольку $n \in X$, имеем некоторые попарно различные числа $a_1,\ldots, a_n$ такие, что 
	$$1 = \frac{1}{a_1} + \ldots + \frac{1}{a_n}.$$
	Умножив это равенство на $\frac{1}{2}$ и прибавив затем $\frac{1}{2}$ к обеим частям, получим
	$$1 = \frac{1}{2} + \frac{1}{2} = \frac{1}{2} + \frac{1}{2a_1} + \ldots + \frac{1}{2a_n}.$$
	Очевидно, мы можем взять числа $2, 2a_1, \ldots, 2a_n$ в качестве искомых, если докажем, что они попарно различны. Если $i \neq j$, то $a_i \neq a_j$ и $2a_i \neq 2a_j$. Но что если $2 = 2 a_i$ для какого-то $i$? Тогда $\frac{1}{a_i} = 1$, откуда $n$ не может быть больше единицы, хотя $n \geq 3$. Следовательно, это невозможно.
	
	И база индукции, и индукционный переход проверены, поэтому на основании принципа индукции мы можем заключить, что $X = \N$. Это означает, что для любого натурального $n \geq 3$ найдутся соответствующие числа $a_1,\ldots, a_n$.
\end{exm}

Другая распространённая форма индукции --- \emph{принцип сильной индукции}. Он позволяет нам использовать в качестве индукционного предположения не одно лишь $\phi(n)$, а все утверждения $\phi(0), \phi(1),\ldots, \phi(n)$ вместе. Таким образом,
\begin{quote}
	Для произвольного предиката $\phi$, если верно $\phi(0)$ и для каждого $n \in \N$ $\phi(n+1)$ выполнено, когда выполнены $\phi(0), \phi(1) \ldots, \phi(n)$, то $\phi(n)$ верно для любого $n \in \N$.
\end{quote}
Сформулируем этот принцип более кратко, опустив заодно всякие <<предикаты>>.  Назовём множество $X \sbs \N$ \emph{прогрессивным}, если для каждого $n \in \N$ из $\forall m < n\  m \in X$ следует $n \in X$.\footnote{Вспомним, что ограниченный квантор $\forall m < n\ \phi$ означает $\forall m\ (m < n \to \phi)$.} Будем писать $Prog(X)$, если $X$ прогрессивно. Тогда принцип можно изложить следующим образом:
\begin{quote}
	Для произвольного множества $X \sbs \N$, если $X$ прогрессивно, то $X = \N$.
\end{quote}
\mcomm{Of course, the Instructor might prefer to call a \emph{predicate} progressive.}

Интуитивно, множество $X$ прогрессивно, если оно <<распространяется>> на множество $\N$, <<захватывая>> каждое следующее $n$, как только оно <<захватило>> все предыдущие. Что поразительно --- в этой форме индукции, на первый взгляд, отсутствует база. Однако же это не совсем так.

\mcomm{In practice, the students usually wonder where the base case has gone to.}
\begin{lemma}
	Если $X$ прогрессивно, то $0 \in X$.
\end{lemma}
\begin{proof}
	Пусть для всякого $n$ имеем, что $\forall m < n\ m \in X$ влечёт $n \in X$. Положим в качестве $n$ число $0$. Рассмотрим посылку $\forall m < 0\  m \in X$, то есть $\forall m\, (m < 0 \to m \in X)$. Это утверждение является <<пустой истиной>>, так как $m < 0$ ложно для любого натурального $m$. Следовательно, $0 \in X$ также истинно.
\end{proof}


\begin{exm}
	Пусть $a + \dfrac{1}{a} \in \mathbb{Z}$ для некоторого $a \in \mathbb{R}$. Покажем, что $a^n + \dfrac{1}{a^n} \in \mathbb{Z}$ для любого $n \in \N$.
	
	Достаточно доказать, что множество $X = \{n \in \N \mid a^n + \dfrac{1}{a^n} \in \mathbb{Z}\}$ прогрессивно. Рассмотрим произвольное $n \in \N$. Пусть $m \in X$ для любого $m < n$. Если $n \geq 2$, то натуральные числа $n - 1$ и $n - 2$, которые меньше, чем $n$, принадлежат $X$. Имеем:
	$$a^n + \dfrac{1}{a^n} = \left (a + \dfrac{1}{a}\right)\left (a^{n-1} + \dfrac{1}{a^{n-1}} \right) - \left (a^{n-2} + \dfrac{1}{a^{n-2}} \right).$$
	Произведение и разность целых чисел есть число целое, поэтому $n \in X$. В случае же $n \leq 1$ это очевидно. Итак, получаем $Prog(X)$. Согласно принципу сильной индукции, $X = \N$.
	
	Более того, легко видеть, что $a^x + \dfrac{1}{a^x} \in \mathbb{Z}$ для любого $x \in \mathbb{Z}$, поскольку $a^x + \dfrac{1}{a^x} = a^{|x|} + \dfrac{1}{a^{|x|}}$ и $|x| \in \N$, если $x \in \mathbb{Z}$.
\end{exm}

Последняя форма индукции, которую мы рассмотрим, --- \emph{принцип наименьшего числа}:
\begin{quote}
	Для произвольного множества $X \sbs \N$, если $X \neq \void$, то существует наименьший элемент $\min X$ множества $X$.
\end{quote}
Символьно этот принцип записывается следующим образом (для фиксированного $X$):
$$\exists m\  m \in X \to \exists n\, \bigl(n \in X \wedge \forall m\, (m < n \to  m \notin X)\bigr).$$
\mcomm{As we have stated it, this is a `\emph{minimal} element principle' despite its traditional name. Although, for the linear ordering of $\N$, the choice between `minimal' and `least' does not logically matter, just the `minimal' form is equivalent to `strong' (or transfinite) induction for an arbitrary poset. Therefore we prefer to keep the traditional name for this general form of the principle. On the other hand, students may have questions on this ``misleading'' name when they have learned about minima and maxima in posets. The notation $\min X$ will be redefined later as well.}

\begin{exm}
	Найдём все целочисленные решения уравнения $8a^4 + 4b^4 + 2c^4 = d^4$.
	
	Очевидно, набор $(0,0,0,0)$ является (целочисленным) решением. Покажем, что оно единственно. Если $(a,b,c,d)$ --- решение, то $(|a|,|b|,|c|,|d|) \in \N^4$ --- тоже решение. Значит, достаточно показать отсутствие ненулевых решений из $\N$. Предположим противное. Тогда множество
	$$Y = \{ (a,b,c,d) \in \N^4 \mid 8a^4 + 4b^4 + 2c^4 = d^4 \wedge a+b+c+d > 0\}$$
	непусто. Следовательно, и множество $X = \{ a + b + c + d \mid (a,b,c,d) \in Y \} \sbs \N_+$ тоже непусто. Согласно принципу наименьшего числа, существует такое $x = \min X > 0$, что $x = a + b + c +d$ для некоторого решения $(a,b,c,d)$. Так как $8a^4 + 4b^4 + 2c^4 = d^4$, то число $d^4$ чётно. Из арифметики известно, что $d$ тоже должно быть чётным, т.\,е. $d = 2\delta$ для некоторого $\delta \in \N$.
	
	Получаем $8a^4 + 4b^4 + 2c^4 = 16\delta^4$, откуда $c^4 = 8\delta^4 - 4a^4 - 2b^4$. Легко видеть, что $c= 2\gamma$ и, из аналогичных рассуждений, $a = 2\alpha$, $b = 2\beta$ для некоторых $\alpha, \beta, \gamma \in \N$. 
	
	Заменив $a$ на $2\alpha$ и т.\,д. и разделив обе части изначального уравнения на $16$, получим $8\alpha^4 + 4\beta^4 + 2\gamma^4 = \delta^4$.  Значит, набор $(\alpha,\beta,\gamma,\delta) \in \N^4$ также является его решением. Однако $2(\alpha + \beta + \gamma + \delta) = a + b + c + d = x > 0$, откуда $\alpha + \beta + \gamma + \delta > 0$ --- и мы имеем новое \emph{ненулевое} решение. Тогда $(\alpha,\beta,\gamma,\delta) \in Y$ и $\alpha + \beta + \gamma + \delta \in X$, но $\alpha + \beta + \gamma + \delta < x = \min X$. Противоречие.
\end{exm}

На самом деле все три формы индукции, изложенные выше, логически эквивалентны друг другу, и каждую из них можно считать \emph{принципом индукции}.
\begin{thm}
	Следующие утверждения эквивалентны:
	\begin{enumerate}
		\item принцип сильной индукции;
		\item принцип наименьшего числа;
		\item принцип математической индукции.
	\end{enumerate}
\end{thm}
\mcomm{We use straightforward but abstract logical manipulations in order to prove this theorem. They pose a serious challenge for many students. Nevertheless, we prefer to make the students ``eat the frog'' now---just spending more time on this proof. In my experience, the `predicate' version of this proof is no better, whereas the `predicate' forms of the induction principle might be so for some audiences.}
\begin{proof}
	Пусть выполняется принцип сильной индукции. Докажем, что любое непустое множество $X \sbs \N$ имеет наименьший элемент. Предположим, что в каком-нибудь $X$ нет наименьшего элемента. Покажем, что множество $\bar X$ должно быть в таком случае прогрессивным. В самом деле, если $\forall m < n\; m \notin X$, то $n \notin X$, поскольку в противном случае $n$ было бы $\min X$, что невозможно. Согласно принципу сильной индукции, $\bar X = \N$, откуда $X = \void$.
	
	Пусть выполняется принцип наименьшего числа. Выведем $X = \N$ из предположений $0 \in X$ и $\forall n\, (n \in X \to n+1 \in X)$ для произвольного множества $X \sbs \N$. Рассмотрим множество $\bar X$ и допустим, что $\bar X \neq \void$. Тогда существует число $n = \min \bar X$. Согласно предположениям, $n \neq 0 \notin \bar X$. Следовательно, $n = m + 1$ для некоторого $m \in \N$. Так как $m < n$, то $m \in X$. Согласно предположениям, $n = m + 1  \in X$, что не так. Значит, такого $n$ не существует, а множество $\bar X$ пусто. Тогда $X = \N$.
	
	Пусть выполняется принцип математической индукции. Докажем, что для любого множества $X \sbs \N$ из $Prog(X)$ следует $X =\N$. Рассмотрим множество
	$$Y = \{ n \in \N \mid \forall m < n\; m \in X \}.$$
	Очевидно, $0 \in Y$. Пусть $n \in Y$. По определению имеем $\forall m < n\; m \in X$, откуда получаем $n \in X$, так как $X$ прогрессивно. Если $m < n + 1$, то $m < n$ или $m = n$. В обоих случаях имеем $m \in X$, откуда $n + 1\in Y$. Для множества $Y$ мы проверили базу и шаг индукции; согласно принципу математической индукции, $Y = \N$. Для любого $n \in \N$ выполнено $n < n + 1 \in Y$, откуда $n \in X$. Следовательно, $X = \N$.
\end{proof}

\section{Divisibility}
\mcomm{This section opens a series of classical results on integer arithmetic. We looked upon them as a playground for the students to try induction principles in solving concrete problems  and, of course, to have some rest before coming back to more abstract concepts.}

It is time to apply our knowledge of induction to natural and integer arithmetic. As earlier, we will assume some facts without a proof, like the \emph{commutativity law} for addition: $n + m = m + n$ for every $m, n \in \Z$. Such facts can be easily derived by induction from the so called \emph{recursive definitions}\footnote{Given the \emph{successor} function $S$ as a primary object ($S 0 = 1$, $S 1 = 2$, etc.), consider the identities $0 + m = m$ and $S n + m = S(n + m)$. One can prove that there exists a unique operation ${+}$ satisfying these identities for every $n, m  \in \N$.} for arithmetical operations. But this topic is a bit bigger than the scope of our Course can contain.

\paragraph{Divisibility.} From now on, the term `number' will denote an integer number by default. We say that a number $a$ \emph{divides} a number $b$ if there exists some $k \in \Z$ such that $b = ak$. Conversely, $b$ is said to be \emph{divisible} by $a$ in this case. Also, we call $a$ a \emph{divisor} of $b$ and call $b$ a \emph{multiple} of $a$. We write $a \dvd b$ when $a$ divides $b$.

\mcomm{Most students feel quite uncomfortable with the fact that $0$ divides $0$. They tend to ignore the \emph{definition} in favor of \emph{connotations}: ``as it is not possible to \emph{divide by} zero, zero cannot \emph{divide} anything''. Of course, it is a general problem of mathematical education that the students just do not \emph{read} what is written.  The Instructor should use such examples to demonstrate the importance of clear and, perhaps, \emph{slow} reading in mathematics (likewise the latter is important in philology, according to Nietzsche's famous maxim).}
\begin{exm}
	So, $2 \dvd 6$ as $6 = 2 \cdot 3$; $2 \dvd 2$ as $2 = 2 \cdot 1$; $-2 \dvd 6$ as $6 = (-2) \cdot (-3)$; $2 \dvd 0$ as $0 = 2 \cdot 0$; and, finally, $0 \dvd 0$ as $0 = 0 \cdot 2019$. In general, $a \dvd 0$  and $1 \dvd a$ for each $a$ as $0 = a \cdot 0$ and $a = 1 \cdot a$.
\end{exm}

\begin{lemma}\label{L4:l1}
	For every $a,b,c$, the following hold:
	\begin{enumerate}
		\item $a \dvd a$;
		\item if $a \dvd b$ and $b \dvd c$, then $a \dvd c$;
		\item if $a \dvd b$ and $b \dvd a$, then $a = \pm b$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The fact that $a = a \cdot 1$ gives us the first statement. For the second one, assume $a \dvd b$ and $b \dvd c$, that is, $b = ak_1$ or $c = bk_2$ for some numbers $k_1, k_2$. Hence we get $c = bk_2 = (a k_1) k_2 = a (k_1 k_2)$, which implies $a \dvd c$. For the final one, assume both $a \dvd b$ and $b \dvd a$. Then $a = b k_1$ and $b = a k_2$ for some $k_1, k_2$. This yields $a = a (k_1 k_2)$. Now, let us see if $a = 0$. If it is so, then $b = 0 \cdot k_2 = 0 = a$.
	
	Otherwise, one can cancel $a$ out to obtain $1 = k_1 k_2$. We take it as a fact that $1$ can be factorized either as $1\cdot 1$ or as $(-1) \cdot (-1)$. Thus, either $a = b$ or $a = -b$.
\end{proof}

\begin{lemma}\label{L4:l2}
	If $a \dvd b$ and $a \dvd c$, then $a \dvd (b + c)$ and $a \dvd (b - c)$.
\end{lemma}
\begin{proof}
	Assuming $b = ak_1$ and $c = ak_2$, we get $b \pm c = ak_1 + ak_2 = a(k_1 + k_2)$ by applying the distributivity law (taken without a proof).
\end{proof}
\begin{corr}\label{L4:c3}
	If $a \dvd (b + c)$ or $a \dvd (b - c) $ and $a \dvd b$, then $a \dvd c$.
\end{corr}
\begin{proof}
	Clearly, $c = (b + c) - b$ and $c = b - (b - c)$. Then apply Lemma~\ref{L4:l2}.
\end{proof}

Now, let us review the familiar procedure of integer division.
\begin{thm}\label{L4:t3}
	For every natural numbers $a$ and $b \neq 0$, there exists a unique pair $(q,r) \in \N^2$ such that
	$a = b q + r $ and $0 \leq r < b$.
\end{thm}
\begin{proof}
	Consider the set $X = \{ s \in \N \mid a < b s \}$. We take as a fact, that multiplying by a positive number $b$ is \emph{monotonic}, i.\,e., $b x < b y$ when $x < y$. Hence, $b (a + 1) > b a \geq  1 \cdot a = a$ and $a + 1 \in X$. So, $X$ is non-empty. By the Least Number Principle, there exists some $s' = \min X$. If $s' = 0$, then $a < bs' = 0$, which is impossible for a natural $a$. Otherwise, $s' = q + 1$ for some $q \in \N$, where $b q \leq a$. Hence, $0 \leq a - bq$. If $a - bq \geq b$, there would be $a - bs' = a - b(q + 1)  \geq 0$, that is, $a \geq bs'$, which is not the case. Therefore, $0 \leq a - bq < b$ and one can safely put $r = a - bq$. The existence of a required pair is thus proved.
	
	Suppose there are two such pairs $(q,r)$ and $(q',r')$. We have both $a = b q + r$ and $a = b q' + r'$, while $0 \leq r, r' < b$. If $q = q'$, then, clearly, $r = r'$ as well. Otherwise, w.\,l.\,o.\,g.,\footnote{\emph{Without loss of generality}---this means that we are going to consider just one of all possible cases but are sure that all the rest can be treated similarly.} we assume that $q < q'$, that is, $q + 1 \leq q'$. Hence, $r - r' = b q' - b q = b (q' - q) \geq b$. Then $r \geq b + r' \geq b$, which is not so.
\end{proof}
Such a number $q$ is called the \emph{partial quotient} and $r$ is called the \emph{remainder} after division of $a$ by $b$. This result can be easily generalized to arbitrary integers:
\begin{corr}\label{L4:c4}
	For every numbers $a$ and $b \neq 0$, there exists a unique pair $(q,r) \in \Z \times \N$ such that
	$a = b q + r $ and $0 \leq r < |b|$.
\end{corr}
\begin{proof}
	By Theorem~\ref{L4:t3}, there exists a pair of naturals $(q',r')$ such that $|a| = |b| q' + r'$, while $0 \leq r' < |b|$. It is clear that $c = |c| \cdot \sgn c$ for any integer $c$. So,
	$a = \sgn a (\frac{b}{\sgn b} q' + r') = \frac{\sgn a}{\sgn b} q' b + r' \sgn a$. When $a \geq 0$ or $r' = 0$, it suffices to put $(q, r) = (\frac{\sgn a}{\sgn b} q', r')$. Suppose that $a < 0$ and $r' > 0$. Then, we have
	$$a = -\frac{q'}{\sgn b} b - r' = -\frac{q'}{\sgn b} b - \frac{b}{\sgn b} + \frac{b}{\sgn b} - r' = -\frac{1 +q'}{\sgn b} b + (|b| - r'),$$
	where $0 < |b| - r' < |b|$. Now, put $(q,r) = (-\frac{1 + q'}{\sgn b} , |b| - r')$.
	
	Let's prove the uniqueness. Suppose that $b q + r = b q' + r'$, while $0 \leq r, r' < |b|$. Then $|r - r'| = |b q' - b q| = |b (q' - q)| = |b| \cdot |q' - q|$. If $q \neq q'$, then $|r - r'| \geq |b|$. W.\,l.\,o.\,g., $r > r'$, whence $r \geq |b| + r' \geq |b|$, which is not the case.
\end{proof}

\begin{exm}
	One has $-5 = -2 \cdot 3 + 1$ and $-5 = 2 \cdot (-3) + 1$. Please notice that the remainder is never negative.
\end{exm}

\paragraph{Modular arithmetic.}

Let $m$ be a positive number. We say that $a$ is \emph{congruent} to $b$ \emph{modulo} $m$ if $m \dvd (a - b)$.  We write $a \equiv b \pmod m$ and call the number $m$ a \emph{modulus} in such a case.

\begin{exm}
	We have $23 \equiv -31 \pmod 9$ since $23 - (-31) = 54$ and $9 \dvd 54$. It holds that $x \equiv y \pmod 2$ iff $x - y$ is even, that is, either both $x$ and $y$ are even or both are odd (they say that $x$ and $y$ have the same \emph{parity} in this case).
\end{exm}

\begin{rem}
	As each number is divisible by $1$, $x \equiv y \pmod 1$ for all numbers $x$ and $y$. This makes congruence modulo $1$ a trivial, uninteresting property. Therefore, we will usually suppose $m > 1$ in $x \equiv y \pmod m$.
	
	While it is possible to consider congruence modulo $0$, where $x \equiv y \pmod 0$ means $0 \dvd (x - y)$, that is, $x - y = 0$, this predicate is no more interesting: every number is only congruent to itself modulo $0$. Congruence for negative moduli is not needed either since $m \dvd (x - y)$ is equivalent to $-m \dvd (x - y)$.
\end{rem}

\mcomm{In practice, we also used simplified notation $a \equiv b\ (m)$ as well. Many students are already familiar with this concept but prefer to define it in terms of remainders. The Instructor should underline that each of these two equivalent definitions may be preferable to the other one in various situations.}

\begin{lemma}\label{L4:l5}
	For any numbers $a, b, m$, it holds that $a \equiv b \pmod m$ iff $a$ and $b$ leave the same remainder when divided by $m$.
\end{lemma}
\begin{proof}
	Suppose that $m \dvd (a - b)$. By Corollary~\ref{L4:c4}, one have $a = m q + r$ and $b = m q' + r'$, where $0 \leq r, r' < m$. So, $m \dvd (m(q - q') + (r - r'))$. By Corollary~\ref{L4:c3}, this implies $m \dvd (r - r')$. We are done if $r = r'$. Otherwise, w.l.o.g., assume that $r > r'$. Then $r = mk + r'$ for some $k > 0$, whence $r \geq mk \geq m$, which is not so.
	
	For the other direction, suppose that $a$ and $b$ give the same remainder $m$, so $a = m q + r$ and $b = m q' + r$ for some $q$ and $r$. Then $a - b = m(q - q')$, which is clearly a multiple of $m$.
\end{proof}

\begin{corr}\label{L4:c9}
	No two of the numbers $0, 1,\ldots, m - 1$ are congruent modulo $m$.
\end{corr}

\begin{corr}\label{L4:rem_congr}
	Suppose $r$ is the remainder after dividing $a$ by $m$. Then $a \equiv r \pmod m$.
\end{corr}

\begin{corr}
	For any numbers $a, b, m$, the following hold:
	\begin{enumerate}
		\item $a \equiv a \pmod m$;
		\item if $a \equiv b \pmod m$, then $b \equiv a \pmod m$;
		\item if $a \equiv b \pmod m$ and $b \equiv c \pmod m$, then  $a \equiv c \pmod m$.
	\end{enumerate}
\end{corr}
The most interesting and most important point about congruence is that it `respects' the arithmetical operations.
\begin{lemma}\label{L4:l7}
	Suppose that $a \equiv b \pmod m$ and $c \equiv d \pmod m$. Then the following hold:
	\begin{enumerate}
		\item $a + c \equiv b + d \pmod m$;
		\item $a c \equiv b  d \pmod m$;
		\item $a^n \equiv b^n \pmod m$ for every $n \in \N$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For the first statement, let us see that $a + c - (b + d) = (a - b) + (c - d)$. As both these summands are multiples of $m$, the sum is a multiple of $m$ too by Lemma~\ref{L4:l2}.
	
	Consider the second one. We know that $a = b + mk_1$ and $c = d + mk_2$ for some numbers $k_1, k_2$. So, $ac = bd + m(b k_2 + d k_1 + m k_1 k_2)$. Hence $m \dvd (ac - b d)$.
	
	The third is obtained from the second one by a trivial induction on $n$.
\end{proof}
\begin{exm}
	This fact simplifies computations modulo $m$ a lot. Indeed, one may freely replace any summand or factor by any other one he sees convenient given the two are congruent modulo $m$. Often, one prefers positive or negative numbers small in their absolute value for this procedure (say, one can replace $99$ by $-1$ in \emph{any} computation modulo $100$ that involves just addition and multiplication).
	
	Let us compute the remainder after dividing $71^{59}$ by $97$. As $71 \equiv -26 \pmod{97}$, we get
	$$x = 71^{59} \equiv -2^{59}\cdot 13^{59} \equiv -2^{59}\cdot (-4\cdot 21)^{59} \equiv 2^{177} \cdot 3^{59} \cdot 7^{59} \equiv 2^{177} \cdot (3^4)^{14} \cdot 3^3 \cdot (7^2)^{29} \cdot 7  \pmod {97}.$$
	Furthermore, $7^2 \equiv -48 \equiv -7^2 + 1$, whence $2\cdot7^2 \equiv 1 \pmod{97}$; also, $3^4 \equiv -2^4$, and $27\cdot 7 \equiv 3 \cdot 63 \equiv 3 \cdot (-34) \equiv -102 \equiv -5$. Then,
	\begin{multline*}
		x \equiv (2\cdot 7^2)^{29} \cdot 2^{148} \cdot 2^{56} \cdot (-5) \equiv 1^{29} \cdot (2^6)^{34} \cdot (-5) \equiv -5 \cdot (-33) ^{34} \equiv -5 \cdot 3^{34} \cdot 11^{34}  \equiv \\
		-45 \cdot (3^4)^{8} \cdot (11^2)^{17} \equiv -45 \cdot 2^{32} \cdot (3 \cdot 2^3)^{17} \equiv -135 \cdot 2^{83} \cdot (3^4)^4  \equiv -38 \cdot 2^{99} \equiv -19 \cdot (2^6)^{16} \cdot 2^4 \equiv \\
		-19 \cdot 3^{16} \cdot (11^2)^8 \cdot 2^4 \equiv -19 \cdot 3^{24} \cdot 2^{24} \cdot 2^4 \equiv -19 \cdot 2^{24} \cdot 2^{24} \cdot 2^4  \equiv -19 \cdot (2^6)^8 \cdot 2^4 \equiv -19 \cdot 3^{8} \cdot (11^2)^4 \cdot 2^4 \equiv\\
		-19 \cdot 3^{12} \cdot 2^{12} \cdot 2^{4} \equiv -19 \cdot (-2^{4})^3 \cdot 2^{12} \cdot 2^4 \equiv 19 \cdot 2^{24} \cdot 2^4 \equiv 19 \cdot (2^6)^4 \cdot 2^4 \equiv 19 \cdot 3^{4} \cdot (11^2)^2 \cdot 2^4 \equiv\\
		-19 \cdot 2^{4} \cdot (24)^2 \cdot 2^4 \equiv -19 \cdot 2^2 \cdot 3^2 \cdot (2^6)^2 \equiv -19 \cdot 2^2 \cdot 3^4 \cdot (11)^2 \equiv 19 \cdot 2^2 \cdot 2^4 \cdot 3 \cdot 2^3 \equiv 19 \cdot 3 \cdot 2^9 \equiv -40 \cdot 2^9 \equiv\\
		-5 \cdot 2^{12} \equiv -5 \cdot (2^6)^2 \equiv -5 \cdot 3^2 \cdot 11^2 \equiv
		-5 \cdot 3^3 \cdot 2^3 \equiv -10 \cdot 108 \equiv -10 \cdot 11 \equiv -13 \equiv 84 \pmod {97}.
	\end{multline*}
	So, the remainder in question is $84$. This computation was far from optimal, as we ignored many noticeable facts (like $2^{48} \equiv 1 \pmod {97}$ or $2^{96} \equiv 1 \pmod {97}$). We shall soon see how it is possible to easily prove these. Nevertheless, this tedious computation was very easy in the respect that we had no need to manipulate any number greater than $204$ in absolute value.
\end{exm}

\paragraph{Prime numbers and factorization.} A number $p > 1$ is called \emph{prime} if it is divisible by $\pm 1$ and by $\pm p$ but nothing else. Otherwise, a number greater than $1$ is called \emph{composite}.

\begin{exm}
	The number $101$ is prime, while $91 = 7 \cdot 13$ is composite. The negatives, $0$, and $1$ are neither prime nor composite.
\end{exm}

%\begin{lemma}
%Each number $n > 1$ is either prime or a multiple of a lesser prime.
%\end{lemma}
%\begin{proof}
%Assuming the contrary and applying the Least Number Principle, consider the least $n > 1$ with does not meet the requirement. Clearly, $n$ is not prime. Hence, it has a divisor $m$ different from $1$ and $n$. As multiplication is monotonic, we get $m < n$, which implies that $m$ is either prime (which is impossible) or is divisible by some prime $p < m$. But then $p \dvd n$ and $p < n$. Thus, $n$ meets the requirement. A contradiction.
%\end{proof}

A representation of the form $n = p_1^{a_1} p_2^{a_2} \ldots p_s^{a_s}$ for a number $n > 1$, where $p_i$ are pairwise distinct primes and $a_i \in \N$, is called a \emph{(prime) factorization} of $n$. One may naturally suppose that \emph{every} prime number $p$ takes part in a factorization of $n$ but just finitely many of them have non-zero degrees $a$. It is possible to factorize $1$ as well, but each prime shall have degree $0$ then. Two factorization are \emph{distinct} iff some prime number has different degrees in these.

\mcomm{It would be very natural to define a factorization as a function from primes to naturals with a finite support. Unfortunately, we have not enough set theory up to this point to do so. However, the students are usually happy with these ugly notions of `distinct' factorizations etc. for those are intuitive enough.}

\begin{exm}
	The prime $11$ has degree $0$ in the factorization $3 \cdot 5 \cdot 7$ of the number $105$. One may say $11$ is \emph{absent} from that factorization. The factorizations $2 \cdot 3^2 \cdot 5^3$ and $3^3 \cdot 5^2 \cdot 7$ are distinct.
\end{exm}

\begin{thm}[Fundamental Theorem of Arithmetic]\label{L4:t14}
	For each number $n > 1$, there exists a unique prime factorization.
\end{thm}
\begin{proof}
	Firstly, we check existence of a factorization. Assume the contrary and consider the least number $n > 1$ lacking a factorization (such a number must exist by the Least Number Principle). If $n$ is prime, there clearly is a trivial factorization. Hence, $n$ is composite and has a non-trivial divisor $m$, that is, $n = m k$, where $1 < m, k < n$ (as multiplication is monotonic). Then $m = p_1^{a_1} p_2^{a_2} \ldots p_s^{a_s}$ and $k = p_1^{b_1} p_2^{b_2} \ldots p_s^{b_s}$ for some primes $p_1, p_2, \ldots, p_s$ (if $p_i$ is absent from the factorization of, say, $m$, just put $a_i = 0$). We can thus factorize $n$ as $p_1^{a_1 + b_1} p_2^{a_2 + b_2} \ldots p_s^{a_s + b_s}$. A contradiction.
	
	Secondly, we should prove that no two factorizations of $n > 1$ are distinct. Assume the contrary and consider the least number $n > 1$ with two distinct factorizations:  $n = p_1^{a_1} p_2^{a_2} \ldots p_s^{a_s} = p_1^{b_1} p_2^{b_2} \ldots p_s^{b_s}$. We have $\min(a_i, b_i) = 0$ for each $i$ as otherwise one can cancel $p_i^{\min(a_i, b_i)}$ out thus decreasing $n$ but keeping the factorizations distinct. Hence, in each factorization, there should be some prime absent from the other (for otherwise at least one of these equals $1$). W.\,l.\,o.\,g., let it be that $a_1 > 0 = b_1$, $a_2 = 0 < b_2$, and $p_1 > p_2$. Consider the number
	$$m = (p_1 - p_2)p^{a_1 - 1}_1 p_2^{a_2} \ldots p_s^{a_s} = n - p_2 p^{a_1 - 1}_1 p_2^{a_2} \ldots p_s^{a_s} = p_2 (p^{b_1}_1 p_2^{b_2 - 1} \ldots p_s^{b_s} - p^{a_1 - 1}_1 p_2^{a_2} \ldots p_s^{a_s}) > 0.$$
	Clearly, $m < n$, so $m$ has a unique factorization, which is provided by multiplying some factorization of the number $p^{b_1}_1 p_2^{b_2 - 1} \ldots p_s^{b_s} - p^{a_1 - 1}_1 p_2^{a_2} \ldots p_s^{a_s}$ by $p_2$. As $a_2 = 0$, we have $m = (p_1 - p_2)p^{a_1 - 1}_1 p_3^{a_3} \ldots p_s^{a_s}$, so another factorization of $m$ is provided by multiplying $p^{a_1 - 1}_1 p_3^{a_3} \ldots p_s^{a_s}$ by some factorization of $p_1 - p_2$. Since these two factorizations of $m$ cannot be distinct, $p_2$ must be present in both. So, $p_2 \dvd (p_2 - p_1)$, whence $p_2 \dvd p_1$. This is impossible for $p_1$ and $p_2$ are distinct primes. A contradiction.
\end{proof}

As we have already said, $1$ has a trivial factorization from which any prime is absent. Zero has infinitely many factorizations, but each containing zero itself: $0 = 673 \cdot 2 \cdot 3 \cdot 23 \cdot 0 \cdot 1$ etc. To factor a negative integer $n$, it is enough to factor its absolute value $|n|$ and multiply the result by $-1$. So, every number $a \neq 0$ can be identified with its prime factorization.

\begin{thm}
	There are infinitely many prime numbers.
\end{thm}
\begin{proof}
	Assume it is not so. Clearly, there exists at least one prime. Then some $p_1, p_2, \ldots, p_n$ are the only primes in existence. Consider the number $N = p_1 p_2 \ldots p_n + 1$. Since multiplication is monotonic, $N > p_i \geq 2$ for each $i$. Hence, $N$ is not prime. Because of Theorem~\ref{L4:t14}, $N$ should be a multiple of some prime $p_j$. By Corollary~\ref{L4:c3}, get the impossible conclusion $p_j \dvd 1$. Our assumption thus fails.
\end{proof}



\begin{lemma}[Divisibility Criterion]\label{L4:l16}
	For any non-zero numbers $a$ and $b$, $a$ divides $b$ iff $\alpha_i \leq \beta_i$ for each prime $p_i$, where $\alpha_i$ (or $\beta_i$) is the degree of $p_i$ in the factorization of $a$ (respectively, $b$).
\end{lemma} 
\begin{proof}
	Let us factor both $a$ and $b$ to obtain $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$ and $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$, where $\eps, \delta = \pm 1$.
	
	Suppose we have $b = a k$. By factoring $k$, get $k = \pm p^{\gamma_1}_1\ldots p^{\gamma_s}_s$. As the factorization of $b$ is unique, there should be $\beta_i = \alpha_i + \gamma_i \geq \alpha_i$.
	
	For the other direction, one may put $\gamma_i = \beta_i - \alpha_i \geq 0$ to see that $b = a \cdot \frac{\delta}{\eps}  p^{\gamma_1}_1\ldots p^{\gamma_s}_s$.
\end{proof}

\section{A Few Classical Theorems}

We say that a number $d \in \N$ is a \emph{greatest common divisor} of numbers $a$ and $b$ if (1) $d \dvd a$ and $d \dvd b$, and (2) for each $d' \in \N$, from $d' \dvd a$ and $d' \dvd b$, it follows that $d' \dvd d$. We write $\gcd(a, b) = d$ in such a case. In other words, $d$ is such a common divisor of $a$ and $b$ that is a multiple of any (other) common divisor $d'$. A priori, it is not obvious why such a number $d$ exists.

\begin{lemma}
	For every $a$ and $b$, there exists a unique number $d$ such that \ $d = \gcd(a,b)$.
\end{lemma}
\begin{proof}
	Firstly, we check uniqueness. If there are two such numbers $d$ and $d'$, we have both $d \dvd d'$ and $d' \dvd d$, whence $d = d'$ by Lemma~\ref{L4:l1}.
	
	Secondly, let us demonstrate existence. If $a = 0$, put $d = |b|$. Clearly, $|b| \dvd 0$ and $|b| \dvd b$. Whenever $d' \dvd b$, we get $d' \dvd d$. The case when $b = 0$ is similar.
	
	Suppose that neither of $a$ and $b$ is zero. Then factor these to get $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$ and $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$. Consider $d = p^{\min(\alpha_1, \beta_1)}_1\ldots p^{\min(\alpha_s, \beta_s)}_s$. By Lemma~\ref{L4:l16}, obtain $d \dvd a$ and $d \dvd b$. Assume both $d' \dvd a$ and $d' \dvd b$ for some $d'  \in \N$. Clearly, $d' \neq 0$, so one factors $d'$ as $p^{\gamma_1}_1\ldots p^{\gamma_s}_s$. Applying Lemma~\ref{L4:l16}, we have both $\gamma_i \leq \alpha_i$ and $\gamma_i \leq \beta_i$, whence $\gamma_i \leq \min(\alpha_i, \beta_i)$ for each $i$. Thus, $d' \dvd d$.
\end{proof}

So, it is easy to find the number $\gcd(a, b)$ given some factorizations of these numbers. But it is not that easy to factor a number. We shall see a more practical way to compute $\gcd(a, b)$ soon.

\begin{exm}
	Inspecting the proof of the previous lemma, one observes that $\gcd(15, -12) = 3$, $\gcd(-14, -21) = 7$, $\gcd(1, a) = 1$, and $\gcd(0, a) = |a|$ for each $a$. In particular, $\gcd(0, 0) = 0$. Evidently, $\gcd(a, b) = \gcd(|a|,|b|)$.
\end{exm}

We say that a number $m \in \N$ is a \emph{least common multiple} of numbers $a$ and $b$ if (1) $a \dvd m$ and $b \dvd m$ and (2) for each $m' \in \N$, from $a \dvd m'$ and $b \dvd m'$, it follows that $m \dvd m'$. We write $\lcm (a, b) = m$ in such a case. In other words, $m$ is such a common multiple of $a$ and $b$ that divides any (other) common multiple $m'$.
\begin{exc}
	For each $a$ and $b$, their least common multiple truly exists and is unique, while $\lcm(0, a) = 0$ and $\lcm(a, b) = p^{\max(\alpha_1, \beta_1)}_1\ldots p^{\max(\alpha_s, \beta_s)}_s$ if $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$ and $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$.
\end{exc}
\mcomm{There is a clear similarity between $\gcd$ and minimum (respectively, $\lcm$ and maximum)---since both are infima for suitable posets (we will elaborate on the idea later). The Instructor might want to highlight this fact.}
\begin{exc} For every $a, b, c$ the following hold:
	\begin{enumerate}
		\item $\gcd(a, a) = |a| = \lcm(a, a)$;
		\item $\gcd(a, b) = \gcd(b,a)$; $\lcm(a, b) = \lcm(b,a)$;
		\item $\gcd(a, \gcd(b, c)) = \gcd(\gcd(a, b), c)$; $\lcm(a, \lcm(b, c)) = \lcm(\lcm(a, b), c)$;
		\item $\gcd(a, \lcm(a, b)) = |a| = \lcm(a, \gcd(a, b))$;
		\item $\gcd(a,b)\cdot \lcm(a, b) = |ab|$.
	\end{enumerate}
\end{exc}

\begin{lemma}
	For any numbers $a$, $b$, and $q$, $\gcd(a, b) = \gcd(a + bq, b)$.
\end{lemma}
\begin{proof}
	Suppose that $d = \gcd(a,b)$ and $d' = \gcd(a + bq, b)$. Clearly, $d \dvd (a + bq)$ and $d \dvd b$, so $d \dvd d'$ by definition. Conversely, $d' \dvd b$, $d' \dvd bq$, and $d' \dvd a$, for $a = (a + bq) - bq$. Hence, $d' \dvd d$. Therefore, $d = d'$.
\end{proof}

\begin{corr}\label{L5:c_gcd_rem}
	If $r$ is the remainder after dividing $a$ by $b$, then $\gcd(r, b) = \gcd(a, b)$.
\end{corr}
\begin{proof}
	As $a = bq + r$, get $r = a - bq$, but $\gcd(a - bq, b) = \gcd(a, b)$ by the previous lemma.
\end{proof}

\paragraph{Coprimes.} Numbers $a$ and $b$ are called \emph{coprime} (to each other) if $\gcd(a, b) = 1$. That is, the only common natural divisor of $a$ and $b$ is $1$.
\begin{exm}
	Clearly, every two distinct primes are coprime. The number $1$ is coprime to any number. The numbers $2^2 \cdot 3$ and $- 5 \cdot 7^3$ are coprime unlike $2^2 \cdot 3$ and $3 \cdot 5$. The number $0$ is coprime to just $\pm 1$.
\end{exm}

\begin{lemma}\label{L5:coprime}
	For every numbers $a$ and $b$, the following statements are equivalent:
	\begin{enumerate}
		\item $a$ and $b$ are coprime;
		\item for every $d \in \N$, if $d \dvd a$ and $d \dvd b$, then $d = 1$;
		\item for every $n$, if $n \dvd a$ and $n \dvd b$, then $n$ is not prime;
	\end{enumerate}
	and, when $a, b$ are non-zero, also
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item $\min(\alpha, \beta) = 0$ for each prime $p$, where $\alpha$ and $\beta$ are the degrees of $p$ in the factorizations of $a$ and $b$, respectively.
	\end{enumerate}
\end{lemma}

\begin{corr}
	If both $a$ and $b$ are non-zero, then the numbers $a' = \frac{a}{\gcd(a, b)}$ and $b' = \frac{b}{\gcd(a, b)}$ are integer and coprime.
\end{corr}
\begin{proof}
	Let $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$ and $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$. Then $a' = \eps p^{\alpha'_1}_1\ldots p^{\alpha'_s}_s$ and $b' = \delta p^{\beta'_1}_1\ldots p^{\beta'_s}_s$, where $\alpha'_i = \alpha_i - \min(\alpha_i, \beta_i)$ and $\beta'_i = \beta_i - \min(\alpha_i, \beta_i)$ for each $i$. As $\alpha_i, \beta_i \geq \min(\alpha_i, \beta_i)$, both the numbers $a', b'$ are integer. Since $\min(\alpha_i, \beta_i)$ is either $\alpha_i$ or $\beta_i$, then $\alpha'_i = 0$ or $\beta'_i = 0$, whence $\min(\alpha'_i, \beta'_i) = 0$. By Lemma~\ref{L5:coprime}, $a'$ and $b'$ are coprime.
\end{proof}

%\begin{corr}
%For any numbers $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$  and $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$, $a$ and $b$ are coprime iff $\min(\alpha_i, \beta_i) = 0$ for each $i$.
%\end{corr}

\begin{lemma}\label{L5:l2}
	Suppose that $a \dvd b c$, while $a$ and $b$ are coprime. Then $a \dvd c$.
\end{lemma}
\begin{proof}
	If $c = 0$ or $a =  \pm 1$, the conclusion is immediate. Assume $c \neq 0$ and $a \neq \pm 1$. Then $\gcd(a,0) = |a| \neq 1 = \gcd(a,b)$, whence $b \neq 0$. Since $bc \neq 0$, $a$ is non-zero as well. Let us factor our numbers: $a = \eps p^{\alpha_1}_1\ldots p^{\alpha_s}_s$, $b = \delta p^{\beta_1}_1\ldots p^{\beta_s}_s$, and $c = \eta p^{\gamma_1}_1\ldots p^{\gamma_s}_s$. From Lemmas~\ref{L4:l16} and~\ref{L5:coprime}, we know that $\alpha_i \leq \beta_i + \gamma_i$ and $\min(\alpha_i, \beta_i) = 0$ for each $i$. If $\alpha_i = 0$, we have $\alpha_i \leq \gamma_i$; otherwise, $\beta_i = 0$ and $\alpha_i \leq 0 + \gamma_i = \gamma_i$ as well. Therefore, $a \dvd c$.
\end{proof}
\noindent This means that one can sometimes cancel numbers out from congruences.
\begin{corr}\label{L5:c_cancel}
	If $a x \equiv a y \pmod m$ and $\gcd(a, m) = 1$, then $x \equiv y \pmod m$.
\end{corr}
\begin{proof}
	Indeed, from $m \dvd a(x - y)$,  it follows $m \dvd (x - y)$.
\end{proof}

\begin{exc}\label{L5:gcd_copirme1}
	If $a$ and $b$ are coprime, then $\gcd(ac, b) = \gcd(c, b)$.
\end{exc}

\begin{exc}\label{L5:gcd_copirme2}
	If $a$ and $b$ are coprime, then $\gcd(ab, c) = \gcd(a,c) \cdot \gcd(b,c)$.
\end{exc}

\noindent The following fact of fundamental import establishes a link between divisibility and addition.
\begin{thm}[B\'ezout's Identity]\label{L5:t1}
	Let numbers $a$ and $b$ be coprime. Then there exist numbers $u$ and $v$ such that $a u + b v = 1$.
\end{thm}
\begin{proof}
	Consider the set $X = \{ d \in \N_+  \mid  d = a s + b t\ \mbox{for some}\ s, t \in \Z \}$. Clearly, at least one of $a$ and $b$ is non-zero. Let it be $a$. Then $d = a \sgn a + b \cdot 0 = |a| > 0$, so $X$ is non-empty. By the Least Number Principle, the exists some $d = \min X$ and numbers $s,t$ so that $d = a s + b t > 0$.
	
	Let us divide $a$ by $d$ to obtain $q, r$ such that $a = d q + r$ and $0 \leq r < d$. We get
	$$r = a - d q = a - (a s + b t) q = a (1 - sq) + b(-tq).$$
	Therefore, either $r \in X$ or $r = 0$. The former case is not possible since $r < d = \min X$. Hence, $r = 0$ and $d \dvd a$. One can prove that $d \dvd b$ in a similar way.
	
	So, $d > 0$ is a common divisor of coprime numbers. Hence, $a s + b t = d = 1$.
\end{proof}

\begin{corr}\label{L5:bezout_int}
	For every numbers $a$ and $b$, there exist numbers $u$ and $v$ such that \ $a u + b v = \gcd(a,b)$.
\end{corr}
\begin{proof}
	This is trivial when either number is zero. Otherwise, the numbers $a' = \frac{a}{\gcd(a, b)}$ and $b' = \frac{b}{\gcd(a, b)}$ are coprime integers. Applying B\'ezout's Identity, obtain some $u$ and $v$ such that $a'u + b'v = 1$. Then multiply both sides by $\gcd(a,b)$ to get $a u + b v = \gcd(a, b)$.
\end{proof}

Unlike rationals or reals, one cannot turn an arbitrary integer $b \neq 0$ into $1$ by multiplying it by some integer $c$. Hence, it is not generally possible to construct an \emph{integer} $\frac{a}{b}$ such that $\frac{a}{b} b = a$. But this may be possible modulo $m$.
\begin{corr}\label{L5:c2}
	Let $a$ and $m > 1$ be some numbers. There exists $x$ such that $a x \equiv 1 \pmod m$ iff $a$ and $m$ are coprime.
\end{corr}
\begin{proof}
	If $a$ and $m$ are coprime, we obtain $1 = a x + m y$ for some $x, y$ by applying Theorem~\ref{L5:t1}. Then $m \dvd (a x - 1)$.
	
	Conversely, if $m \dvd (a x - 1)$, then $ax + m y = 1$ for some $y$. For any $d \in \N$, from $d \dvd a$ and $d \dvd m$, it clearly follows that $d \dvd 1$, that is, $d = 1$; hence, $a$ and $m$ are coprime.
\end{proof}
\noindent The number $x$ is then called a \emph{(multiplicative) inverse} of $a$ modulo $m$.
\begin{exm}
	As $15$ and $14$ are coprime, there should be some $x$ such that $14 x \equiv 1 \pmod {15}$. Using Lemma~\ref{L4:l7}, one sees that $14 x \equiv -x \pmod {15}$. So, it suffices to take $-1$ (or $15 - 1 = 14$) for $x$. We shall see a uniform way to find the inverse $x$ in the next section.
\end{exm}


\paragraph{Theorems of Euler's and Fermat's.} Let $m$ be greater than $1$. Consider the numbers $1, 2, \ldots, m - 1$. Some of these (including $1$ and $m-1$, of course) are coprime with $m$. Let $q_1, \ldots, q_s$ be all such integers. The number $s$ thereof is denoted by $\phi(m)$. For this number depends on $m$, one may consider the function $\phi$ returning $s$ given $m$. This function is called \emph{Euler's totient function}.

\begin{exm}
	One has $\phi(9) = 6$ for $1, 2, 4, 5, 7, 8$ are the only positive numbers lesser than $9$ and coprime to it. Clearly, $\phi(p) = p - 1$ for every prime $p$. We shall derive a general formula for $\phi$ later.
\end{exm}

\begin{thm}[Euler's Theorem]
	If $a$ and $m > 1$ are coprime, then $a^{\phi(m)} \equiv 1 \pmod m$.
\end{thm}
\begin{proof}
	Let $s$ be $\phi(m)$ and consider the numbers $q_1, \ldots, q_s$ discussed in the above. By Corollary~\ref{L4:c9}, $q_i \not\equiv q_j \pmod m$ if $i \neq j$. But then $a q_i \not\equiv a q_j \pmod m$, as Corollary~\ref{L5:c_cancel} implies. This means the numbers $a q_1, \ldots, a q_s$ have $s$ pairwise distinct remainders when divided by $m$. Let us denote these remainders by $r_1,\ldots, r_s$ respectively. Obviously, $0 \leq r_i \leq m - 1$ for each $i$.
	
	Consider an arbitrary natural divisor $d$ of both $m$ and $a q_i$. It is easy to see that $a$ and $d$ are coprime, as it follows that $d' \dvd a$ and $d' \dvd m$ from $d' \dvd  a$ and $d' \dvd d$. Then $d \dvd a q_i$ implies $d \dvd q_i$ by Lemma~\ref{L5:l2}. For $m$ and $q_i$ being coprime, we obtain $d = 1$, that is, $a q_i$ and $m$ are coprime as well. 
	
	On the other hand, $\gcd(r_i, m) = \gcd(a q_i, m) = 1$ by Corollary~\ref{L5:c_gcd_rem}. Thus, $r_1, \ldots, r_s$ are pairwise distinct numbers from the set $\{1,\ldots, m-1\}$, each being coprime with $m$. But $q_1, \ldots, q_s$ are the only such numbers. This means that $\{r_1, \ldots, r_s \} = \{q_1, \ldots, q_s\}$ (but not necessarily that $q_i = r_i$ for every $i$).
	
	Finally, we obtain
	$$r_1\cdot \ldots \cdot r_s \equiv a q_1 \cdot \ldots \cdot a q_s \equiv a^s \cdot q_1 \cdot \ldots \cdot q_s \equiv a^s \cdot r_1 \cdot \ldots \cdot r_s \pmod m.$$
	Applying Corollary~\ref{L5:c_cancel} to each $r_i$, we cancel them out to get $1 \equiv a^s \pmod m$, which was to be proved.
\end{proof}

\begin{corr}[Fermat's Little Theorem]
	If $p$ is prime and $p \ndvd a$, then $a^{p - 1} \equiv 1 \pmod p$.
\end{corr}
\begin{proof}
	For $p$ is prime, $\phi(p) = p - 1$ and $\gcd(p, a)$ is either $1$ or $p$; but $p \ndvd a$, hence $\gcd(p,a) = 1$.
\end{proof}
\begin{rem}
	Another equivalent statement of Fermat's Little Theorem is as follows:
	\begin{quote}
		If $p$ is prime, then $a^p \equiv a \pmod p$.
	\end{quote}
\end{rem}
\noindent  These theorems could be handy in modular calculations and spare us a great deal of work.
\begin{corr}
	If $\gcd(a, m) = 1$, $x, y \geq 0$, and $x \equiv y \pmod {\phi(m)}$, then $a^x \equiv a^y \pmod m$.
\end{corr}
\begin{proof}
	We have both $x = k\phi(m) + r$ and $y = l\phi(m) + r$, where $0 \leq r < m$. Then
	$$a^x \equiv a^{k\phi(m) + r} \equiv (a^{\phi(m)})^k a^r \equiv 1^k \cdot a^r \equiv a^r \equiv 1^l \cdot a^r \equiv (a^{\phi(m)})^l a^r \equiv a^{l\phi(m) + r} \equiv a^y \pmod m.$$
\end{proof}
\begin{exm}
	What is the last digit of the number $1234567^N$ in decimal notation, where $N = {\underbrace{11\ldots1}_{2019}}$?
	
	Clearly, that last digit is just the remainder after dividing $1234567^N$ by $10$. It is easy to see that $\phi(10) = 4$, but $N \equiv 100K + 11 \equiv 0 + 11 \equiv 3 \pmod 4$. Therefore, $1234567^N \equiv 7^N \equiv 7^3 \equiv (-3)^2 \cdot 7 \equiv -7 \equiv 3 \pmod{10}$.
\end{exm}



\section{Linear Equations and Congruences}

From Corollary~\ref{L5:bezout_int}, we know that for every numbers $a$ and $b$, there exist some $u$ and $v$ such that $a u + b v = \gcd(a,b)$. Let us learn an elegant way to compute all the numbers $u$, $v$ and $\gcd(a,b)$ simultaneously.

We begin with a particular example. Consider the numbers $26$ and $34$. Dividing the former by the latter, obtain
$$
\begin{array}{lcl}
	26 &=& 34\cdot0 + 26.
\end{array}
$$
Here, $26$ is the remainder. Let us divide the former divisor $34$ by the remainder:
$$
\begin{array}{lcl}
	34 &=& 26 \cdot 1 + 8.
\end{array}
$$
Then, we are to iterate such a procedure: \emph{divide the divisor by the remainder}---obtaining
$$
\begin{array}{lcl}
	26 &=& 8 \cdot 3 + 2,\\
	8 &=& 2 \cdot 4 + 0.\\
\end{array}
$$
No more divisions are possible as the last remainder is zero. The last \emph{non-zero} remainder is $2$, while $\gcd(26,34) = 2$. This is no coincidence.

We need a more general setting to see why it is so. Let $a$ and $b$ be arbitrary integers such that $b \neq 0$. Dividing $a$ by $b$ first and \emph{the divisor by the remainder} hereafter, one gets
$$\begin{array}{lcll}
	a &=& bq_1 + r_2,\quad &0 < r_2 < |b|;\\
	b &=& r_2 q_2 + r_3,\quad &0 < r_3 < r_2;\\
	r_2 &=& r_3 q_3 + r_4,\quad &0 < r_4 < r_3;\\
	\ldots&&&\\
	r_{k} &=& r_{k + 1} q_{k + 1} + r_{k+2};\quad &0\leqslant r_{k+2} < r_{k+1};\\
	\ldots&&&\\
\end{array}$$
Since $0 \leq r_k$ for each $k$, there must exist a \emph{least} such number $r_{s+1}$ (by the Least Number Principle). If $r_{s+1} > 0$, it is still possible to divide $r_{s}$ by $r_{s+1}$ to obtain a lesser remainder $r_{s+2} < r_{s+1}$; thus, $r_{s+1}$ would not be the least remainder. Hence, $r_{s+1} = 0$ while $r_s > 0$.

The procedure presented is known as the \emph{Euclidean Algorithm}. We have just proved 
\begin{lemma}
	The Euclidean Algorithm terminates for each input $(a, b)$ where $b \neq 0$.
\end{lemma}

\begin{lemma}
	Let $r_s$ be the last non-zero remainder resulting from applying the Euclidean Algorithm to numbers $a$ and $b \neq 0$. Then $\gcd(a, b)= r_s$. 
\end{lemma}
\begin{proof}
	Applying Corollary~\ref{L5:c_gcd_rem}, we obtain consecutively:
	\begin{multline*}
		\gcd(a, b) = \gcd(b, r_2) = \gcd(r_2, r_3) = \gcd(r_3, r_4) = \ldots =\\
		\gcd(r_{k}, r_{k+1}) = \gcd(r_{k+1}, r_{k+2}) = \ldots = \gcd(r_s, r_{s+1}) = \gcd(r_s, 0) = r_s.
	\end{multline*}
\end{proof}

So, we have got a way to calculate $\gcd(a, b)$ without factoring either number. But how can one extract numbers $u$ and $v$ such that $a u + b v = \gcd(a, b)$ from this procedure? Let us see!

Given numbers $a$ and $b$, the Euclidean Algorithm constructs the sequences of numbers $r_k$ and $q_k$. Denoting $a$ by $r_0$ and $b$ by $r_1$, we can rearrange all the equations from the algorithm to the form
$$r_{k+2} = r_k - r_{k+1}q_{k+1}.$$
Let us define two more number sequences: those of $u_k$ and $v_k$. It is easy to see that the equations
$$
\begin{array}{lcccl}
	a &=& r_0 &=& au_0 + bv_0,\\
	b &=& r_1 &=& au_1 + bv_1.\\
\end{array}
$$
have a solution $u_0 = 1$, $v_0 = 0$, $u_1 = 0$, and $v_1 = 1$. Let this be the \emph{definition} for $u_0, v_0, u_1, v_1$. We want to extend those equations to all values of $k$ so that
$$
\begin{array}{lcl}
	r_k &=& au_k + bv_k.\\
\end{array}
$$
With this in view, we define
$$
\begin{array}{lcl}
	u_{k+2} &=& u_k - u_{k+1}q_{k+1},\\
	v_{k+2} &=& v_k - v_{k+1}q_{k+1},\\
\end{array}
$$
so that $u_k$ and $v_k$ mimic the behavior of the sequence of $r_k$. Let us check the identity $r_k = au_k + bv_k$ by (strong) induction on $k$. This boils down to inferring $r_{k + 2} = a u_{k+2} + b v_{k+2}$ from the hypotheses $r_{k + 1} = a u_{k+1} + b v_{k+1}$ and $r_{k} = a u_{k} + b v_{k}$. Indeed,
\begin{multline*}
	a u_{k+2} + b v_{k+2} = a(u_k - u_{k+1}q_{k+1}) + b(v_k - v_{k+1}q_{k+1}) =\\
	(a u_k + b v_k) - (a u_{k+1} + b v_{k+1})q_{k+1} = r_k - r_{k+1}q_{k+1} = r_{k+2}. 
\end{multline*}
In particular, we obtain $\gcd(a, b) = r_s = a u_s + b v_s$. This is called the \emph{Extended Euclidean Algorithm}.

Let us come back to our numeric example. We know that $q_1 = 0$, $q_2 = 1$, $q_3 = 3$, $q_4 = 4$, and $\gcd(26, 34) = r_4 = 2$. Then,
$$
\begin{array}{lcccl}
	u_{2} &=& u_0 - u_{1}q_{1} &=& 1;\\
	v_{2} &=& v_0 - v_{1}q_{1} &=& 0;\\
	u_{3} &=& u_1 - u_{2}q_{2} &=& -1;\\
	v_{3} &=& v_1 - v_{2}q_{2} &=& 1;\\
	u_{4} &=& u_2 - u_{3}q_{3} &=& 4;\\
	v_{4} &=& v_2 - v_{3}q_{3} &=& -3.\\
\end{array}
$$
So, there should be $2 = 26 \cdot 4 + 34 \cdot (-3)$, which is surely the case. 

\paragraph{Linear equations.} The Extended Euclidean Algorithm allows us to solve any equation of the form $a x + b y = c$ in integer numbers. This equation is trivial when $a$ or $b$ equals zero. For example, if $a = 0 \neq b$, the equation $b x = c$ has a (unique) solution $\frac{c}{b}$ iff $b \dvd c$.

\begin{thm}\label{L6:t_eq} For any numbers $a \neq 0$, $b \neq 0$, and $c$, the equation
	\begin{equation}\label{L6:eq3}
		a x + b y = c
	\end{equation}
	has a solution iff $d \dvd c$, where $d = \gcd(a, b)$. If $d \dvd c$, the set of solutions to (\ref{L6:eq3}) is
	$$\{ (x_0 - b't,\ y_0 + a't) \mid t \in \Z \},$$
	where $a' = \frac{a}{d}$, $b' = \frac{b}{d}$, $c' = \frac{c}{d}$, and $(x_0, y_0)$ is an arbitrary solution to the equation
	\begin{equation}\label{L6:eq4}
		a' x + b' y = c'.
	\end{equation}
\end{thm}
\begin{proof}
	Clearly, equation (\ref{L6:eq3}) has no solution unless $d \dvd c$. Suppose that $d \dvd c$; then $d \neq 0$ cancels out from (\ref{L6:eq3}), which results in the equivalent equation (\ref{L6:eq4}). Hence, it suffices to consider just the latter.
	
	Let $(x_0, y_0)$ be an arbitrary solution (to (\ref{L6:eq4})). Then the pair $(x_0 - b't, y_0 + a't)$ is a solution as well for whatever $t \in \Z$. Clearly,
	$$a'(x_0 - b't) + b'(y_0 + a't) = a'x_0 + b'y_0 - a'b't + b'a't = a'x_0 + b'y_0 = c.$$
	
	Moreover, any solution relates this way to $(x_0, y_0)$. Indeed, let $(x, y)$ be a solution. Then
	$$a'(x_0 - x) + b'(y_0 - y) = c' - c' = 0,$$
	i.\,e., $a'(x_0 - x) = b'(y - y_0)$. As $\gcd(a', b') = 1$, apply Lemma~\ref{L5:l2} to conclude $b' \dvd (x_0 -  x)$ and $a' \dvd (y - y_0)$, that is, $x = x_0 - b't_1$ and $y = y_0 + a't_2$ for some $t_1, t_2 \in \Z$. Replacing $x$ and $y$ in (\ref{L6:eq4}) by the respective right-hand sides, one gets
	$$c' = a'(x_0 - b't_1) + b'(y_0 + a't_2)  = a'x_0 - a'b't_1 + b'y_0 + b'a't_2 = c' + a'b'(t_2 - t_1),$$
	whence $a'b'(t_2 - t_1) = 0$ and $t_1 = t_2$ (for $a', b' \neq 0$).
	
	It remains to show that equation (\ref{L6:eq4}) has a solution indeed. By Corollary~\ref{L5:bezout_int}, there are some numbers $u$ and $v$ such that
	$$a' u + b' v = 1.$$
	(In practice, these numbers can be found with the Extended Euclidean Algorithm.) If $x_0 = uc'$ and $y_0 = vc'$, then $(x_0, y_0)$ is clearly a solution to (\ref{L6:eq4}).
\end{proof}

\begin{exm}
	Solve the equation $45x - 37y = 25$ in integer numbers.
	
	As $\gcd(45, -37) = 1$, the equation has a solution $(x_0, y_0)$. Moreover, the solutions are exactly the pairs $(x_0 + 37t, y_0 + 45t)$ for all possible $t \in \Z$. Given $u$ and $v$ with $45 u - 37 v = 1$, it is enough to put $(x_0, y_0) = (25 u, 25 v)$.
	
	Let us use the Extended Euclidean Algorithm to obtain these numbers $u$ and $v$ (as well as $\gcd(45,-37)$, indeed). It might be somewhat easier for a human to divide \emph{positive} integers, so we shall seek for $u'$ and $v'$ satisfying $45 u' + 37 v' = 1$ to change the sign thereafter.
	$$
	\begin{array}{lcllcllcl}
		45 &=& 37\cdot1 + 8;\quad& u_2 &=& u_0 - u_1 q_1 = 1 - 0\cdot 1 = 1;&\quad v_2 &=& v_0 - v_1 q_1 = 0 - 1\cdot  1 = -1;\\
		37 &=& 8\cdot 4 + 5;\quad& u_3 &=& u_1 - u_2 q_2 = 0 - 1\cdot 4 = -4;&\quad v_3 &=& v_1 - v_2 q_2 = 1 - (-1)\cdot  4 = 5;\\
		8 &=& 5\cdot 1 + 3;\quad& u_4 &=& u_2 - u_3 q_3 = 1 - (-4)\cdot 1 = 5;&\quad v_4 &=& v_2 - v_3 q_3 = -1 - 5\cdot  1 = -6;\\
		5 &=& 3\cdot 1 + 2;\quad& u_5 &=& u_3 - u_4 q_4 = -4 - 5\cdot 1 = -9;&\quad v_5 &=& v_3 - v_4 q_4 = 5 - (-6)\cdot  1 = 11;\\
		3 &=& 2\cdot 1 + 1;\quad& u_6 &=& u_4 - u_5 q_5 = 5 - (-9)\cdot 1 = 14;&\quad v_6 &=& v_4 - v_5 q_5 = -6 - 11\cdot  1 = -17;\\
		2 &=& 1\cdot 2 + 0.&&&&&&\\
	\end{array}
	$$
	Thus $1 = r_6 = 45 u_6 + 37 v_6 = 45 \cdot 14 + 37 \cdot (-17)$. Changing the sign, we see that $1 = 45 u - 37 v$ when $u = 14$ and $v = 17$. Hence, $(x_0, y_0) = (350, 425)$ and the set of solutions of the equation in question is just $\{ (350 + 37t, 425 + 45t) \mid t \in \Z \}$.
\end{exm}

\paragraph{Linear congruences.} Given $a$, $c$, and $m$, we say that the expression $a x \equiv c \pmod m$ is a \emph{congruence} in the variable $x$. This congruence can be solved for $x$. Namely, we say that a number $x_0$ is a \emph{solution} to the congruence if $a x_0 \equiv b \pmod m$ and $0 \leq x_0 < m$. In other words, we are exclusively interested in the remainders modulo $m$ of possible values for $x$. Why?

Indeed, knowing all the solutions suffices to find \emph{every} number $x'$ such that $ax' \equiv c \pmod m$. On the one hand, if $x'$ is such a number and $x''$ is its remainder after dividing by $m$, we have $x' \equiv x'' \pmod m$ by Corollary~\ref{L4:rem_congr}, whence $a x'' \equiv ax' \equiv c \pmod m$ by Lemma~~\ref{L4:l7}, so $x''$ is a solution. On the other hand, if $x$ is a solution and $x' \equiv x \pmod m$, we see that $ax' \equiv ax \equiv c \pmod m$. Finally, for every number $x'$, $a x' \equiv c \pmod m$ iff there exists a \emph{solution} $x$ to this congruence such that $x' \equiv x \pmod m$.

\begin{thm}\label{L6:t_cong} For any numbers $a \neq 0$, $c$, $m > 1$, the congruence
	\begin{equation}\label{L6:eq1}
		a x \equiv c \pmod m
	\end{equation}
	has a solution iff $d \dvd c$, where $d = \gcd(a, m)$. If $d \dvd c$, there are exactly $d$ pairwise distict solutions:
	$$x_0,\ x_0 + m',\ x_0 +m'2,\ \ldots,\ x_0 + m'(d - 1),$$
	where $x_0$ is a unique solution to the congruence
	\begin{equation}\label{L6:eq2}
		a' x \equiv c' \pmod {m'},
	\end{equation}
	and $a' = \frac{a}{d}$, $c' = \frac{c}{d}$, and  $m' = \frac{m}{d}$.
\end{thm}
\begin{proof}
	For each $x$, the number $x$ satisfies (\ref{L6:eq1}) iff there exists some $k$ such that $ax = c + mk$. This is impossible unless $d \dvd c$. Suppose $d \dvd c$. Then the latter equation is equivalent to $a'x = c' + m'k$ (as $d \neq 0$ cancels out), which indeed has a solution $(x_1, k_1)$ due to Theorem~\ref{L6:t_eq}. Consequently, $x$  satisfies congruence (\ref{L6:eq1}) iff it satisfies (\ref{L6:eq2}). However, this does not mean their solutions are the same since the condition $0 \leq x < m$ does not imply $0 \leq x < m'$.
	
	At first, we observe that if $x$ satisfies (\ref{L6:eq1}) (or, equivalently, (\ref{L6:eq2})), then the remainder $x'$ after dividing $x$ by $m'$ is a solution to (\ref{L6:eq2}) as  $x \equiv x' \pmod {m'}$ and $0 \leq x' < m'$. As we know, some $x_1$ indeed satisfies (\ref{L6:eq2}). Therefore, (\ref{L6:eq2}) has a solution $x_0$.
	
	Let us show that this solution is unique. For, if $x$ is another solution to $(\ref{L6:eq2})$, then, obviously, $a'x \equiv a' x_0 \pmod{m'}$, whence $x \equiv x_0 \pmod{m'}$ by Corollary~\ref{L5:c_cancel}. On the other hand, $0 \leq x, x_0 < m'$; one concludes that $x = x_0$ by Corollary~\ref{L4:c9}.
	
	Clearly, $x_0 + m' k \equiv x_0 \pmod {m'}$ for each $k$, hence any number of the form $x_0 + m' k$ satisfies both (\ref{L6:eq2}) and (\ref{L6:eq1}). Among such numbers, just the following satisfy $0 \leq x < m = m'd$:
	$$x_0,\ x_0 + m',\ x_0 + m'2,\ \ldots,\ x_0 + m'(d - 1).$$
	So, these are distinct solutions to (\ref{L6:eq1}). Conversely, let $x$ be a solution to (\ref{L6:eq1}). Then $x$ satisfies (\ref{L6:eq2}) as well, whence $a' x \equiv c' \equiv a' x_0 \pmod {m'}$. By Corollary~\ref{L5:c_cancel}, this yields $x \equiv x_0 \pmod {m'}$, i.\,e., $x = x_0 + m'k$ for some $k$, whereas $0 \leq x_0 < m'$ and $0 \leq x < m = m'd$. This results in $0 \leq k < d$. Hence, congruence (\ref{L6:eq1}) has no solution except those listed above.
\end{proof}

\begin{exm}
	Solve the congruence $12 x \equiv -15 \pmod 9$.
	
	Clearly, $d = \gcd(12, 9) = 3$. So, $a' = 4$, $c' = -5$, and $m' = 3$. The only solutions to this congruence are the numbers:
	$x_0,\ x_0 + 3\cdot 1,\ x_0 + 3\cdot 2$,
	where $x_0$ is a unique solution to $4 x \equiv -5 \equiv 1 \pmod 3$. One can easily guess this unique solution, so $x_0 = 1$. Hence, $\{1, 4, 7\}$ is the set of solutions to the original congruence. Should it be harder to guess, one might use the Extended Euclidean Algorithm to find $x_0$ likewise the case of equation.
\end{exm}

\begin{rem}
	Notice that any polynomial congruence, that is, of the form $P(x_1,\ldots,x_n) \equiv 0 \pmod m$, where $P$ is a polynomial in variables $x_1,\ldots,x_n$ with integer coefficients,---not just a linear one---could be solved by applying brute force: it suffices to test $m \dvd P(x_1,\ldots,x_n)$ for all possible tuples $(x_1,\ldots,x_n)$ of numbers $x_i \in \{0, 1,\ldots m - 1\}$. Lemma~\ref{L4:l7} then guarantees that $P(x'_1,\ldots,x'_n) \equiv 0 \pmod m$ iff there exists a solution $(x_1,\ldots,x_n)$ such that $x'_i \equiv x_i$ for each $i$ (for $x_i$, take the remainder after dividing $x'_i$ by $m$). Of course, applying brute force is generally inefficient for moduli big enough---even for the simplest linear congruence we have just considered.
\end{rem}

\begin{exm}
	Let us solve the congruence $P(x) = 7x^3 - 3x^2 + x - 4 \equiv 0 \pmod 6$. Clearly, $P(x) \equiv x^3 - 3x^2 + x - 4 \equiv x^2(x - 3) + (x - 3) - 1 \equiv (x^2 + 1)(x-3) - 1 \pmod 6$.
	$$
	\begin{array}{rcl}
		P(0) &\equiv& -4 \equiv 2 \pmod 6;\\
		P(1) &\equiv& 2\cdot(-2) - 1 \equiv -5 \equiv 1 \pmod 6;\\
		P(2) &\equiv& 5\cdot(-1) - 1 \equiv -6 \equiv 0 \pmod 6;\\
		P(3) &\equiv& 10\cdot 0 - 1 \equiv -1 \equiv 5 \pmod 6;\\
		P(4) &\equiv& P(-2) \equiv 5\cdot (-5) - 1 \equiv -1 \cdot 1 - 1 \equiv -2 \equiv 4 \pmod 6;\\
		P(5) &\equiv& P(-1) \equiv 2\cdot (-4) - 1 \equiv -9 \equiv 3 \pmod 6.\\
	\end{array}
	$$
	Thus, $2$ is the only solution to this congruence.
\end{exm}

\paragraph{Simultaneous congruences.} Consider some moduli $m_1, \ldots, m_n$ and numbers $a_1, \ldots, a_n$. Is it always possible to find some number $x$ congruent to $a_i$ modulo $m_i$ for each $i$? In other words, is it always possible to recover a number given all the remainders after dividing it by $m_i$?

It is easy to see that the answer is negative as no number $x$ satisfies both $x \equiv 4 \pmod {6}$ and  $x \equiv 1 \pmod {8}$. Indeed, from the first congruence it follows that $x$ is even, so $1$ should be even as well when taking into account the second one.

However, the answer becomes positive assuming the numbers $m_1, \ldots, m_n$ are pairwise coprime.  We say that a number $x_0$ is a \emph{solution} to the system of simultaneous congruences
\begin{equation}\label{L6:eq5}
	\begin{array}{l}
		\begin{cases}
			x \equiv a_1 \pmod {m_1}\\
			x \equiv a_2 \pmod {m_2}\\
			\ldots\\
			x \equiv a_n \pmod {m_n},\\
		\end{cases}
	\end{array}
\end{equation}
iff $x_0$ satisfies each congruence and $0 \leq x_0 < M$, where $M = m_1 m_2 \ldots m_n$.

\begin{rem}
	Again, why are we interested in solutions from $\{0, \ldots, M-1\}$ solely? Do they suffice to find \emph{all} the numbers which satisfy the system?
	
	Let us consider a more general situation. We say that $\mu$ is a \emph{least common multiple} of numbers $c_1,\ldots,c_n$ iff (1) $c_i \dvd \mu$ for each $i$; and (2) for every number $\mu'$, if $c_i \dvd \mu'$ for each $i$, then $\mu \dvd \mu'$. This is a straightforward generalization of ${\lcm}$ for two numbers, so we write $\mu = \lcm(c_1,\ldots,c_n)$.
	
	\begin{exc}\label{exc_n_lcm}
		Prove that for every $c_1, \ldots, c_n$, there exists a unique such $\mu$ that $\mu = \lcm(c_1,\ldots,c_n)$. Moreover, if every $c_i$ is non-zero, so that $c_i = p^{\gamma_{i\; 1}}_1 \ldots p^{\gamma_{i\; s}}_s$, then $\mu = p^{\max_i \gamma_{i\; 1}}_1 \ldots p^{\max_i \gamma_{i\; s}}_s$.
	\end{exc}
	
	\noindent Consider a system 
	\begin{equation*}
		\begin{array}{l}
			\begin{cases}
				P_1(x_1, \ldots, x_k) \equiv 0 \pmod {c_1}\\
				P_2(x_1, \ldots, x_k) \equiv 0 \pmod {c_2}\\
				\ldots\\
				P_n(x_1, \ldots, x_k) \equiv 0 \pmod {c_n},\\
			\end{cases}
		\end{array}
	\end{equation*}
	where every $P_i$ is a polynomial in variables $x_1,\ldots,x_k$ with integer coefficients. We call a tuple $(x_1, \ldots, x_k)$ a \emph{solution} to this system if it satisfies each congruence and $0 \leq x_1, \ldots, x_k < \mu$, where $\mu = \lcm(c_1, \ldots, c_n)$. We shall show that a tuple $(x'_1, \ldots, x'_k)$ satisfies the system iff there exists a solution $(x_1, \ldots, x_k)$ to the latter such that $x'_j \equiv x_j \pmod \mu$ for every $j$.
	
	Indeed, if a tuple $(x'_1, \ldots, x'_k)$ satisfies the system, one can consider another tuple $(x''_1, \ldots, x''_k)$, where every $x''_j$ is the remainder after dividing $x'_j$ by $\mu$. Then $\mu \dvd (x'_j - x''_j)$ and $c_i \dvd \mu$, whence $x''_j \equiv x'_j \pmod {c_i}$ for every $i$ and $j$. By Lemma~\ref{L4:l7}, $P_i(x''_1,\ldots, x''_k) \equiv P_i(x'_1,\ldots, x'_k) \equiv 0 \pmod {c_i}$ for each $i$. Hence, $(x''_1, \ldots, x''_k)$ is a solution to the system. The other direction is similar. Thus, solutions are sufficient to restore all the satisfying tuples.
	\begin{lemma}\label{congr:lcm}
		If the numbers $c_1, \ldots, c_n$ are pairwise coprime, then $\lcm(c_1, \ldots, c_n) = c_1 \cdot \ldots \cdot c_n$. 
	\end{lemma}
	\begin{proof}
		The case when some $c_i$ is zero is trivial. Assume none is. Then every prime $p_j$ has degree $\gamma_{1\; j} + \ldots + \gamma_{n\; j}$ in the right-hand side and degree $\max_{1 \leq i \leq n} \gamma_{i\; j}$ in the left-hand side (see Exercise~\ref{exc_n_lcm}). When $i \neq k$, we have $\gcd(c_i, c_k) = 1$, whence $\min(\gamma_{i\; j}, \gamma_{k\; j}) = 0$ for each $j$. This means that all the numbers $\gamma_{1\; j}, \ldots, \gamma_{n\; j}$, except at most one---the greatest among them, equal zero. Therefore, $\gamma_{1\; j} + \ldots + \gamma_{n\; j} = \max_{1 \leq i \leq n} \gamma_{i\; j}$ for each $j$; hence, $\lcm(c_1, \ldots, c_n) = c_1 \cdot \ldots \cdot c_n$.
	\end{proof}
	\noindent This explains why we have taken $M = m_1 m_2 \ldots m_n$ for system~(\ref{L6:eq5}).
\end{rem}

\begin{thm}[Chinese Remainder Theorem]\label{L6:t_chinese}
	Given the numbers $m_1, \ldots, m_n$ are pairwise coprime, system (\ref{L6:eq5}) has a unique solution.
\end{thm}
\begin{proof}
	We check the uniqueness first. Let $x$ and $x'$ be two solutions to (\ref{L6:eq5}). Clearly, $x \equiv x' \pmod {m_i}$, that is, $m_i \dvd (x - x')$ for each $i$. By Lemma~\ref{congr:lcm}, $M = \lcm(m_1, \ldots, m_n)$; hence, $M \dvd (x - x')$. Therefore, $x \equiv x' \pmod M$, whence $x = x'$ by Corollary~\ref{L4:c9}.
	
	We prove the existence of a solution by induction on $n$. For $n = 1$, the remainder after dividing $a_1$ by $m_1$ is a sure solution. Suppose that there is a solution to any system of $n$ congruences of the form (\ref{L6:eq5}), while we are given with an arbitrary system of the form:
	\begin{equation}\label{L6:eq6}
		\begin{array}{l}
			\begin{cases}
				x \equiv a_1 \pmod {m_1}\\
				x \equiv a_2 \pmod {m_2}\\
				\ldots\\
				x \equiv a_n \pmod {m_n}\\
				x \equiv a \pmod {m},
			\end{cases}
		\end{array}
	\end{equation}
	where $m, m_1, \ldots, m_n$ are pairwise coprime.
	Let $x_0$ be a solution to the first $n$ simultaneous congruences. Those congruences are thus equivalent to the system
	\begin{equation}\label{L6:eq7}
		\begin{array}{l}
			\begin{cases}
				x \equiv x_0 \pmod {m_1}\\
				x \equiv x_0 \pmod {m_2}\\
				\ldots\\
				x \equiv x_0 \pmod {m_n},\\
			\end{cases}
		\end{array}
	\end{equation}
	since given $x_0 \equiv a_i \pmod{m_i}$, for each $x$ one has $x \equiv a_i \pmod {m_i}$ iff $x \equiv x_0 \pmod {m_i}$. Let $\mu$ be $m_1 m_2 \ldots m_n$. By the proven uniqueness statement, system~(\ref{L6:eq7}) has a unique solution among $0, 1, \ldots, \mu - 1$, that is, if $x$ satisfies~(\ref{L6:eq7}), then $x \equiv x_0 \pmod \mu$. Clearly, the latter is enough for $x$ to satisfy~(\ref{L6:eq7}). Finally, one sees that system~(\ref{L6:eq6}) is equivalent to
	\begin{equation}\label{L6:eq8}
		\begin{array}{l}
			\begin{cases}
				x \equiv x_0 \pmod {\mu}\\
				x \equiv a \pmod {m}.\\
			\end{cases}
		\end{array}
	\end{equation}
	As $\mu$ and $m$ are coprime, there exist some numbers $u$ and $v$ such that $\mu u + m v = 1$ due to Corollary~\ref{L5:bezout_int}. Consider the number $x_1 = a \mu u + x_0 m v$. We see that $x_1 - x_0 = a \mu u + x_0 (mv - 1) = a \mu u + x_0 (- \mu u) = \mu u (a - x_0)$, whence $\mu \dvd (x_1 - x_0)$. Likewise, $x_1 - a= a (\mu u - 1)  + x_0mv = a (- m v) + x_0mv = m v (x_0 - a)$, which yields $m \dvd (x_1 - a)$. Therefore, $x_1$ satisfies both systems (\ref{L6:eq8}) and (\ref{L6:eq6}). Let $x_2$ be the remainder after dividing $x_1$ by $M = \mu m$. As $x_2 \equiv x_1 \pmod m$ and $x_2 \equiv x_1 \pmod {m_i}$ for each $i$, the number $x_2$ is a solution to (\ref{L6:eq6}).
\end{proof}

In fact, one can easily extract a recursive algorithm for solving a system of the form (\ref{L6:eq5}) from the proof presented. The numbers $u$ and $v$ could surely be found using the Extended Euclidean Algorithm.

\begin{exm}
	Solve the simultaneous congruences:
	$$
	\begin{array}{l}
		\begin{cases}
			x \equiv 12 \pmod {15}\\
			x \equiv 8 \pmod {17}\\
			x \equiv 3 \pmod 8.\\
		\end{cases}
	\end{array}
	$$
	
	Let us solve the first two congruences. Firstly, we are to find some numbers $u$ and $v$ such that $15 u + 17 v = 1$. Applying the Extended Euclidean Algorithm, obtain $u = 8$ and $v = -7$. According to the proof of Theorem~\ref{L6:t_chinese}, the number $x_1 = 8 \cdot 15 \cdot 8  + 12 \cdot 17 \cdot (-7) \equiv -15 \cdot 4 - 51 \cdot (-7) \equiv 357 - 60 \equiv 42 \pmod {15 \cdot 17}$ satisfies those congruences and $42$ is the only solution to the system thereof. We have thus reduced the problem to solving the following simultaneous congruences:
	$$
	\begin{array}{l}
		\begin{cases}
			x \equiv 42 \pmod {255}\\
			x \equiv 3 \pmod 8.\\
		\end{cases}
	\end{array}
	$$
	It is easy to guess the values $u' = -1$ and $v' = 32$ for $255u' + 8v' = 1$ to hold. Hence, $x_2 = 3\cdot (-255) + 42 \cdot 32 \cdot 8 = 3\cdot (14 \cdot 256 - 255) = 3 \cdot (13 \cdot 255 + 14) \equiv 7 \cdot 255 + 3 \cdot 14 \equiv 1827 \pmod {255 \cdot 8}$ satisfies the latter system and $1827$ is the unique solution to the original problem.
\end{exm}

\begin{exm}
	This method can be easily generalized to the case when some congruences are of the form $bx \equiv a \pmod m$ (or any other form we can however solve). For example, let us consider the system
	$$
	\begin{array}{l}
		\begin{cases}
			2x \equiv 10& \pmod {22}\\
			x \equiv 8& \pmod {31}.\\
		\end{cases}
	\end{array}
	$$
	By Theorem~\ref{L6:t_cong}, the first congruence has solutions $5$ and $16$ and is thus equivalent to the \emph{disjunction} $x \equiv 5 \pmod{22} \vee x \equiv 16 \pmod{22}$, which may be also written as
	$$
	\begin{array}{l}
		\begin{sqcases}
			x \equiv 5& \pmod {22}\\
			x \equiv 16& \pmod {22}.\\
		\end{sqcases}
	\end{array}
	$$
	As simultaneous congruences form a \emph{conjunction}, one can apply $(A \vee B) \wedge C \equiv (A \wedge C) \vee (B \wedge C)$ to obtain
	$$
	\begin{array}{l}
		\begin{sqcases}
			\begin{cases}
				x \equiv 5\phantom{1}& \pmod {22}\\
				x \equiv 8& \pmod {31}\\
			\end{cases}\\[15pt]
			\begin{cases}
				x \equiv 16& \pmod {22}\\
				x \equiv 8& \pmod {31}.\\
			\end{cases}
		\end{sqcases}
	\end{array}
	$$
	Now, it suffices to solve each subsystem separately.
\end{exm}


\section{Binary Relations}\label{sect:7}
\mcomm{We introduce the algebra of binary relations to provide students with a unified formalism for functions, orders and equivalences. First, this allows to solve many problems via a straight-forward symbolic computation. Second, we try to overcome the well-known difficulties many students experience when learning images and pre-images (which are typically presented in an asymmetric manner and for the function case only) and various concepts of a function's `inverse' as well.}

Let $A$ and $B$ be some sets. Any set $R$ such that $R \sbs A \times B$ is called a \emph{(binary) relation between} sets $A$ and $B$. This simple notion forms the base for modeling functions and orderings in the formalism of sets. The set
$$\dom R = \{a \in A \mid \exists b\, (a, b) \in R \}$$
is called the \emph{domain} of the relation $R$ while the set
$$\rng R = \{b \in B \mid \exists a\, (a, b) \in R \}$$
is called the \emph{range} of the relation $R$. The set $\dom R \cup \rng R$ is called the \emph{field} of the relation $R$. Clearly, $\dom R \sbs A$ and $\rng R \sbs B$. We see that $\dom R$ ($\rng R$) is the set of the first (second) coordinates of the pairs from $R$. If we mention just a \emph{binary relation} $R$, we imply that some sets $A$ and $B$ with $R \sbs A \times B$ are fixed yet their exact specification is immaterial.

\mcomm{We prefer this `parametric' definition to the more natural one (where every set $R$ of ordered pairs is called a relation) since the latter would require taking ${\cup} {\cup} R$ for the relation's field, whereas we want to avoid infinite unions generally.}

\begin{exm}\label{rel:exm_rel}
	The sets $\void$ and $A \times B$ are binary relations between $A$ and $B$.
	
	Let $A = \{0,2, x, y\}$ and $B = \{z, 0, 1\}$ for some sets $x, y, z$. The set
	$$R = \{ (0, 0), (0,z), (x, 1), (y,1) \}$$
	is a binary relation between $A$ and $B$. Clearly, $\dom R = \{0,x,y\} \sbs A$ (but $\dom R \neq A$ when neither $x$ nor $y$ equals $2$) and $\rng R = B$.
\end{exm}

If $R \sbs A \times A$ for a set $A$, the relation $R$ is called a \emph{binary relation on} the set $A$.
\begin{exm}
	The set $\void$ is a binary relation on any set. The sets $A^2 = A \times A$ and
	$$\id_A = \{(x,x) \mid x \in A \}$$
	are binary relations on $A$. The \emph{identity relation} $\id_A$ is of some import. Clearly, $(x,y) \in \id_A$ iff $x = y$ and $x,y \in A$. One may think of $\id_A$ as of the set-theoretic representation for equality predicate (restricted to the set $A$).
	
	Likewise, one can consider the predicate `less than' over natural numbers as a \emph{set}---namely, the binary relation ${<} = \{ (m, n) \in \N^2 \mid m\ \mbox{is less than}\ n\}$. This way, one gets $(2,3) \in {<}$ (as $2 < 3$) yet $(2,2) \notin {<}$ (as $2 \not< 2$). In fact, \emph{every} binary predicate over a set may be identified with a binary relation. Thus, binary relations comprise the set-theoretic model for binary predicates.
	
	On the other hand, for any given binary relation $R$, we will use a `predicate-style' notation, that is, we will often write $x R y$ instead of $(x,y) \in R$.
	
	Every geometric figure in the coordinate plane $\R^2$ is a binary relation on $\R$. Its domain and range are the projections thereof onto the abscissa and ordinate axes, respectively. (See Figure~\ref{rel:proj}).
	\begin{figure}[h]
		\centering
		\includegraphics*[width=0.7\textwidth]{rel_proj.pdf}
		\caption{\label{rel:proj} A planar figure (equivalently, a binary relation on $\R$) $F$ with its two projections $\dom F$ and $\rng F$ are highlighted.}
	\end{figure}
\end{exm}

For any finite relation $R$ one can draw a \emph{diagram} (in principle, at least). That is, one labels some plane points with elements of the field of $R$ (or a superset thereof). Sometimes, they allow multiple points with identical labels or multiple labels for the same point (if the respective elements are equal). Then one draws an arrow from the point (labeled by) $a$ to the point $b$ iff $(a,b) \in R$. When plotting two or more relations on one diagram, it is useful to label the arrows themselves with relation symbols such as `$R$'.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{rel_diag.pdf}
	\caption{A diagram for the relation $\{ (5,5), (5,3), (1,5), (1,2), (4, 0) \}$ on the set $\{0, 1, 2, 3, 4, 5, 8\}$.}
\end{figure}


\begin{exc}
	Let $A = \{1,2,3,4\}$. Draw a diagram depicting relations $P = A^2$, $Q = \id_A$, and $R = \{ (m,n) \in A^2 \mid m < n\}$.
\end{exc}

\paragraph{Algebra of relations.} Let $R \sbs A \times B$. The \emph{converse} relation of $R$ is the relation
$$R^{-1} = \{(b,a) \in \rng R \times \dom R \mid (a, b) \in R\},$$
that is, each pair from $R$ is overturned in $R^{-1}$.

\begin{exc}
	Prove that $\dom R^{-1} = \rng R$ and $\rng R^{-1} = \dom R$.
\end{exc}

\begin{exm}
	For every sets $A, B$, it is clear that $\void^{-1} = \void$, $\id^{-1}_A = \id_A$, and $(A \times B)^{-1} = B \times A$.
	
	As we have already seen, the predicates `less than', `less or equal' (for natural arguments) etc. may be treated as binary relations on $\N$. We have ${<}^{-1} = {>}$, ${>}^{-1} = {<}$, and ${\leq}^{-1} = {\geq}$.
\end{exm}

Since binary relations are sets, every set algebra operation makes sense for them. For example, one has ${<} \cup \id_\N = {\leq}$. Indeed, a pair $(x, y) \in \N^2$ belongs to the left-hand side iff $x < y$ or $x = y$, which is equivalent to $x \leq y$. As usual, to define the \emph{complement} of a relation $R$, one must fix a \emph{universe} first. Since we have just \emph{relations between two sets} formally, one has certain sets $A$ and $B$ with $R \sbs A \times B$ already fixed. In general, we put $\bar R = (A \times B) \setminus R$, but caution is recommended at this point.

\begin{rem}
	It is important to realize that \emph{conversion} and \emph{complement} operations are not the same. (We shall see in a moment that they result in distinct relations `almost surely'.) In particular, ${\bar <} = {\geq}$ (since $x \not< y$ is equivalent to $x \geq y$ for all $x, y \in \N$) whereas ${<}^{-1} = {>}$.
\end{rem}
\mcomm{In my experience, the students often confuse the two.}

\begin{lemma} Let $P, Q \sbs A \times B$ be arbitrary binary relations. Then
	\begin{enumerate}
		\item $(P^{-1})^{-1} = P$;
		\item $(P \cup Q)^{-1} = P^{-1} \cup Q^{-1}$;
	\end{enumerate}
\end{lemma}
\begin{proof}
	For the first statement, for an arbitrary pair $(a,b)$ obtain
	$$
	\begin{array}{rcl}
		(a, b) \in (P^{-1})^{-1} &\iff& (b,a) \in P^{-1}\\
		&\iff&  (a,b) \in P.\\
	\end{array}
	$$
	For the second one, 
	$$
	\begin{array}{rcl}
		(a, b) \in (P \cup Q)^{-1} &\iff& (b,a) \in P \cup Q\\
		&\iff& (b,a) \in P \vee (b,a) \in Q\\
		&\iff& (a,b) \in P^{-1} \vee (a,b) \in Q^{-1}\\
		&\iff& (a,b) \in P^{-1} \cup Q^{-1}.\\
	\end{array}
	$$
\end{proof}
\begin{corr}
	If $P \sbs Q$, then $P^{-1} \sbs Q^{-1}$.
\end{corr}
\begin{proof}
	Suppose that $P \sbs Q$. By Lemma~\ref{ch0:lattice}, get $Q = P \cup Q$. Hence,
	$Q^{-1} = (P \cup Q)^{-1} = P^{-1} \cup Q^{-1}$
	and $P^{-1} \sbs Q^{-1}$ by the same Lemma.
\end{proof}

\begin{exc}
	Prove that $(P \cap Q)^{-1} = P^{-1} \cap Q^{-1}$.
\end{exc}

\begin{exm}
	Suppose that $R \sbs A \times B$ for non-empty sets $A$ and $B$. Then $R^{-1} \neq \bar R$.
	
	Assuming the contrary, let $R^{-1} = \bar R$. First, we are to prove that $A \cap B \neq \void$.  If $R = \void$, then $R^{-1} = \void$. On the other hand, $\bar R = (A \times B) \setminus \void = A \times B \neq \void$. The contradiction implies $R \neq \void$, i.\,e.,\ $(a,b) \in R$ for some $a \in A$ and $b \in B$. Then $(b,a) \in R^{-1}$, whence $(b,a) \in (A \times B) \setminus R$. Hence, $b \in A$ and $b \in A \cap B$. 
	
	Let $x$ be an element of $A \cap B$. If $(x,x) \in R$, then $(x,x) \in R^{-1} = \bar R$, i.\,e.,\ $(x,x) \notin R$. The contradiction yields $(x,x) \notin R$. But then $(x,x) \in \bar R = R^{-1}$. We thus get $(x,x) \in R$ and a new contradiction. The only remaining assumption $R^{-1} = \bar R$ must be false.
\end{exm}

Let $P$ and $Q$ be arbitrary binary relations. The set 
$$Q \circ P = \{(a, c) \in \dom P \times \rng Q \mid \exists b\, \bigl( a P b \wedge b Q c \bigr)\}$$
is then called the \emph{composition} of the relations $P$ and $Q$. Clearly, $Q \circ P \sbs A \times C$ if $P \sbs A \times B$ and $Q \sbs B \times C$. On the other hand, we have
$$(a,c) \in Q \circ P \iff \exists b\, \bigl( a P b \wedge b Q c \bigr)$$
for \emph{any} pair $(a, c)$ since the right-hand side implies that $a \in \dom P$ and $c \in \rng Q$.

\begin{rem}
	Note that the relation $P$ `acts' first but comes second (from left to right) in the expression $Q \circ P$. This counter-intuitive convention is motivated by the traditional notation for the composition of functions (which we discuss later), where $(g \circ f)(x) = g(f(x))$.
\end{rem}

In a diagram, points $a$ and $c$ are connected by a $(Q \circ P)$-arrow iff there exists a two arrow path from $a$ to $c$ first going along a $P$-arrow from $a$ to some $b$ and then along a $Q$-arrow from $b$ to $c$. You can think about composition as the set of pairs of places connected by a route with a change.


\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{conv_comp.pdf}
	\caption{Conversion and composition.}
\end{figure}


\begin{exm} Another illustration. Let $A$ be a group of authors, $C$ be a group of critics, and $B$ be a set of books. Assume that $a P b$ means that \emph{Author $a$ has authored Book $b$}, while  $b Q c$ means that \emph{Book $b$ is praised by Critic $c$}. Then  $a (Q \circ P) c$ means that Author $a$ has authored a book praised by Critic $c$.
\end{exm}

\begin{exc}
	Let $A$ be a group of men and let $a P b$ mean that \emph{$a$ is a son of $b$}. Which concepts are expressed by the relations $P\circ P$, $P^{-1}$, $P^{-1} \circ P$, and $P \circ P^{-1}$?
\end{exc}

\begin{exm}
	On $\N$, we have ${\leq}\circ{<} = \{(m,  n) \in \N^2 \mid \exists k\in \N\, (m < k \wedge k \leq n)\}$. If $m < k$ and $k \leq n$, then $m < n$. If $m < n$, then we get $m < k$ and $k \leq n$ by assigning $k = n$. Hence, ${\leq}\circ{<} = {<}$.
\end{exm}

As we shall see, composition is similar to numerical multiplication in many respects. For example, there is a `zero' element.
\begin{exc}
	Prove that $R \circ \void = \void \circ R = \void$.
\end{exc}

Also, there is an analogue for unity, or `neutral element' w.\,r.\,t. composition (a quantity does not change if you multiply it by unity). For relations between $A$ and $B$, we have  distinct `left' and `right' unities indeed.
\begin{exm}
	If $R \sbs A \times B$, then $R \circ \id_A = R$ and $\id_B \circ R = R$.
	
	Let us prove the first equality. If $(x,y) \in R$ then $x \in A$ and $(x,x) \in \id_A$. Thus we have $(x,z) \in \id_A$ and $(z,y) \in R$ for a suitable $z$---namely, for $z = x$. Hence $(x,y) \in R \circ \id_A$ by the definition of composition. For the other direction, suppose that $(x,y) \in R \circ \id_A$. By the same definition, we conclude that $(x,z) \in \id_A$ and $(z,y) \in R$ for some $z$. But the former means that $z = x$, whence $(x,y) \in R$.
\end{exm}

\begin{thm}[Composition associativity]\label{ch0:rel_comp}
	Let $P, Q, R$ be arbitrary binary relations. Then
	$$R \circ (Q \circ P) = (R \circ Q) \circ P.$$
\end{thm}
\begin{proof} For any pair $(a,d)$, obtain:
	$$
	\begin{array}{rcl}
		(a, d) \in R \circ (Q \circ P) &\iff& \exists c\, \bigl( a(Q \circ P)c \wedge cRd \bigr)\\
		&\iff&\exists c\, \bigl( \exists b\, ( aPb \wedge bQc ) \wedge cRd \bigr)\\
		&\iff& \exists c\, \exists b\, \bigl(  (aPb \wedge bQc)  \wedge cRd \bigr)\\
		&\iff& \exists b\, \exists c\, \bigl(  aPb \wedge (bQc  \wedge cRd) \bigr)\\
		&\iff&\exists b\, \bigl( aPb \wedge \exists c\, (bQc  \wedge cRd ) \bigr)\\
		&\iff&\exists b\, \bigl( aPb \wedge b(R \circ Q)d ) \bigr)\\
		&\iff& (a,d) \in (R \circ Q)\circ P.\\
	\end{array}
	$$
	Here, we have observed that $c R d$ does not depend on $b$ (and does not mention at all); therefore the quantifier on $b$ does not change the meaning of this statement, so the later can be safely placed in or out of the scope of that quantifier.
	
\end{proof}
\mcomm{For this argument, the Instructor might wish to explain the logical equivalences $\exists x\, (A(x) \wedge B) \equiv \exists x\, A(x) \wedge B$ (where $B$ does not mention $x$) and $\exists x \exists y\, A \equiv \exists y \exists x\, A$ in more detail.}
\begin{rem}
	Associativity allows us to omit parentheses in expressions like $P_1 \circ P_2 \circ \ldots \circ P_n$ as it does not matter how one places them.
\end{rem}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{comp_assoc.pdf}
	\caption{Proving Theorem~\ref{ch0:rel_comp}.}
\end{figure}

\noindent Conversion is \emph{somewhat} similar to taking a multiplicative inverse for numbers: one has $\frac{1}{xy} = \frac{1}{y} \cdot \frac{1}{x}$.

\mcomm{The Instructor might wish to compare conversion to matrix transposition or string reversion as well.}
\begin{thm}\label{ch0:comp_inv}
	Let $P, Q$ be arbitrary binary relations. Then $(Q \circ P)^{-1} = P^{-1} \circ Q^{-1}$.
\end{thm}
\begin{proof}
	For any pair $(a,c)$, obtain:
	$$
	\begin{array}{rcl}
		(a, c) \in (Q \circ P)^{-1} &\iff& (c, a) \in Q \circ P\\
		&\iff&\exists b\, (cPb \wedge bQa)\\
		&\iff&\exists b\, (a Q^{-1} b \wedge b P^{-1} c)\\
		&\iff&(a, c) \in P^{-1} \circ Q^{-1}.
	\end{array}
	$$
\end{proof}
\begin{rem}
	For numbers, one also has $x \cdot \frac{1}{x} = x$. Nevertheless, a similar statement fails for binary relations. Indeed, consider the relation $R = \{ (1, 2) \}$ on the set $A = \{1, 2\}$. Clearly, $R^{-1} = \{ (2,1) \}$, $R \circ R^{-1} = \{ (2,2)\}$, and $R^{-1} \circ R = \{ (1, 1)\}$. Neither of the relations $R \circ R^{-1}$ and $R^{-1} \circ R$ equals $\id_A$, which is the analogue of unity as we have seen.
	
	For this reason, we have preferred to call $R^{-1}$ the \emph{converse} rather than the \emph{inverse} of the relation $R$.
\end{rem}
\begin{exc}
	The analogues of the following statements hold for numerical multiplication. Do they hold for arbitrary binary relations?
	\begin{enumerate}
		\item $P \circ Q = Q \circ P$;
		\item if $P \neq \void$ and $P \circ Q = P \circ R$, then $Q = R$.
	\end{enumerate}
\end{exc}


\begin{lemma}\label{ch0:ex11}
	Let $R, P, Q $ be arbitrary binary relations. Then
	\begin{enumerate}
		\item $(P \cup Q) \circ R = (P \circ R) \cup (Q \circ R)$;
		\item $(P \cap Q) \circ R \sbs (P \circ R) \cap (Q \circ R)$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For any pair $(a,c)$,
	$$
	\begin{array}{rcl}
		(a, c) \in (P \cup Q) \circ R &\iff& \exists b\, \bigl( a R b \wedge (b,c) \in P \cup Q \bigr)\\
		&\iff& \exists b\, \bigl( a R b \wedge ( b P c\ \mbox{or}\ b Q c) \bigr)\\
		&\iff& \exists b\, \bigl( (a R b \wedge  b P c)\ \mbox{or}\ (a R b \wedge b Q c) \bigr)\\
		&\iff& \exists b\, \bigl( a R b \wedge  b P c \bigl)\ \mbox{or}\ \exists b\, \bigl(a R b \wedge b Q c \bigr)\\
		&\iff& (a,c) \in P \circ R\ \mbox{or}\ (a,c) \in Q \circ R\\
		&\iff& (a,c) \in (P \circ R) \cup (Q \circ R).\\
	\end{array}
	$$
	Here we have applied a certain law of logic. Indeed, $\exists x\, (A \vee B)$ (``there exists a  unicorn either black or tame'') iff $\exists x\, A \vee \exists x\, B$ (``there exists a black unicorn or there exists a tame unicorn'').
	
	Consider the second statement.
	$$
	\begin{array}{rcl}
		(a, c) \in (P \cap Q) \circ R &\iff& \exists b\, \bigl( a R b \wedge (b,c) \in P \cap Q \bigr)\\
		&\iff& \exists b\, \bigl( a R b \wedge ( b P c \wedge b Q c) \bigr)\\
		&\iff& \exists b\, \bigl( (a R b \wedge  b P c) \wedge (a R b \wedge b Q c) \bigr)\\
		&\ply& \exists b\, \bigl( a R b \wedge  b P c \bigl) \wedge \exists b\, \bigl(a R b \wedge b Q c \bigr)\\
		&\iff& (a,c) \in P \circ R \wedge (a,c) \in Q \circ R\\
		&\iff& (a,c) \in (P \circ R) \cap (Q \circ R).\\
	\end{array}
	$$
	Thus, from $(a,c) \in (P \cap Q) \circ R$, it follows that $(a,c) \in (P \circ R) \cap (Q \circ R)$. We have applied the fact that $\exists x\, (A(x) \wedge B(x))$ (``there exists a unicorn both black and tame'') implies $\exists x\, A(x) \wedge \exists x\, B(x)$ (``there exists a black unicorn and there exists a tame unicorn''). The reverse implication does not hold in general.
\end{proof}

\begin{exm}
	Here is an example where the last inclusion of Lemma~\ref{ch0:ex11} cannot be strengthened to become an equality. Consider relations $R = \{(0,1), (0,2)\}$, $Q = \{(1,3)\}$ and $P = \{(2,3)\}$. Clearly, $(0,3) \in (P \circ R) \cap (Q \circ R)$, while $P \cap Q$ and $(P\cap Q) \circ R$ are empty.
\end{exm}

\begin{exm}
	Suppose that $P \sbs Q$. Then $P \circ R \sbs Q \circ R$.
	
	Indeed, we get $Q = P \cup Q$, whence
	$Q \circ R = (P \cup Q) \circ R = (P \circ R) \cup (Q \circ R) \supseteq P \circ R$ by Lemma~\ref{ch0:ex11}.
\end{exm}

\begin{exc}
	Apply the above example to derive Claim 2 of Lemma~\ref{ch0:ex11}.
\end{exc}


\begin{exm}\label{ch0:exm17}
	By Lemma~\ref{ch0:ex11} and Theorem~\ref{ch0:comp_inv}, it holds that
	\begin{multline*}
		(R \circ (P \cup Q))^{-1} = (P \cup Q)^{-1} \circ R^{-1} =\\
		=(P^{-1} \cup Q^{-1}) \circ R^{-1} = (P^{-1} \circ R^{-1}) \cup (Q^{-1} \circ R^{-1}) =\\
		=(R \circ P)^{-1} \cup (R \circ Q)^{-1} = ((R \circ P) \cup (R \circ Q))^{-1}.
	\end{multline*}
	Hence,
	$$
	R \circ (P \cup Q) = ((R \circ (P \cup Q))^{-1})^{-1} = ((R \circ P) \cup (R \circ Q))^{-1})^{-1} = (R \circ P) \cup (R \circ Q).
	$$
	One can derive $R \circ (P \cap Q) \sbs (R \circ P) \cap (R \circ Q)$ in a similar manner.
\end{exm}

\paragraph{Image of set.} Let $R$ be a binary relation and $X$ be some set. Then the set
$$R[X] = \{ b \in \rng R \mid \exists a \in X\, a R b \}$$
is called the \emph{image of $X$ under $R$} (or simply the \emph{$R$-image of $X$}). It is easy to see that
$$b \in R[X] \iff \exists a \in X\, a R b$$
for the right-hand side clearly implies $b \in \rng R$. The set $R^{-1}[X]$ (that is, the image of $X$ under $R^{-1}$) is usually called the \emph{preimage of $X$ under $R$}. Clearly,
$$R ^{-1}[X] = \{ a \in \rng R^{-1} \mid \exists b \in X\, b R^{-1} a \} =  \{ a \in \dom R \mid \exists b \in X\, a R b \}.$$

Considering arrow diagrams, we see that $R[X]$ is the set of endings of $R$-arrows beginning somewhere in $X$, while $R^{-1}[X]$ is the set of beginnings of $R$-arrows with endings in $X$.

\begin{rem}
	If $R \sbs A \times B$, then $\dom R = R^{-1}[B]$ и $\rng R = R[A]$.
\end{rem}

\begin{exm}
	Let $A$ be a set of cities and let $xRy$ mean that there is some route from $x$ to $y$ along existing highways for a relation $R \sbs A^2$. Then $R[X]$ is the set of cities accessible from at least one city that belongs to $X$.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{rel_img.pdf}
	\caption{An image and a pre-image.}
\end{figure}

\begin{exm}
	Let us show that $R[X \cup Y] = R[X] \cup R[Y]$.
	For any set $b$, obtain
	$$
	\begin{array}{rcl}
		b \in R[X \cup Y] &\iff& \exists a\, \bigl(a \in X \cup Y \wedge a R b \bigr)\\
		&\iff& \exists a\, \bigl((a \in X \vee a \in Y) \wedge a R b \bigr)\\
		&\iff& \exists a\, \bigl((a \in X \wedge a R b) \vee (a \in Y \wedge a R b)\bigr)\\
		&\iff& \exists a\, \bigl(a \in X \wedge a R b) \vee \exists a\, (a \in Y \wedge a R b)\\
		&\iff& b \in R[X] \vee b \in R[Y]\\
		&\iff& b \in R[X] \cup R[Y].\\
	\end{array}
	$$
\end{exm}

\begin{exc}\label{ch0:img_monot}
	Prove that $X \sbs Y$ implies $R[X] \sbs R[Y]$.
\end{exc}

\begin{exc}
	Prove that $R[\void] = \void$.
\end{exc}

\begin{exm}\label{ch0:exm25}
	$R[X \cap Y] \sbs R[X] \cap R[Y]$.
	
	From $X \cap Y \sbs X$ and $X \cap Y \sbs Y$, it follows that $R[X \cap Y] \sbs R[X]$ and $R[X \cap Y] \sbs R[Y]$ by Exercise~\ref{ch0:img_monot}, which implies the required inclusion.
	
	The reverse inclusion is not yet necessary. E.\,g., consider the relation $R = \{(0,2), (1,2)\}$. For $X = \{0\}$ and $Y = \{1\}$, we have $R[X] = R[Y] = R[X] \cap R[Y]  = \{2\}$ but $R[X \cap Y] = R[\void] = \void$.
\end{exm}

\begin{exm}
	$(R \circ Q)[X] = R[Q[X]]$.
	
	For any $c$, we have
	$$
	\begin{array}{rcl}
		c \in (R \circ Q)[X] &\iff& \exists a\, \bigl( a \in X \wedge (a,c) \in R \circ Q \bigr)\\
		&\iff& \exists a\, \bigl( a \in X \wedge \exists b\, ( a Q b \wedge b R c) \bigr)\\
		&\iff& \exists a\, \exists b\,\bigl( (a \in X \wedge a Q b) \wedge b R c \bigr)\\
		&\iff& \exists b\, \bigl( \exists a\, (a \in X \wedge a Q b) \wedge b R c \bigr)\\
		&\iff& \exists b\, \bigl( b \in Q[X] \wedge b R c \bigr)\\
		&\iff& c \in R[Q[X]].\\
	\end{array}
	$$
\end{exm}
The above equation gives another reason for that strange ``acting first is written last'' rule for composition as long as we prefer the notation $R[X]$ to $[X]R$ for image. 
\section{Functions}
Function is one of the most basic concepts in mathematics. Loosely speaking, this notion expresses the idea of \emph{dependence} of one `quantity' on another, as in the laws of nature. This idea can be modeled in set-theoretic framework by using binary relations of a special kind.

A binary relation $R \sbs A \times B$ is called:
\begin{enumerate}
	\item \emph{functional} iff $\forall x \forall y \forall z\, ( (x R y \wedge x R z) \ply y = z )$;
	\item \emph{injective} iff $\forall x \forall y \forall z\, ( (y R x \wedge z R x) \ply y = z )$;
	\item \emph{total for a set $Z$} iff $\forall x \in Z\, \exists y\; x R y$;
	\item \emph{surjective for a set $Z$} iff $\forall x \in Z\, \exists y\; y R x$.
\end{enumerate}
By default, we say that $R$ is \emph{total} if $R$ is total for $A$ and we say that $R$ is \emph{surjective} when $R$ is surjective for $B$.

\begin{exm}
	Let $R = \{(0,1), (0,2), (1,1), (3,4)\}$. The relation $R$ is not functional as $0 R 1$ and $0 R 2$ hold but $1 \neq 2$. That is, we have two $R$-arrows sharing their beginning whose endings are yet separate. Nor is the relation $R$ injective as $(0, 1)$ and $(1, 1)$ are two $R$-arrows with same end but different beginnings. Clearly, $R$ is total for $\dom R = \{0,1,3\}$ and surjective for $\rng R = \{1,2,4\}$ but $R$ is neither total nor surjective for its field $\{0,1,2,3,4\}$ as there is no $R$-arrow beginning at $2$ nor an $R$-arrow ending at $0$.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{func_etc.pdf}
	\caption{(a) $R$ is \emph{not} functional when $y \neq z$; (a) $R$ is \emph{not} injective when $y \neq z$; (c) $R$ is total for $Z$; (d) $R$ is surjective for $Z$.}
\end{figure}


\begin{exm}
	The relation ${<}$ on $\N$ is total since for any natural $m$ there exists a greater number $n$. But ${<}$ is not functional as such a number $n$ is not unique. This relation is not injective for $0 < 2$, $1 < 2$ but $0 \neq 1$, nor surjective as $n < 0$ holds for no $n$. 
	
	Let $R = \{(x,y) \in \R_{+} \times \R \mid x = y^2 \}$ for $\R_+ = \{a \in \R  \mid a > 0\}$. Then the relation $R$ is total but not surjective ($xR0$ is not possible). $R$ is injective as from $x = y^2$ and $z = y^2$, it follows that $x = z$, but $R$ is not functional for $(1, 1) \in R$ and $(1, -1) \in R$.
\end{exm}

\begin{rem}
	Clearly, $R$ is total for $Z$ iff $Z \sbs \dom R$ and $R$ is surjective for $Z$ iff $Z \sbs \rng R$.
\end{rem}


The notions of functionality and injectivity (as well as those of totality and surjectivity) are dual to each other.
\begin{lemma}\label{ch0:fnc_inv}\rule{1pt}{0pt}
	\begin{enumerate}
		\item $R\ \mbox{is functional} \iff R^{-1}\ \mbox{is injective}$; $R\ \mbox{is injective} \iff R^{-1}\ \mbox{is functional}$;
		\item $R\ \mbox{is total for}\ Z \iff R^{-1}\ \mbox{is surjective for}\ Z$; $R\ \mbox{is surjective for}\ Z \iff R^{-1}\ \mbox{is total for}\ Z$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	By definition,
	\begin{multline*}
		R\ \mbox{is functional} \iff \forall x \forall y \forall z\, ( (x R y \wedge x R z) \ply y = z ) \iff\\
		\forall x \forall y \forall z\, ( (y R^{-1} x \wedge z R^{-1} x) \ply y = z )  \iff R^{-1}\ \mbox{is injective}.
	\end{multline*}
	Now, we apply the statement just proved to $R^{-1}$:
	$$
	R\ \mbox{is injective} \iff (R^{-1})^{-1}\ \mbox{is injective} \iff R^{-1}\ \mbox{is functional}.
	$$
	The second claim can be proved similarly.
\end{proof}


\noindent All four notions are invariant w.\,r.\,t.\ the relation composition.
\begin{lemma}\label{ch0:fnc_cmp} Let $Q \sbs A \times B$ and $R \sbs B \times C$.  If $Q$ and $R$ are functional (injective, total, surjective), then so is $R \circ Q$.
\end{lemma}
\begin{proof}
	Firstly, let us check functionality. Suppose that $x(R \circ Q)y$ and $x(R \circ Q)z$. Then there exist $u,v\in B$ such that $xQu$, $xQv$, $u R y$, and $v R z$. As $Q$ is functional, obtain $u = v$, whence $u R y$ and $u R z$. Thus, $y = z$ by functionality of $R$.
	
	We use duality (Lemma~\ref{ch0:fnc_inv}) to check injectivity. As $Q = (Q^{-1})^{-1}$ and $R = (R^{-1})^{-1}$ are injective, the relations $Q^{-1}$ and $R^{-1}$ are functional as well as $Q^{-1} \circ R^{-1} = (R \circ Q)^{-1}$ is. Hence, $R \circ Q$ is injective.
	
	Now, assume that $Q$ is total for $A$ and $R$ is total for $B$. We have to show that $R \circ Q$ is total for $A$. For any $x \in A$, there exists some $u$ such that $x Q u$. As $Q \sbs A \times B$, we have $u \in B$. Hence, there exists some $y$ such that $u R y$. So, $x Q u$ and $u R y$ for a certain $u$, that is, $x(R \circ Q)y$.
	
	Surjectivity can be easily proved by Lemma~\ref{ch0:fnc_inv} now.
\end{proof}

\begin{exm}
	Let a relation $R$ be functional. Then $R^{-1}[X \cap Y] = R^{-1}[X] \cap R^{-1}[Y]$ for every sets $X$ and $Y$.
	
	The inclusion from left to right is due to Example~\ref{ch0:exm25}. For the opposite direction, suppose that $a \in R^{-1}[X]$ and $a \in R^{-1}[Y]$. Then there are some $b \in X$ and $c \in Y$ such that $bR^{-1}a$ and $cR^{-1}a$, i.\,e., $a R b$ and $a R c$. Since $R$ is functional, we get $b = c$. Then $b \in X \cap Y$; hence $a \in R^{-1}[X \cap Y]$.
\end{exm}

\begin{exm}
	Let a relation $R \sbs A \times B$ be total. Then $X \sbs R^{-1}[R[X]]$ for any $X \sbs A$.
	
	Indeed, let $a \in X$. By totality, get $a R b$ for some $b \in B$. See that $b \in R[X]$ and $b R^{-1} a$, whence $a \in R^{-1}[R[X]]$.
	
	Is the converse inclusion necessary? No, it is not. Indeed, consider the sets $A = \{0,1\}$, $B = \{2\}$, $R = \{(0,2), (1,2)\}$, and $X = \{1\}$. Then $R[X] = \{2\}$ and $R^{-1}[\{2\}] = \{0,1\} \not\sbs X$. Moreover, the relation $R$ is functional in this example.
\end{exm}

\paragraph{Functions and their values} We are ready now to give a set-theoretic definition of function. Let  $A$ and $B$ be some sets. A binary relation $f \sbs A \times B$ is called a \emph{function from the set $A$ to the set $B$} iff $f$ is functional and total (for $A$). The symbol $f\colon A \to B$ reads: ``$f$ is a function from $A$ to $B$''. Clearly, $\dom f = A$ and $\rng f \sbs B$.

\begin{exm}
	The relation $f = \{(x,y) \in \R_+ \times \R \mid x = y^2 \wedge y > 0 \}$ is the well-know numeric function from $\R_+$ to $\R$ called ``the principal square root'' (of a positive number). As the principal square root of a positive number is positive, the statement $f\colon \R_+ \to \R_+$ holds as well.
	
	The relation $\{ (1,2), (2,2) \}$ is a function from $\{1, 2\}$ to $\{2\}$. In fact, every functional relation is a function from its domain to its range. For each set $A$, the relation $\id_A$ is a function from $A$ to $A$.
	
	The relation $\sin = \{ (x, y) \in \R^2 \mid \mbox{the sine of $x$ equals $y$} \}$ on the set $\R^2$ is a function from $\R$ to $\R$ and also from $\R$ to the segment $[-1, 1]$. Notice that we have identified the well-known sine function with its \emph{graph} (that is, the sinusoid, the curve of sines)---a part of the plane, hence, a binary relation on $\R$. Here lies the main idea of modeling functions in set theory.
	
	The relation $<$ on the set $\N$ is not a function for it is not functional ($1 < 2$ and $1 < 3$ while $2 \neq 3$). The relation $\tan = \{ (x, y) \in \R^2 \mid \mbox{the tangent of $x$ equals $y$} \}$ is \emph{not} a function from $\R$ to $\R$ since there is no such $y$ that $\tan \frac{\pi}{2} = y$, so this relation is not total for $\R$. Being functional, $\tan$ is nevertheless a function from its domain to $\R$. They usually reserve the term \emph{partial function} for such cases but we will not use it.
\end{exm}

The main idea of a function is that it ``maps'' each element of its domain $A$ to a \emph{unique} element of $B$, so the latter ``depends on'' or ``is determined by'' the former solely. More formally, for every $x \in A$, there exists $y \in B$ such that $(x,y) \in f$ (by totality), while such $y$ is unique, i.\,e., for any $z$, it follows from $(x,z) \in f$ that $z = y$ (by functionality).

This allows to denote the element $y$ by the symbol $f(x)$ implying its dependence on $x$. In particular, $f(x) = f(x')$ when $x = x'$. The symbol $f(x)$ reads: ``the value of the function $f$ at the element $x$''. Clearly, this notation makes sense iff $x \in \dom f$.


\begin{exm}\label{ch0:void_fnc}
	Is it possible that $f\colon \void \to B$ for some set $B$?
	
	Suppose this holds. Then $f \sbs \void \times B = \void$, hence $f = \void$. On the other hand, the empty relation $f = \void$ is, indeed, functional, injective, and total for the set $\void$. Clearly, $\void$ is surjective for $B$ iff $B = \void$. Finally, $\void$ is the only function from $\void$ to $B$ for each $B$.
\end{exm}

Intuitively, a function with a fixed domain is fully determined by its values. In particular, to establish the equality of two such functions, it suffices to check if their respective values are equal at every point. Indeed, the following holds.
\begin{lemma}\label{ch0:tot_fnc_cmp}
	Let $f\colon A \to B$ and $g\colon C \to D$. Then
	$$f = g \iff A = C\ \wedge\ \forall x \in A\; f(x) = g(x).$$
\end{lemma}
\begin{proof}
	Assume that $f = g$. Then $A = \dom f = \dom g = C$. Consider an arbitrary $x \in A = C$. By totality, there exist such $y \in B$ and $z \in C$ that $(x,y) \in f$ and $(x,z) \in g$. By $f = g$, obtain $(x, y), (x, z) \in f$, whence $y = z$ by functionality. Thus, $f(x) = y = z = g(x)$.
	
	For the other direction, suppose that $f(x) = g(x)$ for all $x \in A = C$. Let $(x,y) \in f$. We have $x \in A$ and $f(x) = y$. On the other hand, $x \in C$ and $g(x) = f(x) = y$. Hence $(x,y) \in g$. The opposite inclusion is similar.
\end{proof}

\begin{rem}
	As we have already said, to define a function it suffices to describe its domain and value at each element thereof. In practice, this results in the following abbreviation. For example, they write $f\colon \N \to \N$, $f\colon n \mapsto n + 1$ instead of
	$$f = \{(n,m) \in \N^2 \mid m = n + 1 \}.$$
	Sometimes, the function defined is still `anonymous', as in the statement ``the function $x \mapsto x^2$ from $\R$ to $\R$ takes only non-negative values''. Such abbreviations should be used with care: say, the symbol  $a^2 \mapsto a$ does not define a function $\R \to \R$, since it puts both $1$ and $-1$ into correspondence to $1$. 
\end{rem}

\begin{rem}
	Obviously,  $\{f(a)\} = f[\{a\}]$ if $f\colon A \to B$ and $a \in A$.
\end{rem}

\mcomm{The students may have already defined composition of functions in their calculus course. So, it is important to show them the identity of `our' composition (when restricted to functions) with that of calculus.}
\begin{rem}
	Suppose that $f\colon A \to B$ and $g\colon B \to C$. From Lemma~\ref{ch0:fnc_cmp}, it follows that the composition $g \circ f$ is a function from $A$ to $C$. It is easy to see that $(g\circ f)(a) = g(f(a))$ for all $a \in A$.
	
	Indeed, let $b = (g \circ f)(a)$; then $(a, b) \in g \circ f$. Hence, $a\, f\, c$ and $c\, g\, b$ for some $c$. This implies $c = f(a)$ and $b = g(c) = g (f(a))$, as required.
	
	The equation $(g\circ f)(a) = g(f(a))$ is the main reason for the `strange' definition of relation composition, where the rightmost relation `acts' first. The natural alternative would be to denote the value of a function at $a$ as $(a)f$ but this seems way too radical.
\end{rem}

\begin{exm}
	Let $f\colon A \to B$ and $g\colon A \to B$. Then ${f \cap g}\colon A \to B$ iff $f = g$. That is, the intersection of two functions with the same domain can be a function when trivial only.
	
	If $f = g$, then $f \cap g = f \colon A \to B$. Assume now that ${f \cap g}\colon A \to B$. Let $(a,b) \in f$. Since $f \cap g$ is total, there exists some $c \in B$ with $(a,c) \in f \cap g$, whence $(a,c) \in f$ and $(a,c) \in g$. As $f$ is functional, we have $b = c$; hence, $(a,b) \in g$. Thus, $f \sbs g$. The other inclusion is similar.
\end{exm}

\begin{exc}
	Consider a similar statement for the union of two functions. Does it hold true?
\end{exc}

For arbitrary sets $A$ and $B$, the set $\{ f \in \mP(A \times B) \mid f\ \mbox{is a function from $A$ to $B$}\}$ is denoted by $B^A$. According to Example~\ref{ch0:void_fnc}, $B^\void = \{ \void \}$ (which is the set of exactly \emph{one} element) for any $B$ just like $n^0 = 1$ in the case of numerical exponentiation. More parallels between the set $B^A$ and numerical powers will be drawn later.

\paragraph{Bijections and friends.} Now, let us consider some special classes of functions. A function $f\colon A \to B$ is an \emph{injection from $A$ to $B$} if $f$ is injective. Likewise, if $f$ is surjective (for $B$), it is called a \emph{surjection from $A$ to $B$}. Finally, if the function $f\colon A \to B$ is both injective and surjective, it is called a \emph{bijection from $A$ to $B$} (or a \emph{one-to-one correspondence between $A$ and $B$}, or a \emph{bijective} function from $A$ to $B$).

\begin{exm}
	The relation $\id_A$ is a bijection from $A$ to $A$ for any set $A$.
\end{exm}

\begin{exm}
	The function $x \mapsto e^x$ is an injection $\R \to \R$ and a bijection $\R \to \R_+$. The function $x \mapsto x^2$ is a surjection $\R \to (\R_+ \cup \{0\})$, but not an injection. Likewise, the function $(n,m) \mapsto n + m$ is a surjection $\N^2 \to \N$, but not an injection. 
\end{exm}

\begin{rem}
	Given a function $f\colon A \to B$, $f$ is injective iff it follows that $x = y$ from $f(x) = f(y)$, for any $x$ and $y$. Also, $f$ is surjective (for $B$) iff for every $y \in B$, there exists such $x$ that $f(x) = y$.
\end{rem}
\mcomm{This point may be not that obvious for some students. It makes sense to give a detailed proof.}

\begin{exc}
	Let $f\colon A \to B$ and $g\colon B \to C$. Prove that $f$ is an injection if $g\circ f$ is, and $g$ is a surjection when $g \circ f$ is such.
\end{exc}

\begin{lemma}\label{ch0:bi_inv}
	A relation $R \sbs A \times B$ is a bijection from $A$ to $B$ iff $R^{-1}$ is a bijection from $B$ to $A$.
\end{lemma}
\begin{proof} By Lemma~\ref{ch0:fnc_inv}.
\end{proof}

\begin{exc}
	From $f\colon A \to B$ and $f^{-1}\colon B \to A$ derive that $f$ is a bijection from $A$ to $B$.
\end{exc}

Sometimes, it proves convenient to describe functionality, injectivity, etc.\ in terms of operations on binary relations.
\begin{lemma}\label{rel:fnc_algebra}
	Let $R \sbs A \times B$. Then
	\begin{enumerate}
		\item $R\ \mbox{is functional} \iff R \circ R^{-1} \sbs \id_B$;
		\item $R\ \mbox{is injective} \iff R^{-1} \circ R \sbs \id_A$;
		\item $R\ \mbox{is surjective} \iff \id_B \sbs R \circ R^{-1}$;
		\item $R\ \mbox{is total} \iff \id_A \sbs R^{-1} \circ R$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Suppose that $R$ is functional and $(x, y) \in R \circ R^{-1}$. Then $x, y \in B$, $x R^{-1} z$, and $z R y$ for some $z$. By functionality, $z R x$ and $z R y$ imply $x = y$, whence $(x, y) \in \id_B$. For the other direction, assume the inclusion $R \circ R^{-1} \sbs \id_B$ and $z R x$ and $z R y$ for arbitrary $x, y, z$. Clearly, $x R^{-1} z$, whence $(x, y) \in R \circ R^{-1} \sbs \id_B$; so, $x = y$.
	
	For injectivity, we shall apply the first claim to the relation $R^{-1}$ (notice changing $A$ to $B$ and vice versa). Clearly,
	$$R\ \mbox{is injective} \iff R^{-1}\ \mbox{is functional} \iff R^{-1} \circ (R^{-1})^{-1} \sbs \id_A \iff R^{-1} \circ R \sbs \id_A.$$
	
	Now, let $R$ be surjective (for $B$). Consider an arbitrary $x \in B$. By surjectivity, there exists $y$ such that $y R x$, whence $x R^{-1} y$. These two imply $(x, x) \in R  \circ R^{-1}$. For the other direction, assume $\id_B \sbs R \circ R^{-1}$ and consider an arbitrary $x \in B$. Then $(x, x) \in R \circ R^{-1}$; so there exists $y$ such that $x R^{-1} y$ and $y R x$. Each of these two statements implies surjectivity of $R$.
	
	For totality, we have
	$$R\ \mbox{is total for}\ A \iff R^{-1}\ \mbox{is surjective for}\ A \iff \id_A \sbs R^{-1} \circ (R^{-1})^{-1} \iff \id_A \sbs R^{-1} \circ R$$
	by applying the previous claim to $R^{-1}$ (and changing $B$ to $A$, of course).
\end{proof}

\begin{corr}\label{ch0:bi_cmp}
	The relation $R \sbs A \times B$ is a bijection from $A$ to $B$ iff
	$$R^{-1}\circ R = \id_A\quad \mbox{and}\quad R \circ R^{-1} = \id_B.$$
\end{corr}
In particular, we see that for a \emph{bijection} on a set $A$, the converse relation is indeed the \emph{inverse} w.\,r.\,t.\ composition and $\id_A$ as the unity. The set of bijections on a given set is thus similar to the set of (say) rational non-zero numbers, where for every $x$ one has $\frac{1}{x}$ such that $x \cdot \frac{1}{x} = 1 = \frac{1}{x} \cdot x$.


\section{Equivalent Sets}
\mcomm{The main goal of this section is to define set equivalence and embedding formally. The Instructor should try to persuade the students that these notions are natural models for comparing sets by their `sizes'. Here we present some basic facts (including Cantor---Schr\"{o}der---Bernstein Theorem) applicable to both the finite and infinite cases.}

Bijections are of utmost importance in mathematics since they allow to formalize the intuitive notion of the `number of elements' in a set.

Our intuition does not serve us well when the set we consider is infinite. Indeed, not every natural number is a square of a natural (like $9 = 3^2$ or $25 = 5^2$ are). So the set of squares is a proper subset of $\N$. Intuitively, `a part is less than the whole', hence the `number' (amount, quantity) of squares should be less than that of all natural numbers. On the other hand, each natural number has its `own' square, which differs from the square of any other natural. That is, there are at least as many squares as there are natural numbers. Thus, the `number' of squares is greater or equal to that of all naturals.

This is known as \emph{Galileo's Paradox}. The first step to tame our paradox-bearing intuition is to avoid  mentioning a `number of elements' as some distinct entity\footnote{Although, this can be redefined rigorously at some later stage of set-theoretic developments.}. Rather than that, we will say that \emph{two sets have the same number of elements} or \emph{one set has no more elements than the other}.

Formally, we say that sets $A$ and $B$ are \emph{equivalent} (or \emph{equinumerous}) iff there exists a bijection from $A$ to $B$. Then we write $A \sim B$ or $A \stackrel{f}{\sim} B$ if $f$ is such a bijection.

Intuitively, equivalent sets must have the same number of elements, must be of the same `size'. How can one show that this \emph{formal} definition is adequate to the \emph{intuitive} idea? (At least, any `number' is absent from the former, which clearly violates our intuition\dots) Again, we shall do that by \emph{proving} that equivalent sets `behave' the same way we would expect from ``sets of the same size''. This sounds much like the `axiomatic method' we have employed for building a theory of sets.

Of course, the formal notion is not the same as the intuitive one. Indeed, it is easy to see that $\N$ is equivalent to the set of squares from Galileo's Paradox. Hence, a proper part is not necessarily less than the whole when infinite sets are involved. This fact is not very intuitive; one the other hand, we have got rid of the paradox!

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{eq_sets.pdf}
	\caption{Equivalent sets: $A \stackrel{f}{\sim} B$.}
\end{figure}

Clearly, $A$ must be of the same size as $A$; if $A$ has as many elements as $B$, then $B$ must have the same number of elements as $A$ does, etc. These intuitive expectations are indeed met by set equivalence.
\begin{lemma}\label{ch0:l157} For any sets $A, B, C$,
	\begin{enumerate}
		\item $A \sim A$;
		\item if $A \sim B$, then $B \sim A$;
		\item if $A \sim B$ and $B \sim C$, then $A \sim C$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Clearly, $A \stackrel{\id_A}{\sim} A$. If $A \stackrel{f}{\sim} B$, then $B \stackrel{f^{-1}}{\sim} A$ by Lemma~\ref{ch0:bi_inv}. If $A \stackrel{f}{\sim} B$ and $B \stackrel{g}{\sim} C$, then $A \stackrel{g \circ f}{\sim} C$ by Lemma~\ref{ch0:fnc_cmp}.
\end{proof}

\begin{rem}\label{ch0:inj_img}
	Let $f\colon A \to B$ be an injection. Then $A \stackrel{f}{\sim} f[A]$ and, in general, $X \sim f[X]$ for each $X \sbs A$.
\end{rem}

\begin{exm}\label{ch0:N_sq}
	Let us show that $\N^2 \sim \N$ by constructing an appropriate bijection. This fact is quite important for computer science since it makes possible to encode a finite sequence of numbers (e.g., a program code) with one natural number.
	
	Define a function $f$ such that $f(m,n) = 2^m(2n + 1) - 1$ for all $(m,n) \in \N^2$. If $f(m,n) = f(m',n')$, then $2^m(2n + 1) = 2^{m'}(2n' + 1)$. Suppose that $m \neq m'$. W.\,l.\,o.\,g.\footnote{\emph{Without loss of generality}, that is, the cases remaining are similar to the cases being considered.}, assume $m < m'$. Then $2n + 1 = 2^{m' - m}(2n' + 1)$. Here the left-hand side is odd, whereas the right-hand side is even. By contradiction, $m = m'$. Hence, $2n + 1 = 2n' + 1$ and $n = n'$. Thus, $f$ is an injection.
	
	Now, let us check if $f$ is surjective. Assume that there is some \emph{positive} natural number which \emph{cannot}  be presented in the form $2^m (2n + 1)$. Consider the \emph{least} such number $k$. The number $k$ is even, for otherwise it would be of the form $2^0 (2n + 1)$. So, $k = 2k'$ for suitable $k' < k$. By our choice of $k$, it holds that $k' = 2^{m'}(2n' + 1)$ for some $m',n' \in \N$. But then $k = 2^{m' + 1}(2n' + 1)$. A contradiction. Thus, every positive natural is of the form $f(m,n) + 1$. Hence every natural equals $f(m,n)$ for some numbers $m,n \in \N$.
\end{exm}

It is clear that there exist non-equivalent sets. For example, $\void \nsim \{ \void \}$ as the only function $\void$ from $\void$ to $\{ \void \}$ is not surjective. Actually, for every set $A$ there exists a non-equivalent set $\mP(A)$, which is `strictly greater' in `number of elements' as we shall see a little bit later.
\begin{thm}[Cantor]\label{L9:t_cantor}
	For every set $A$, $A \nsim \mP(A)$.
\end{thm}
\begin{proof}
	Assume the contrary and let $\phi$ be a bijection from $A$ to $\mP(A)$. Consider the set
	$$X = \{ a \in A \mid a \notin \phi(a) \}.$$
	Since $X \in \mP(A)$ and $\phi$ is surjective, there must be some $a_0 \in A$ such that $\phi(a_0) = X$. If $a_0 \in X$, then $a_0 \notin \phi(a_0) = X$. The contradiction shows that $a_0 \notin X = \phi(a_0)$, which implies $a_0 \in X$ by the definition of $X$. So, our first assumption must be false: no such bijection $\phi$ is possible.
\end{proof}

\begin{rem}
	We have proved more indeed: there is no surjection from $\mP(A)$ to $A$ for any set $A$.
\end{rem}

As we have already seen, $A \times B \neq B \times A$ generally. But if one replaces ${=}$ with ${\sim}$, Cartesian product behaves much like the familiar numerical product; moreover, the operation $B^A$ becomes similar to numerical exponentiation.

\mcomm{The following theorem can spare a lot of effort when proving set equivalence as Example~\ref{eq_sets:big_exm} shows. So, it is recommended to highlight this result and make the students remember its statement. It depends on the audience whether it is worth a detailed proof.}
\begin{thm}\label{ch0:bi_power}
	For any sets $A, B, C$,
	\begin{enumerate}
		\item if $A \sim B$, then $A \times C \sim B \times C$, $A^C \sim B^C$ and $C^A \sim C^B$;
		\item $A \times B \sim B \times A$;
		\item $(A \times B) \times C \sim A \times (B \times C)$;
		\item $(A\times B)^C \sim A^C \times B^C$;
		\item $(C^B)^A \sim C^{A \times B}$.
	\end{enumerate}
\end{thm}
\begin{proof}[Proof sketch.]
	We omit the proof, but give some directions. For the first statement, given that $A \stackrel{\phi}{\sim} B$, one can consider the bijections $(a,c) \mapsto (\phi(a),c)$ from $A \times C$ to $B \times C$, $f \mapsto \phi \circ f$ from $A^C$ to $B^C$, and $f \mapsto f \circ \phi^{-1}$ from $C^A$ to $C^B$. The bijections $(a,b) \mapsto (b,a)$ and $((a,b),c) \mapsto (a,(b,c))$ certify the second and third equivalences, respectively. For the fourth statement, apply the bijection $f \mapsto (\pi_1 \circ f, \pi_2 \circ f)$, where the functions $\pi_1 \colon (a,b) \mapsto a$ and $\pi_2 \colon (a,b) \mapsto b$ are known as \emph{projectors}.
	
	The fifth statement is the trickiest one. If $f \in (C^B)^A$, then $f$ is a function which returns a function $f(a)$ from $B$ to $C$ given an element $a \in A$. One can map $f$ to the function $g_f\colon A \times B \to C$ such that $g_f(a,b) = (f(a))(b)$. This mapping $f \mapsto g_f$ is the required bijection.
	
	For injectivity, we assume that $g_f = g_{f'}$ and, for the sake of contradiction, $f \neq f'$. Then there exists a point $a$ such that $f(a) \neq f'(a)$ (by Lemma~\ref{ch0:tot_fnc_cmp}) and, in its turn, there is such $b$ that $(f(a))(b) \neq (f'(a))(b)$. By definition, $g_f(a,b) \neq g_{f'}(a,b)$, whence $g_f \neq g_{f'}$. For surjectivity, consider an arbitrary function $h\colon A \times B \to C$. We need to find such a function $f$ that $g_f = h$. Let us put $f(a) = (b' \mapsto h(a,b'))$. Clearly, $f(a) \in C^B$ for every $a \in A$, so $f \in (C^B)^A$. Finally, $g_f(a,b) = (b' \mapsto h(a,b'))(b) = h(a,b)$ for all $a, b$, whence $g_f = h$.
	
	The fifth statement establishes equivalence between the set of function-valued functions and the set functions of two arguments (i.\,e., one from $A$, the other from $B$). Thus, in principle, functions of two (or more) arguments are not necessary as they can be encoded by functions of just one argument. This idea, known as \emph{currying}\footnote{After Haskell B. Curry, a logic pioneer.}, is important for \emph{functional} programming paradigm.
	
	Let us give an example of currying. Let ${+}\colon \N^2 \to \N$ be the usual numeric addition, which clearly takes two natural arguments. For each $k \in \N$, consider the function $f_k\colon \N \to \N$ such that $f_k(n) = k + n$ for all $n \in \N$ (in other words, $f_k = (n \mapsto k + n)$; in particular, $f_0 = \id_\N$). Now, define the function $f\colon \N \to \N^\N$ so that $f(n) = f_n$. Then $n + m = f_n(m) = (f(n))(m)$. There are no two-argument functions in the right-hand side.
\end{proof}

\paragraph{Indicator function.} For the further discussion, we need some `exemplary' finite sets. These are the sets
$$\ul{n} = \{ k \in \N \mid k < n \}$$
for all possible $n \in \N$. Clearly, $\ul{0} = \void$, $\ul{1} = \{0\}$, and $\ul{2} = \{0, 1\}$. In general, $\ul{n + 1} = \ul{n} \cup \{ n \}$. It is intuitively clear that the set $\ul{n}$ has exactly $n$ elements; in fact, we shall later \emph{use} these sets to \emph{define} the property of having exactly $n$ elements.

\begin{rem}
	From the standard formal definition of the set $\N$, which is omitted\footnote{Otherwise, we would have to introduce a new axiom and develop some `recursive' techniques, which would comprise a huge detour from the main line of our Course.} from this Course, it follows that $0 = \void$, $1 = \{0\} = \{ \void \}$, $2 = \{0, 1\} = \{ \void, \{\void\} \}$, and $n = \ul{n}$ for every $n \in \N$ in general.
\end{rem}
\mcomm{We usually ask students not to use the inner structure of naturals in any argument since this has not been ``officially'' defined in our Course.}

Let $X$ be a set and $A \sbs X$. Define the function $\1_A \colon X \to \ul{2}$ by the equation:
$$
\begin{array}{rcl}
	\1_A(x) &=& \begin{cases}
		1,\ \mbox{if}\ x \in A;\\
		0,\ \mbox{if}\ x \notin A.\\
	\end{cases}
\end{array}
$$
The function $\1_A$ is called the \emph{indicator} (or \emph{characteristic}) function of the set $A$. This function relates to $A$ very closely as it discriminates elements thereof from non-elements, thus `containing' the same `information' as $A$ itself. This fact might be formalized as follows.
\begin{lemma}\label{L9:char1}
	For every sets $A, B \sbs X$, $A = B$ iff $\1_A = \1_B$.
\end{lemma}
\begin{proof}
	The implication from the left to the right is obvious. For the other direction, suppose that $\1_A = \1_B$ and $x \in A$. Then $\1_A(x) = 1$, whence $\1_B(x) = 1$, which means $x \in B$. Thus, $A \sbs B$. The converse inclusion is similar.
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{ind_func.pdf}
	\caption{The indicator $\1_A$ of a set $A$.}
\end{figure}

Consider the set $\ul{2}^X$ comprising all possible functions from $X$ to $\ul{2} = \{0, 1\}$. In fact, all these functions are indicators for certain subsets of $X$.
\begin{lemma}\label{L9:char2}
	For every set $X$, $\mP(X) \sim \ul{2}^X$.
\end{lemma}
\begin{proof} 
	Let a function $\phi\colon \mP(X) \to \ul{2}^X$ be such that $\phi(A) = \1_A$ for each $A \in \mP(X)$. By  Lemma~\ref{L9:char1}, from $\phi(A) = \phi(B)$, it follows $A = B$. So, $\phi$ is an injection. For an arbitrary function $g \in \ul{2}^X$, consider the set $A = g^{-1}[\{ 1 \}] = \{ a \in X \mid g(a) = 1 \}$. Clearly, $g(x) = 1$ if $x \in A$, and $g(x) = 0$ otherwise. Hence, $g = \1_A = \phi(A)$, and $\phi$ is a surjection.
\end{proof}
\noindent This explains a widespread notation $2^X$ for the power-set $\mP(X)$.

In practice, indicator functions are used to replace complicated arguments employing set-theoretic operations with numerical calculations.
\begin{exc}
	Prove that for any sets $B,C \in \mP(X)$ and $x \in X$, the following hold:
	$$
	\begin{array}{rcl}
		\1_{B \cap C}(x) &=& \1_B(x) \cdot \1_C(x);\\
		\1_{B \cup C}(x) &=& \1_B(x) + \1_C(x) - \1_B(x) \cdot \1_C(x);\\
		\1_{\bar B}(x) &=& 1 - \1_B(x),\\
	\end{array}$$
	and $B \sbs C$ is equivalent to $\1_B(x) \leq \1_C(x)$ for each $x \in X$.
\end{exc}

\mcomm{In practice, we omit indicators' argument for it typically does not change throughout such proofs.}

\begin{exm} Putting $X = B \cup C$, one can easily prove that $\bar B \cap \bar C = \overline{B \cup C}$.  Indeed, for each $x \in X$ one has
	\begin{multline*}
		\1_{\bar B \cap \bar C}(x) = (1 - \1_B(x))(1 - \1_C(x)) =
		1 - (\1_B(x) + \1_C(x) - \1_B(x) \1_C(x)) = \1_{\overline{B \cup C}}(x).
	\end{multline*}
\end{exm}

\begin{exm}
	Let us infer $B = C$ from $B \cap C = B \cup C$. By the assumption, for each $x \in X$ we have
	\begin{multline*}
		0 = \1_{B \cup C}(x) - \1_{B \cap C}(x) = \1_B(x) + \1_C(x) - 2\cdot\1_B(x)\1_C(x) =\\
		\1^2_B(x) + \1^2_C(x) - 2\cdot\1_B(x)\1_C(x) = (\1_B(x) - \1_C(x))^2.
	\end{multline*}
	Hence $\1_B(x) = \1_C(x)$ for each $x \in X$. Therefore, $B = C$.
\end{exm}

\paragraph{Tuples and functions.} Previously, we introduced $n$-tuples of elements of a set $A$ as elements of the set $A^n$. For example, $\ul{2}^{3}$ is the set of all possible triplets ($3$-tuples) formed by elements of the set $\ul{2}$. Clearly,
$$
\ul{2}^3 = \{(0,0,0),\ (0,0,1),\ (0, 1, 0),\ (0, 1, 1),\ (1,0,0),\ (1,0,1),\ (1, 1, 0),\ (1, 1, 1)\}.
$$
On the other hand, let us consider the set $\ul{2}^{\ul{3}}$. By definition, this is the set of all possible functions from $\ul{3}$ to $\ul{2}$. One such function $f$ is defined by the equations $f(0) = 1$, $f(1) = 0$, $f(2) = 1$. Taking into account that each function is a set, one thus has
$$f = \{ (0,1), (1,0), (2,1) \}.$$
Likewise, the set of all functions of this form is just
$$
\begin{array}{rcll}
	\ul{2}^{\ul{3}} &=& \{& \{(0,0), (1,0), (2,0)\},\ \{(0,0), (1,0), (2,1)\},\\
	&&& \{(0,0), (1,1), (2,0)\},\ \{(0,0), (1,1), (2,1)\},\\
	&&& \{(0,1), (1,0), (2,0)\},\ \{(0,1), (1,0), (2,1)\},\\
	&&& \{(0,1), (1,1), (2,0)\},\ \{(0,1), (1,1), (2,1)\}\\
	&& \}. &
\end{array}
$$
It is easy to see that if one takes the \emph{second} coordinate from each pair in one function and order them as the respective \emph{first} coordinate increases, he obtains a triplet from $\ul{2}^3$. In particular, the function $f$ results in the triplet $(1, 0, 1) = (f(0), f(1), f(2))$. Conversely, each triplet $(a_1, a_2, a_3)$ gives rise to a function $\{(0,a_1), (1, a_2), (2, a_2)\}$. In fact, such a correspondence is bijective and far from accidental.

\mcomm{This easy theorem proves handy when formalizing combinatorial arguments; it is thus unwise to skip it.}
\begin{thm}\label{L9:t_tuples}
	For every set $A$ and each $n \in \N$, $A^{\ul{n}} \sim A^n$.
\end{thm}
\begin{proof}[Proof sketch.]
	If $n = 0$, one clearly gets $A^0 = \{ \void \}$ by definition and $A^{\ul{0}} = A^\void = \{ \void \}$ by Example~\ref{ch0:void_fnc}. If $n = 1$, $A^1 = A$ by the same definition. In this case, $A^{\ul{1}} = A^{\{0\}} = \{ \{ (0, a) \} \mid a \in A\}$ since any function from $\{ 0 \}$ to $A$ contains exactly one pair $(0, a)$ for some $a \in A$. Clearly, the function $\pi_2 \colon (0, a) \mapsto a$ is a bijection from $A^{\ul{1}}$ to $A$ whereas $A = A^1$.
	
	Let $n \ge 2$. Then $\ul{n} = \{0, \ldots, n-1 \}$. We are to define a bijection $\phi \colon A^{\ul{n}} \to A^n$. For each $f \in A^{\ul{n}}$, put $\phi(f) = (f(0), \ldots, f(n - 1))$. As $f\colon \ul{n} \to A$, it is clear that $\phi(f) \in A^n$. 
	
	The function $\phi$ is injective. Indeed, suppose that $\phi(f) = (f(0), \ldots, f(n - 1)) = (g(0), \ldots, g(n - 1)) = \phi(g)$. Then $f(k) = g(k)$ for each $k \in \ul{n}$ by Lemma~\ref{L2:l_tuple_id}, which implies $f = g$ by Lemma~\ref{ch0:tot_fnc_cmp}.
	
	Let us see that the function $\phi$ is surjective as well. Suppose that $(a_0, \ldots, a_{n-1}) \in A^n$ and define a function $f\colon \ul{n} \to A$ by the equations $f(k) = a_k$ for each $k \in \ul{n}$. Then $\phi(f) = (f(0), \ldots, f(a_{n-1})) = (a_0, \ldots, a_{n-1})$.\footnote{This is just a sketch as a rigorous definition for the tuple $(f(0),\ldots,f(n-1))$ would require some recursion.}
\end{proof}

This way, we can see that $n$-tuples and functions from $\ul{n}$ (also know as \emph{finite sequences of length $n$}) are `almost the same' for there is a natural bijection between the two. In mathematical practice, tuples and finite sequences are routinely identified (i.\,e., they view a sequence as a tuple and vice versa, usually without any comment).

\paragraph{Embeddings.} As we know, the intended meaning of $A \sim B$ is just that: `$A$ has the same number of elements as $B$'---and this is supposed to work for finite and infinite sets alike. Let us see what would be the mathematician's interpretation for `$A$ has no more elements than $B$'.

By definition, a set $A$ \emph{embeddable} into a set $B$ iff there is an injection $f\colon A \to B$. The function $f$ is called an \emph{embedding} in this case. We write $A \stackrel{f}{\lesssim} B$ when $f$ is an embedding of $A$ into $B$, and $A \lesssim B$ when such an embedding exists.

Clearly, the ends of $f$-arrows form  a `copy' of $A$ in $B$ since any two arrows sharing their end or their beginning must coincide. So, it should be intuitively plausible that $B$ has no less elements than $A$ does. But how can one make sure that the formal notion reflects the intuitive one adequately? As with set equivalence, we are to \emph{prove} that embedding behaves as one could expect intuitively.

\begin{exm}
	If $A \sbs B$, then $A \stackrel{\id_A}{\lesssim} B$. Let $2\N$ be the set of all even naturals. Then $\N \lesssim 2\N$ and, surely, $2\N \lesssim \N$. Observe that $\N \neq 2\N$ yet $\N \sim 2\N$.
\end{exm}

\begin{lemma}\label{L9:l_emb}
	For any sets $A, B, C$ the following hold:
	\begin{enumerate}
		\item $A \lesssim A$;
		\item if $A \lesssim B$ and $B \lesssim C$, then $A \lesssim C$;
		\item if $A \sim B$, then $A \lesssim B$ and $B \lesssim A$;
		\item $A \lesssim B \iff \exists D \sbs B\ A \sim D$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For the second statement, recall that the composition of two injections is an injection by Lemma~\ref{ch0:fnc_cmp}. For the third one, we know that both $f$ and $f^{-1}$ are injections given $f$ is a bijection.
	
	In view of Remark~\ref{ch0:inj_img}, it suffices to take $f[A]$ as $D$ for the last statement, where $f$ is an arbitrary injection $f\colon A \to B$.
\end{proof}
The intuitive validity of the first three statements is clear: say, if $A$ and $B$ has the same number of elements, none of the sets has more elements than the other one. The last property is also very natural: $A$ has no more elements than $B$ iff $A$ is equinumerous with a \emph{part} (i.\,e., a subset) of $B$.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{embed.pdf}
	\caption{Proving Claim $4$ of Lemma~\ref{L9:l_emb}.}
\end{figure}

\mcomm{Clearly, it depends on the calculus course taught which definition of $\R$ they use (if any). An informal discussion of ``infinite binary fractions'' might be helpful here. I recommend to stress the point that there exist non-equivalent infinite sets. Some students manage to ignore this fact even after having studied many mathematical courses.}

\begin{rem}
	It is well-known from Calculus courses that $\R \sim \mP(\N)$. The set $\mP(\N)$ is called the \emph{continuum} for being equivalent to the `continuous' real line. By Theorem~\ref{L9:t_cantor}, $\N \nsim \R$, that is, the `infinite numbers of elements' for $\N$ and $\R$ cannot be the same; not all `infinities' are thus `equal' (equivalent).
	
	The famous \emph{Continuum Hypothesis} (CH) says that from $\N \lesssim X \lesssim \mP(\N)$, it follows either $X \sim \N$ or $X \sim \mP(\N)$. In other words, there is no `number of elements' strictly between $\N$ and the continuum. It is proved that neither CH nor its negation follows from the set-theoretic axioms usually assumed (provided those axioms are consistent\footnote{It is unprovable that the axioms are consistent given (1) the axioms are indeed consistent; (2) each proof must be based on the axioms themselves.}). Hence, either statement is consistent with the axioms.
	
	So, the question about `intermediate sets' is fundamentally undecidable if one sticks to the well-tested, widely accepted, and intuitively valid axioms. It is an open philosophical question whether CH is intuitively valid.  
	
	In practice, it is not thus justified to assume either CH or its negation. 
\end{rem}

A set $X$ is called \emph{countable} if $X \sim \N$. If an infinite set is not countable, it is then \emph{uncountable}. Thus, the set $\R$ is uncountable.

Interestingly, it is enough to have two `unrelated' injections from $A$ to $B$ and from $B$ to $A$ for these two sets to be equivalent. On the other hand, it is very natural that $A$ and $B$ has the same number of elements when neither set has more elements than the other one. Nevertheless, all known proofs of this theorem are somewhat tricky and employ a `limit' construction.
\begin{thm}[Cantor---Schr\"oder---Bernstein]\label{ch0:bernstein} For any sets $A$ and $B$, if $A \lesssim B$ and $B \lesssim A$, then $A \sim B$.
\end{thm}
\noindent The proof is omitted for being technically demanding.

\begin{corr}
	For no set $A$ is an embedding $\mP(A) \lesssim A$ possible.\footnote{This fact can be proved quite easily without any reference to Theorem~\ref{ch0:bernstein}.}
\end{corr}
\begin{proof}
	Clearly, $A \stackrel{f}{\lesssim} \mP(A)$, where $f\colon x \mapsto \{ x \}$ for every $x \in A$. By Theorem~\ref{L9:t_cantor}, it is not the case that $A \sim \mP(A)$. Hence, $\mP(A) \not\lesssim A$.
\end{proof}

By $A \lnsim B$, we shall denote the situation when $A \lesssim B$ but $A \nsim B$. This is the formal way to say that $A$ has \emph{fewer} elements than $B$ does. By Theorem~\ref{ch0:bernstein}, $A \lnsim B$ iff $A \lesssim B$ but  $B \not\lesssim A$. In particular, $A \lnsim \mP(A)$; so, for every set $A$, the set $\mP(A)$ is `strictly greater'. There is no `greatest set' in existence.

\medskip

Cantor---Schr\"oder---Bernstein Theorem is very practical when a set equivalence is to be proved. Usually, it is \emph{much} easier to construct two injections instead of an explicit bijection.
\begin{exm}
	Clearly, $\N \lesssim \Q$. On the other hand, $\Q \lesssim \N^3$. For, indeed, each \emph{positive} rational $q$ is uniquely expressible as an \emph{irreducible} fraction\footnote{That is, one whose numerator and denominator are coprime.} $\frac{m}{n}$, where $m, n \in \N$; the function $f\colon q \mapsto (m,n,0), 0 \mapsto (0,1,0), -q \mapsto (m,n,1)$ is then a required injecton.
	
	Further, one obtains $\N^3 = \N^2 \times \N \sim \N \times \N \sim \N$ by Theorem~\ref{ch0:bi_power} and Example~\ref{ch0:N_sq}. Hence $\Q \lesssim \N$. Applying Theorem~\ref{ch0:bernstein}, one finally gets $\Q \sim \N$, that is, the set of rationals $\Q$ is countable. Since $\N \lesssim \Z \lesssim \Q \lesssim \N$, one can observe that the set $\Z$ of integers is countable as well.
\end{exm}

\begin{exc}
	Prove that $\ul{2}^\N \sim \ul{3}^\N$.
\end{exc}

\begin{exm}\label{eq_sets:big_exm}
	As we know, $\R \sim \mP(\N)$. By Lemma~\ref{L9:char2}, this implies $\R \sim \ul{2}^\N$, whence
	\begin{multline*}
		\R \sim \R \times \{0\} \lesssim \R \times \R \sim \ul{2}^\N \times \ul{2}^\N \sim (\ul{2} \times \ul{2})^\N \sim\\
		\ul{4}^\N \lesssim \N^\N \lesssim \R^\N \sim (\ul{2}^\N)^\N \sim \ul{2}^{\N \times \N} \sim \ul{2}^\N \sim \R,
	\end{multline*}
	with the help of Theorem~\ref{ch0:bi_power} and Example~\ref{ch0:N_sq}. We conclude that $\R^2 \sim \N^\N \sim \R^\N \sim \R$ applying Theorem~\ref{ch0:bernstein}. The statement $\R^2 \sim \R$ means that the line has `as many' points as the plane. In the 1870s, G.~Cantor remarked on a similar fact: ``I see it, but I do not believe it.''
\end{exm}

\begin{exm}
	The set $C$ of all possible circles in the plane is equivalent to $\R$.
	
	First, we construct an injection $\R \to C$. We map each number $x \in \R$ to the circle with center $(x, 0)$ and of radius $1$. Clearly, this is a well-defined function (total for $\R$ and functional) and an injection indeed as two circles with distinct centers are not equal.
	
	An injection from $C$ to $\R$ is harder. It is natural to map each circle to the triplet $(x, y, r) \in \R^3$, where $(x, y)$ is the center and $r$ is the radius. This is a well-defined function for each circle has exactly one center and one radius. Injectivity is clear: if both the centers and the radii coincide, so do the circles. Thus, $C \lesssim \R^3$. Please notice that this injection is not surjective since radii must be positive. Now, it suffices to observe that $\R^3 = \R^2 \times \R \sim \R \times \R \sim \R$, whence $C \lesssim \R$. 
	
	From $\R \lesssim C$ and $C \lesssim \R$, follows that $\R \sim C$ by Theorem~\ref{ch0:bernstein}.
\end{exm}

\begin{exm} Let $X$ be some set of pairwise disjoint discs\footnote{A \emph{disc} is the region of the plane bounded by a circle and containing the center of that circle. We assume here that all discs in $X$ are non-degenerate, i.\,e. of \emph{positive} radii.} in the plane. Prove that $X \lesssim \N$. So, we are to prove here that it is impossible to choose more than countably many pairwise disjoint discs `simultaneously'.
	
	The main idea is to set a `mark' upon each disc so that (1) distinct discs have distinct marks; (2) marks come from a countable set. To implement this plan, let us recall the following fact from Calculus: for every numbers $a, b \in \R$, if $a < b$, then there exists a \emph{rational} number $q \in \Q$ such that $a < q < b$.
	
	Consider an arbitrary disc $D \in X$ with center $(x, y)$ and of radius $r$. Let $S_D$ be the square with vertices $(x - \frac{r}{2}, y - \frac{r}{2}),\ (x - \frac{r}{2}, y + \frac{r}{2}),\ (x + \frac{r}{2}, y + \frac{r}{2}),\ (x + \frac{r}{2}, y - \frac{r}{2})$. It is easy to see that all points from $S_D$ lie inside the disc $D$.
	
	Let $q$ and $q'$ be rational numbers with $x - \frac{r}{2} < q < x + \frac{r}{2}$ and $y - \frac{r}{2} < q' < y + \frac{r}{2}$. Then the point $(q, q') \in \Q^2$ belongs to $D$. It is quite believable that there exists a function $f$ mapping each disc from $X$ to some point from $\Q^2$ which belongs to that disc.\footnote{In fact, no form of the Axiom of Choice is needed for such a function $f$ to exist. As $\Q^2 \sim \N$, each rational point $(q, q')$ from $D$ corresponds to a natural number. One can just map $D$ to such a point with the \emph{least} possible number.} As any two distinct discs are disjoint, one gets $f(D) \neq f(D')$ when $D \neq D'$. Hence, $X \stackrel{f}{\lesssim} \Q^2$. As $\Q^2 = \Q \times \Q \sim \N \times \N \sim \N$, we have proved that $X \lesssim \N$.
\end{exm}

So far, we have demonstrated that the formal notions of set equivalence and embedding enjoy many intuitively valid properties of their informal counterparts. For other properties, we have not yet check it: e.\,g., for every two sets $A$ and $B$, either one must have more elements than the other or they must be of the same size (intuitively). Is it true that for every two sets $A$ and $B$, either $A \lesssim B$ or $B \lesssim A$ holds? Assuming one more axiom (the Axiom of Choice), one can prove this claim. Nevertheless, these statements are beyond the scope of our Course. In fact, one rarely considers \emph{arbitrary infinite sets} in ``discrete mathematics'', while the Axiom of Choice is not that urgently needed otherwise. 

On the other hand, some facts we have established for set equivalence and embedding are counter-intuitive. Say, that the plane $\R^2$ is equinumerous with the line $\R$. Thus, the formal notions live their own lives and cannot be judged by their informal analogues.

\section{Counting: the Basics}\label{sect:comb1}
\mcomm{This section contains a (reasonably) rigorous presentation of combinatorics' foundations including the Pigeonhole Principle, the Rules of Sum and Product. We try to prove as much as possible when avoiding explicit recursion and the Axiom of Choice.}

Our next goal is to see how it is possible to construct a coherent theory of \emph{finite} sets and, in particular, to \emph{define} the `number of elements' of such a set. To this end, we will use the sets $\ul{n} = \{k \in \N \mid k < n \}$ as `exemplary' finite sets. Namely, a set $A$ is called \emph{finite} iff $A \sim \ul{n}$ for some $n \in \N$. Otherwise, the set $A$ is called \emph{infinite}.

\begin{exm}
	The sets $\void \sim \ul{0}$, $\{\void\} \sim \ul{1}$, and $\{\void, \{\void\}\} \sim \ul{2}$ are finite. The set $A = \{x,y,z\}$ is finite. Depending on which of the elements $x,y,z$ are identical, we have either $A \sim \ul{1}$ or $A \sim \ul{2}$ or $A \sim \ul{3}$. Say, if $x = y \neq z$, then $A \sim \ul{2}$.
\end{exm}

We have assumed as known that $0 \neq 1$. Therefore $\ul{2} = \{0,1\} \neq \{0\} = \ul{1}$. But is it the case that $\ul{2} \sim \ul{1}$? Were it so, our definition of finite set would not be adequate to the `number of elements' intuition as $2 \neq 1$.

Fortunately, this case is easy: if $\{0,1\} \stackrel{\phi}{\sim} \{0\}$, then $\phi(0) = 0 = \phi(1)$, which implies $0 = 1$ by injectivity, which is false. But what if $\ul{n} \sim \ul{m}$ holds for some other distinct numbers $n$ and $m$? We need induction in order to exclude such a possibility.

\begin{lemma}\label{L10:dirichlet_lemma}
	For each $n \in \N$, if $f\colon \ul{n+1} \to \ul{n}$, then $f$ is \emph{not} injective.
\end{lemma}
\begin{proof}
	Assume the contrary. So, there is such a number $n \in \N$ that there exists an injection $f\colon \ul{n+1} \to \ul{n}$. Due to the Least Number Principle, we may consider the \emph{least} such $n$. No injection (nor function, in general) $f\colon \ul{1} \to \ul{0}$ is possible for $f(0) \notin \ul{0}$. Hence, $n \neq 0$, i.\,e., $n = m + 1$ for some $m \in \N$.
	
	Let $f(n) = x \in \ul{n}$ and consider the function $g\colon \ul{n} \to \ul{n}$ that permutes $m$ and $x$. More accurately,
	$$
	\begin{array}{rcll}
		g(k) &=&\begin{cases}
			m&\mbox{if}\ k = x;\\
			x&\mbox{if}\ k = m;\\
			k& \mbox{otherwise}.\\
		\end{cases}
	\end{array}
	$$
	It is clear that $g$ is an injection (and even a bijection). The function $f\rst \ul{n}\colon \ul{n} \to \ul{n}$, which is defined by the trivial equation $(f\rst \ul{n})(x) = f(x)$ for each $x \in \ul{n}$ and called the \emph{restriction} of $f$ to $\ul{n}$, is clearly injective as well. Therefore, the composition $h = g \circ (f \rst \ul{n})$ is an injection $\ul{n} \to \ul{n}$.
	
	If $h(k) = m$, then $(f \rst \ul{n})(k) = x$, which implies  $f(n) = x = f(k)$ despite $n \neq k \in \ul{n}$. This contradicts the injectivity of $f$. So, $h$ does not take the value $m$ and $\rng h \sbs \ul{m}$. Yet then $h$ is an injection $\ul{m+1} \to \ul{m}$. This is not possible due to the choice of $n$ and the fact that $m < n$. A contradiction.
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{dirichlet.pdf}
	\caption{Proving Lemma~\ref{L10:dirichlet_lemma}.}
\end{figure}

\begin{thm}[Pigeonhole Principle]\label{L10:dirichlet}
	If $m > n$ and $f\colon \ul{m} \to \ul{n}$, then the function $f$ is not injective \emph{(}i.\,e., $\ul{m} \not\lesssim \ul{n}$\emph{)}.
\end{thm}
\begin{proof}
	Suppose that a certain injection $f\colon \ul{m} \to \ul{n}$ does exist. Since $m > n$, we get $m \geq n + 1$, whence $\ul{n+1} \sbs \ul{m}$. Consequently, $\ul{n+1} \lesssim \ul{m} \lesssim \ul{n}$. It follows then that $\ul{n+1} \lesssim \ul{n}$, which is not possible.
\end{proof}
\noindent The Pigeonhole Principle can be informally stated the following way:
\begin{quote}
	If $m > n$, it is not possible to place $m$ distinct objects into $n$ distinct boxes in such a manner that each box contains at most one object.
\end{quote}
Evidently, this statement is reducible to the first one if one labels the objects and boxes with numbers from $\ul{m}$ and $\ul{n}$, respectively, and then considers a function that maps   every object's label to the label of the box containing that object.

\begin{corr}
	If $m \neq n$, then $\ul{m} \nsim \ul{n}$.
\end{corr}
\begin{proof}
	If $m \neq n$, then either $m > n$ or $m < n$. In the first case, by the Pigeonhole Principle, it is impossible that $\ul{m} \lesssim \ul{n}$. In the second case, it is impossible that $\ul{n} \lesssim \ul{m}$. Each case but excludes the equivalence $\ul{m} \sim \ul{n}$.
\end{proof}

\begin{corr}
	For every finite set $A$, there exists a \emph{unique} number $n\in \N$ such that $A \sim \ul{n}$.
\end{corr}
This number $n$ with $A \sim \ul{n}$ is called the \emph{cardinality} (or the number of elements, the size) of the finite set $A$. We shall write $n = |A|$ is this case. Clearly, for finite sets $A$ and $B$, one has $A \sim B$ iff $|A| = |B|$.

\begin{exm}\label{L10:N_fin}
	The set $\N$ is infinite. Otherwise, $\N \sim \ul{n}$ for some $n \in \N$. Yet $\ul{n+1} \sbs \N$, which results in $\ul{n+1} \lesssim \N$. Then $\ul{n+1} \lesssim \ul{n}$, contrary to the Pigeonhole Principle.
\end{exm}

It is not hard to establish a dual result to the Pigeonhole Principle. Namely, the following theorem holds.
\begin{thm}\label{L10:pigeon_dual}
	If $m > n$ and $f\colon \ul{n} \to \ul{m}$, then the function $f$ is not surjective.
\end{thm}
\begin{proof}
	Assume it is. Consider the function $g\colon \ul{m} \to \ul{n}$ defined in the following way:
	$$g(k) = \mbox{the least such $x \in \ul{n}$ that } f(x) = k.$$
	Surjectivity of $f$ and the Least Number Principle guarantee totality of $g$, while $g$ is functional as a least element is unique. So, $g$ is a well-defined function. Moreover, it is injective. Indeed, if $g(k) = g(l)$, then $k = f(g(k)) = f(g(l)) = l$. But no injection from $\ul{m}$ to $\ul{n}$ is possible according to the Pigeonhole Principle. A contradiction.
\end{proof}

Another useful result closely related to the Pigeonhole Principle is as follows. (We omit the proof.)
\begin{thm}\label{L10:fin_sur_in}
	Let sets $A$, $B$ be finite and $A \sim B$. Then for every function $f\colon A \to B$, the following holds: $f$ is injective iff $f$ is surjective.
\end{thm}
\mcomm{In fact, one can easily prove that every injection $A \to B$ is surjective under assumption that a proper subset of a finite set is smaller than that set. The latter statement follows from the  Rule of Sum, which we prove below. A forward reference is unwanted here, so we omit the proof, which can be postponed for a seminar class, though. Another---far less elegant---way to prove surjectivity is similar to that of Theorem~\ref{L10:dirichlet}. Next, one can prove injectivity of every surjection $f\colon A \to B$ by constructing its right inverse $g$ (like we have done when proving Theorem~\ref{L10:pigeon_dual}), which is clearly injective. By the first claim, $g$ must be bijective and $f = g^{-1}$.}
\begin{exm}
	If $10$ students have got their marks (from $1$ to $10$ points) in such a way that no two distinct students have identical marks, then each mark has been used.
\end{exm}

\paragraph{Finite and countable sets.} Recall that a set $A$ is  called \emph{countable}\footnote{Sometimes, they call finite sets countable as well.} if $A \sim \N$. Now we want to establish some general facts about finite and countable sets. Most of these are `obvious' and in many cases, we will not bother ourselves with detailed proofs---for \emph{rigorous} proofs are \emph{not} that obvious. But anyway, we should at least put our knowledge of those facts in order.

\begin{lemma}
	If a set $A$ is countable \emph{(}or finite, infinite\emph{)} and $A \sim B$, then $B$ enjoys the same property.
\end{lemma}
\noindent This is truly obvious.

\begin{lemma}\label{L10:count_inf}
	If a set $A$ is countable and $A \lesssim B$, then the set $B$ is infinite.
\end{lemma}
\begin{proof}
	Otherwise, one gets $\ul{n+1} \lesssim \N \lesssim B \sim \ul{n}$ for some $n \in \N$. Yet then $\ul{n+1} \lesssim \ul{n}$ contrary to the Pigeonhole Principle.
\end{proof}

\begin{lemma}
	If $A \subseteq \N$, then the set $A$ is either finite or countable.
\end{lemma}
\begin{proof}[Proof sketch.]
	The idea is simple: let us enumerate the elements of $A$ in increasing order, so that $A = \{a_0, a_1, \ldots\}$ and $a_0 < a_1 < \ldots$ If we run out of elements of $A$ at some stage, we have got a bijection $k \mapsto a_k$ from $\ul{n}$ to $A$ for some $n \in \N$. Otherwise, we have a bijection $\N \to A$.
\end{proof}

\begin{corr}\label{L10:count_sbs}
	If $A \lesssim B$ and the set $B$ is countable, then the set $A$ is either finite our countable.
\end{corr}

\begin{corr}\label{L10:fin_sbs}
	If $A \lesssim B$ and the set $B$ is finite, then set $A$ is also finite and $|A| \leq |B|$.
\end{corr}
\begin{proof}
	By the assumption, $A \lesssim B \sim \ul{n} \lesssim \N$ for some $n \in \N$. Then the set $A$ is either finite or countable by the previous corollary. However, $A$ cannot be countable in view of Lemma~\ref{L10:count_inf}. If $A \sim \ul{m}$ and $m > n$, then we get $\ul{m} \lesssim \ul{n}$ contrary to the Pigeonhole Principle. Hence, $m \leq n$.
\end{proof}

The following \emph{Rules of Sum and Product} form a basis for the \emph{enumerative combinatorics}, which deals with counting of various finite objects (like permutations, arrangement, partitions etc.)

\begin{thm}[Rule of Sum]\label{L10:set_sum}
	Let sets $A$, $B$ be finite and $A \cap B = \void$. Then the set $A \cup B$ is also finite and $|A \cup B| = |A| + |B|$.
\end{thm}
\begin{proof}
	Assume that $A \stackrel{f}{\sim} \ul{n}$ and $B \stackrel{g}{\sim} \ul{m}$. We want to define a bijection $h\colon A \cup B \to \ul{n + m}$ in the following way:
	$$
	\begin{array}{rcll}
		h(x) &=& \begin{cases}
			f(x)&\mbox{if}\ x \in A;\\
			n + g(x)&\mbox{if}\ x \in B.\\
		\end{cases}
	\end{array}
	$$
	As $A \cap B = \void$, $h$ is indeed a function with $h(x) < n + m$. Let $h(x) = h(y)$. If $x, y \in A$, then $x = y$ by injectivity of $f$. If $x, y \in B$, we get $n + g(x) = n + g(y)$, whence $g(x) = g(y)$ by the properties of addition, which, in turn, implies that $x = y$ for $g$ is injective. Now assume that $x \in A$ and $y \in B$. Then $h(x) = f(x) < n \leq n + g(y) = h(y)$ despite $h(x) = h(y)$. The function $h$ is thus injective.
	
	Let us verify its being surjective. Let $k \in \ul{n + m}$. Then either $k < n$ or $n \leq k < n + m$. In the first case, obtain $k = f(x) = h(x)$ for some $x \in A$ as $f$ is surjective. In the second case, we have $k = n + k'$ for a suitable $k' < m$ due to the properties of addition. By surjectivity of $g$, there exists such $y \in B$ that $k' = g(y)$, which yields $k = n + g(y) = h(y)$.
\end{proof}

\begin{corr}\label{L10:in-ex}
	If sets $A$, $B$ are finite, then the set $A \cup B$ is also finite and
	$|A \cup B| = |A| + |B| - |A \cap B|$.
\end{corr}
\begin{proof}
	One has both $A = (A\setminus B) \cup (A \cap B)$ and $A \cup B = (A\setminus B) \cup B$.
	
	The sets $A \setminus B,\; A \cap B \sbs A$ are finite by Corollary~\ref{L10:fin_sbs}. The set $A \setminus B$ intersects neither $A \cap B$ nor $B$. Hence $|A| = |A\setminus B| + |A \cap B|$, $A \cup B$ is finite, and
	$$
	|A \cup B| = |A\setminus B| + |B| = (|A| - |A \cap B|) + |B| = |A| + |B| - |A \cap B|.
	$$
\end{proof}
\begin{corr}\label{L10:fin-union}
	If sets $A$, $B$ are finite, then $|A \cup B| \leq |A| + |B|$.
\end{corr}

\begin{thm}[Rule of Product]\label{prod}
	Let sets $A$ and $B$ be finite. Then the set $A \times B$ is also finite and $|A \times B| = |A| \cdot |B|$.
\end{thm}
\begin{proof}
	Let $A \stackrel{f}{\sim} \ul{n}$ and $B \stackrel{g}{\sim} \ul{m}$. If $m = 0$, then $B = \void$ and $A \times B = \void \sim \ul{0}$. So, assume $m \neq 0$. We want to construct a bijection $h\colon A \times B \to \ul{nm}$. In order to do so, put
	$$h(x, y) = mf(x) + g(y)$$
	for every $x \in A$, $y\in B$.
	
	We recall Theorem~\ref{L4:t3} (on integer division), which states that for every naturals $u$ and $v \neq 0$, there exists a unique pair  $(q,r) \in \N^2$ such that $u = vq + r$ and $r < v$.
	
	Let us check that $h$ is surjective. Suppose that $z \in \ul{nm}$. Then $z = mq + r$ for some $q \in \N$ and $r \in \ul{m}$. Hence, there is such $y \in B$ that $r = g(y)$. We also have $q \in \ul{n}$ for $z \geq nm$ otherwise; it follows that there is an element $x \in A$ with $q = f(x)$. Thus, $z = mf(x) + g(y) = h(x, y)$.
	
	Now, let us check injectivity. Assume that $mf(x) + g(y) = mf(x') + g(y') = z = mq + r$. As $g(y), g(y') < m$, both $g(y) =  g(y')$ and $f(x) = f(x')$ must hold for the uniqueness requirement of Theorem~\ref{L4:t3}. Hence, $x = x'$ and $y = y'$ for both $f$ and $g$ are injective.
\end{proof}

\begin{corr}\label{L10:num_tuple}
	If a set $A$ is finite, then for each $n \in \N$, the set $A^n$ is finite as well and $|A^n| = |A|^n$.
\end{corr}
\begin{proof}
	By induction on $n$. Use the identity $A^{n+1} = A^n \times A$ when $n \geq 1$.
\end{proof}

Let us count all possible functions between two finite sets.
\begin{corr}\label{L10:num_func}
	If sets $A$ and $B$ are finite, then the set $B^A$ is also finite and $|B^A| = |B|^{|A|}$.
\end{corr}
\begin{proof}
	Let $A \sim \ul{n}$ and $B \sim \ul{m}$. By Theorems~\ref{L9:t_tuples} and~\ref{ch0:bi_power}, get
	$$B^A \sim \ul{m}^{\ul{n}} \sim \ul{m}^n,$$
	whence $|B^A| = |\ul{m}|^n = m^n$.
\end{proof}

\begin{corr}\label{L10:num_pow}
	If a set $A$ is finite, then the set $\mP(A)$ is finite as well and $|\mP(A)| = 2^{|A|}$.
\end{corr}
\begin{proof}
	By Lemma~\ref{L9:char2}, obtain $\mP(A) \sim \ul{2}^A$, whence $|\mP(A)| = 2^{|A|}$ in view of Corollary~\ref{L10:num_func}.
\end{proof}
%
%\begin{exm}
%Напротив, если $A$ бесконечно, тривиальное вложение $A \lesssim \mP (A)$ влечет, по следствию~\ref{ch1:fin_sbs}, что $\mP(A)$ также бесконечно.
%\end{exm}
%
%\begin{lemma}
%Если множества $A$ и $B$ счетны, то множество $A \times B$ тоже счетно.
%\end{lemma}
%\begin{proof}
%Согласно примеру~\ref{ch0:N_sq}, $\N \times \N \sim \N$. Затем используем теорему~\ref{ch0:bi_power}.
%\end{proof}
%
%\begin{corr}
%Если множество $A$ счетно и $n \in \N_+$, то множество $A^n$ тоже  счетно \emph{(}а $A^0 = \{ \void \}$ конечно\emph{)}.
%\end{corr}


\begin{exc}\label{L10:fin_img}
	Let $f\colon A \to B$ and a set $A$ be finite. Then the set $f[A]$ is also finite and $|f[A]| \leq |A|$.
\end{exc}
%\begin{proof}
%Проведем индукцию по $n = |A|$. Если $|A| = 0$, то $A = \void$, откуда $f = \void$ и $f[A] = \void$. Пусть $|A| = n + 1$. Рассмотрим некоторый $x \in A$ и положим $A' = A \setminus \{x\}$. По правилу сложения $|A'| = n$, а значит, по предположению индукции для функции $f' = f \rst A'$ имеем $|f'[A']| \lq |A'|$. С другой стороны, $f[A] = f'[A'] \cup \{f(x)\}$, откуда $|f[A]| \lq |f'[A']| + |\{ x \}| \lq n + 1$.
%\end{proof}



\begin{exm}\label{L10:2_union}
	If a set $A$ is countable, while $B$ is either countable or finite, then the set $A \cup B$ is countable.
	
	Clearly, $A \sbs A \cup B$, whence $\N \lesssim A \cup B$. On the other hand, $A \cup B \lesssim (\N \times \{ 0 \}) \cup (\N \times \{ 1 \})$. Indeed, assume $A \stackrel{f}{\sim} \N$ and $B \stackrel{g}{\lesssim} \N$. For every $x \in A \cup B$ we let
	$$
	\begin{array}{rcll}
		h(x) &=&\begin{cases}
			(f(x),0)& \mbox{if}\ x \in A;\\
			(g(x),1)& \mbox{if}\ x \in B \setminus A.\\
		\end{cases}
	\end{array}
	$$
	This yields an injection $h\colon A \cup B \to (\N \times \{ 0 \}) \cup (\N \times \{ 1 \})$. Furthermore, we have $(\N \times \{ 0 \}) \cup (\N \times \{ 1 \}) = \N \times \ul{2} \lesssim \N \times \N \sim \N$. So, $A \cup B \lesssim \N$, whence $A \cup B \sim \N$.
\end{exm}
%
%Интуитивно ясно, что если есть сюръекция из $A$ в $B$, то множество $B$ "<не больше">, чем множество $A$, ибо из каждого элемента $A$ в $B$ идет ровно одна стрелка, а различных концов стрелок не больше, чем начал. В формальных терминах достаточно показать, что $B \lesssim A$. Это легко следует из леммы~\ref{ch0:sur_inv}, поскольку правая обратная любой функции есть инъекция. Однако лемма использует аксиому выбора. В случае конечных и счетных множеств без аксиомы можно обойтись.
%
%\begin{lemma}\label{ch1:count_img}
%Пусть $f\colon A \to B$ и множество $A$ счетно. Тогда множество $f[A]$ конечно или счетно.
%\end{lemma}
%\begin{proof}
%Пусть $\N \stackrel{\phi}{\sim} A$.
%Определим функцию $g\colon f[A] \to \N$, т.\,ч. для всех $y \in f[A]$
%$$g(y) = \min \{k \in \N \mid f(\phi(k)) = y \}.$$
%Содержательно, $g(y)$ есть "<наименьший"> прообраз элемента $y$. В силу принципа наименьшего числа, функция $g$, действительно, тотальна. Если $g(y) = g(z) = k \in \N$, то $y = f(\phi(k)) = z$. Значит, $g$ "--- инъекция. Согласно следствию~\ref{ch1:count_sbs}, множество $f[A]$ оказывается конечно или счетно.
%\end{proof}
%
%\paragraph{Зависимый и счетный выбор.} Следующие результаты о счетных множествах используют аксиому выбора. Однако "<в полную силу"> аксиому можно здесь не применять, обходясь важным следствием "--- \emph{принципом зависимого выбора}\index{Принцип!зависимого выбора}, гласящим:
%\begin{quote}
%Пусть множество $A$ непусто и отношение $R \sbs A^2$ таково, что для всякого $a \in A$ найдется $b \in A$, т.\,ч. $a R b$. Тогда существует функция $f\colon \N \to A$, т.\,ч. $f(n) R f(n+1)$ для всех $n \in \N$.
%\end{quote}
%Иначе говоря, если для каждого элемента $a$ можно выбрать $b$ так, что $a R b$, то существует \emph{бесконечная} (именно, счетная) последовательность $(a_n)_{n \in \N}$ таких выборов, где каждый элемент в находится отношении $R$ к последующему. Как видим, логическая структура $\forall \exists \to \exists \forall$ аксиомы выбора сохранена.
%
%\begin{rem}
%Индукцией по $m$ легко доказать, что в условиях принципа зависимого выбора \emph{для каждого} $m  \in \N$ найдется функция $f\colon \ul{m} \to A$, т.\,ч. $f(n) R f(n+1)$, если $n+1 \in \ul{m}$. Но вот "<склеить"> такие функции в одну $f\colon \N \to A$ без той или иной формы аксиомы выбора не получается.
%\end{rem}
%
%\begin{exm}
%Проверим, что принцип зависимого выбора следует из аксиомы выбора.
%
%В самом деле, рассмотрим множество $X = \{R[\{a\}] \in \mP(A) \mid a \hm\in A \}$. По условию, для каждого $a \in A$ есть $b$, т.\,ч. $a R b$, а значит, $\void \notin X$. Для множества $X$ существует функция выбора $\xi\colon X \to A$ (учли $\cup X \sbs A$). Поскольку $A \neq \void$, можно взять некоторый $a_0 \in A$. Тогда, согласно теореме~\ref{ch1:recursion}, существует функция $f\colon \N \to A$, т.\,ч.
%$$f(0) = a_0\quad \mbox{и}\quad f(n+1) = \xi(R[\{ f(n) \}])$$
%при всех $n \in \N$. Очевидно, функция $f$ искомая.
%\end{exm}

\medskip

Up to now, we have had only a `negative' definition for infinity: a set $A$ is infinite iff it is \emph{not} finite, i.\,e. there is no $\ul{n}$ with $A \sim \ul{n}$. Clearly, this makes assuming some set to be infinite not very helpful in a proof. Using the Axiom of Choice, it is possible to turn infinity into a more explicit, `positive' notion.

\begin{lemma}\label{L10:min_inf}
	If a set $A$ is infinite, then $\N \lesssim A$.
\end{lemma}
\begin{proof}[Proof sketch.]
	As $A$ is infinite, then $A \setminus B$ is also infinite, whence non-empty for every finite $B \sbs A$. (Otherwise, $A = (A \setminus B) \cup B$ would be finite by the Rule of Sum.) Therefore it is possible to choose some $a_0$ from $A$, then $a_1 \in A \setminus \{ a_0 \}$, $a_2 \in A \setminus \{ a_0, a_1 \}$, etc. (an \emph{infinite} sequence of choices needs a special axiom to be well-defined). This procedure gives rise to a function $f\colon \N\to A$ such that $f(n) = a_n$, which is clearly injective.
	
\end{proof}

\begin{corr}
	If a set $A$ is infinite, then there exists a countable set $B$ such that $B \sbs A$.
\end{corr}

\begin{corr}\label{L10:min_inf_corr}
	A set $A$ is infinite iff $\N \lesssim A$.
\end{corr}
\noindent The set $\N$ is thus the `least' infinite set (in the sense of the `relation' ${\lesssim}$).

\begin{exm}
	Let a set $A$ be infinite and $B$ be either finite or countable. Then $A \cup B \sim A$.
	
	Notice that $A \cup B = A \cup (B \setminus A)$. By Corollaries~\ref{L10:fin_sbs} and~\ref{L10:count_sbs}, the set $B \setminus A \sbs B$ is either finite or countable. We know from the above that $A$ has a countable subset $B'$. Then
	$$A \cup B = (A \setminus B') \cup B' \cup (B \setminus A),$$
	where the three sets in the right hand side union are pairwise disjoint. By Example~\ref{L10:2_union}, the set $C = B' \cup (B \setminus A)$ is countable; hence, there is a bijection $f\colon C \to B'$. As $A \cup B = (A \setminus B') \cup C$, we can extend $f$ to a function $g\colon A \cup B \to (A \setminus B') \cup B'$ in the following way:
	$$
	\begin{array}{rcll}
		g(x) &=& \begin{cases}
			f(x)&\mbox{if } x \in C;\\
			x&\mbox{if } x \in A \setminus B'.\\
		\end{cases}
	\end{array}
	$$
	Clearly, $g$ is bijective. Since $(A \setminus B') \cup B' = A$, the bijection $g$ is as required.
\end{exm}

\begin{exc}
	Prove that if $A \setminus B$ is infinite, while $B$ is finite or countable, then $A \sim A \setminus B$.
\end{exc}

\section{More Combinatorics}
\mcomm{Traditionally, the elements of combinatorics are presented informally, with very little reference to set theory. The benefits of this approach are well-known, but it has clear drawbacks in the context of a set-based course: first, it breaks the logical sequence of presentation (so that the Student may think set theory axioms are useless and inadequate for ``everyday mathematics'' besides being abstract and possibly indigestible); second, our intuition is limited, so one may lose comprehension beyond some point with little chance to dissect a complicated intuitive argument into primitive steps. Therefore, our approach is to make combinatorial computations as close to set-theoretic primitives as reasonably possible. Of course, enough practice will develop the Student's intuition so that he can see traditional arguments as shorthands for formal ones.}

Our next goal is to gather more simple facts about finite set cardinalities. These may be looked upon as a kind of `primitives' for solving traditional combinatorial problems (those about arrangements, permutations, and combinations) in a more formal manner.

\paragraph{Counting injections.} Let $A$, $B$ be finite sets and let $\mathrm{Inj}(A,B)$ be the set of all possible \emph{injections} from $A$ to $B$.
How many of these are possible? As $\mathrm{Inj}(A,B) \sbs B^A$, we see that the set $\mathrm{Inj}(A,B)$ is necessarily finite and $|\mathrm{Inj}(A,B)| \leq |B|^{|A|}$ (by Lemma~\ref{L10:num_func}). Then, we observe that the number $|\mathrm{Inj}(A,B)|$ \emph{does not} depend on the sets $A$, $B$ themselves but just on their sizes.

\mcomm{Given enough time (which is unlikely in practice), the Instructor might prove similar results for all the following `combinatorial numbers' (which are omitted traditionally). We do not do it nevertheless.}
\begin{lemma}[``just the size matters'']\label{L11:JSM}
	If $A' \sim A$ and $B' \sim B$, then $\mathrm{Inj}(A',B') \sim \mathrm{Inj}(A,B)$.
\end{lemma}
\begin{proof}
	Assume that $A' \stackrel{\phi}{\sim} A$ and $B' \stackrel{\psi}{\sim} B$. We need to get a bijection $\theta\colon \mathrm{Inj}(A',B') \to \mathrm{Inj}(A,B)$. So, for each $f \in \mathrm{Inj}(A',B')$, we define $\theta(f) = \psi \circ f \circ \phi^{-1}$. As a composition of injections is an injection itself, we have $\theta(f) \in \mathrm{Inj}(A,B)$.
	
	But why is $\theta$ injective? Indeed, suppose that $\theta(f) = \theta(g)$. Then,
	$$f = (\psi^{-1} \circ \psi) \circ f \circ (\phi^{-1} \circ \phi) = \psi^{-1} \circ \theta(f) \circ \phi = \psi^{-1} \circ \theta(g) \circ \phi = (\psi^{-1} \circ \psi) \circ g \circ (\phi^{-1} \circ \phi) = g.$$
	On the other hand, for each $h \in \mathrm{Inj}(A',B')$, we obtain
	$$\theta(\psi^{-1} \circ h \circ \phi) = (\psi \circ \psi^{-1}) \circ h \circ (\phi \circ \phi^{-1}) = h,$$
	where $\psi^{-1} \circ h \circ \phi \in \mathrm{Inj}(A',B')$. Thus, $\theta$ is surjective.
\end{proof}
This means that $|\mathrm{Inj}(A,B)| = |\mathrm{Inj}(\ul{m},\ul{n})|$ if $m = |A|$ and $n = |B|$, and it suffices to do all counting work just for our `exemplary' finite sets. Such a situation is common for traditional combinatorial numbers: \emph{just the size matters}. Now let us do the actual counting.

\begin{lemma}\label{L11:num_inj}
	For every numbers $n, m \in \N$, we have
	$$
	\begin{array}{rcll}
		|\mathrm{Inj}(\ul{m},\ul{n})| &=& \begin{cases}
			0&\mbox{if } m > n;\\
			\dfrac{n!}{(n-m)!}&\mbox{if } m \leq n,\\
		\end{cases}
	\end{array}
	$$
	where $0! = 1$ and $(n + 1)! = (n + 1)\cdot n! = (n+1)\cdot n \cdot (n - 1) \cdot \ldots 2 \cdot 1$ (such a number $n!$ is called the \emph{factorial} of $n$).
\end{lemma}
\begin{proof}
	If $m > n$, then $\mathrm{Inj}(\ul{m},\ul{n}) = \void$ due to the Pigeonhole Principle. Assume that $m \leq n$. Given that $\frac{n!}{(n-m)!} = n\cdot (n - 1) \cdot (n - 2) \cdot \ldots \cdot (n - m + 1)$, it is not hard to explain the required statement informally. Indeed, in order to specify an injection $f$ from $\ul{m}$ to $\ul{n}$, one has to choose its value at each point from $0$ to $m - 1$. There are $n$ ways to choose the value $f(0)$ (it can be anything from $\ul{n}$). But $f(1)$ cannot be the same value, so one has just $n - 1$ options for \emph{whatever} $f(0)$ has been chosen, etc.
	
	Yet let us give a more formal proof. We shall use induction on $m$. As $\mathrm{Inj}(\void,\ul{n}) = \{ \void \}$, for $m = 0$, we get $|\mathrm{Inj}(\ul{m},\ul{n})| = 1 = \frac{n!}{(n - 0)!}$. Consider $m + 1$ and $n \geq m + 1 > 0$; suppose that $|\mathrm{Inj}(\ul{m'},\ul{n'})| = \frac{n'!}{(n'-m')!}$ for every $m' \leq m$ and for \emph{whatever} $n'$ with $n' \geq m'$.
	
	Consider the set $X = \mathrm{Inj}(\ul{m+1},\ul{n})$. We can partition it into subsets according to the value $f(m)$. Indeed, let $X_k = \{ f\in X \mid f(m) = k \}$ for each $k \in \ul{n}$. Clearly, $X = X_0 \cup X_1 \cup \ldots \cup X_{n-1}$, while every two sets of the form $X_k$ are disjoint. By the Rule of Sum, $|X| = |X_0| + |X_1| + \ldots + |X_{n-1}|$.
	
	But how many elements does $X_k$ have? In fact, every $f \in X_k$ is completely determined by its values at the points $0,\ldots,m-1$, that is, by the function $f \rst \ul{m} \in \mathrm{Inj}(\ul{m}, \ul{n} \setminus \{k\})$ ($k$ is not a value of $f\rst \ul{m}$, for otherwise $f$ would not be injective). So, taking Lemma~\ref{L11:JSM} (``just the size matters'') into account, we obtain:
	$$X_k \sim \mathrm{Inj}(\ul{m}, \ul{n} \setminus \{k\}) \sim \mathrm{Inj}(\ul{m}, \ul{n - 1}).$$
	As $n \geq m + 1$, then $n - 1 \geq m$, so the inductive hypothesis is applicable, which gives us
	$$|X_k| = |\mathrm{Inj}(\ul{m}, \ul{n - 1})| = \dfrac{(n-1)!}{((n-1) - m)!}.$$
	Remarkably, $|X_k|$ does not depend on $k$, so
	$$|\mathrm{Inj}(\ul{m+1},\ul{n})| = |X_0| + |X_1| + \ldots + |X_{n-1}| = n \cdot \dfrac{(n-1)!}{((n-1) - m)!} = \dfrac{n!}{(n- (m + 1))!},$$
	as required.
\end{proof}
They traditionally call $|\mathrm{Inj}(\ul{m},\ul{n})|$ \emph{the number of partial permutations} or \emph{(ordered) arrangements} of length $m$ of $n$ elements. This is then denoted by $P^m_n$ or $A^m_n$, etc.
Let us see what idea is behind such terminology. An \emph{arrangement} of length $m$ of a set $A$ elements is a tuple $(a_0,\ldots,a_{m-1})$ where each $a_i \in A$ and $a_i \neq a_j$ if $i \neq j$. As we know from Theorem~\ref{L9:t_tuples}, the set $A^m$ of \emph{all} tuples is equivalent to the set $A^{\ul{m}}$ of functions $\ul{m} \to A$ via the natural bijection which maps $(a_0,\ldots,a_{m-1})$ to the function $f \colon i \mapsto a_i$. Clearly, for an \emph{arrangement}, the function $f$ will be an injection, hence the set of arrangements is equivalent to $\mathrm{Inj}(\ul{m},A)$. When combined with the ``just the size matters'' principle, this justifies the equation $A^m_n = |\mathrm{Inj}(\ul{m},\ul{n})|$.


Any bijection from a set $A$ to $A$ is usually called a \emph{permutation} of $A$ (particularly, when $A$ is finite). How many permutations of a finite set are possible? In analogy with Lemma~\ref{L11:JSM}, this question is reducible to the following one: how many permutations of $\ul{n}$ are possible?

\begin{lemma}\label{L11:num_bij}
	For every number $n \in \N$, there are exactly $n!$ distinct permutations of the set $\ul{n}$.
\end{lemma}
\begin{proof}
	Let us denote the set of all possible permutations of $\ul{n}$ by $X$. Clearly, $X \sbs \mathrm{Inj}(\ul{n},\ul{n})$. On the other hand, every injection from $\ul{n}$ to $\ul{n}$ is surjective by Theorem~\ref{L10:fin_sur_in}; hence, it is a bijection. So, $\mathrm{Inj}(\ul{n},\ul{n}) \sbs X$. Finally, we apply Lemma~\ref{L11:num_inj} to obtain
	$$|X| = |\mathrm{Inj}(\ul{n},\ul{n})| = \dfrac{n!}{(n-n)!} = \dfrac{n!}{1} = n!.$$
\end{proof}

\paragraph{Counting subsets.} As we know, a finite set $A$ has as many as $2^{|A|}$ distinct subsets. But what is the count for the subsets of some fixed size $k$? We denote by $\mP_k(A)$ the set of all subsets of $A$ that are of cardinality $k$. Similarly to Lemma~\ref{L11:JSM}, it is easy to check that $\mP_k(A) \sim \mP_k(B)$ when $A \sim B$. Again, just the size matters. Hence, it suffices to know $|\mP_k(\ul{n})|$ for diverse $n \in \N$, which number is traditionally denoted by $C_n^k$ (``$n$ choose $k$'') since \emph{combinations} is the traditional name for fixed size subsets.

Clearly, $C_n^k = 0$ if $k > n$, since the Pigeonhole Principle forbids a subset to be greater in size than the whole set. Likewise, $C_n^0 = 1$ as the empty set is unique, and $C_n^n = 1$ for if there were two $n$-sized \emph{distinct} subsets $X$ and $Y$ of $\ul{n}$, then it would be that $\bar X \neq \bar Y$ despite $X = \void = Y$ (clearly, $|\bar X| = |\bar Y| = n - n = 0$ by the Rule of Sum).

\begin{lemma}[Pascal's Identity]\label{L11:pascal}
	For every $n, k \in \N$, it holds that
	$$C_{n + 1}^{k + 1} = C_n^{k + 1} + C^k_n.$$
\end{lemma}
\begin{proof} We will give a so-called \emph{combinatorial proof} for this identity, that is, one based on counting the same quantity in two distinct ways.
	
	Consider the set $A = \{ X \in \mP_{k + 1}(\ul{n+1}) \mid n \in X \}$ and its complement $\bar A = \{ X \in \mP_{k + 1}(\ul{n+1}) \mid n \notin X \}$ (i.\,e., we form two classes of subsets based on whether they contain $n \in \ul{n+1}$ or not). By the Rule of Sum, $C^{k+1}_{n+1} = |\mP_{k + 1}(\ul{n+1})| = |A| + |\bar A|$. 
	
	Clearly, $\bar A = \mP_{k + 1}(\ul{n})$ (as $X \in \bar A$ implies $X \sbs \ul{n}$), whence $|\bar A| = C_n^{k+1}$. On the other hand, one has $X = (X \cap \ul{n}) \cup \{ n \}$ for every $X \in A$. By the Rule of Sum, the set $X' = X \cap \ul{n}$ is of cardinality $k$. Then the mapping $X \mapsto X'$ is a clear bijection from $A$ to $\mP_k(\ul{n})$. So, $|A| = C_n^k$. The required identity is now immediate.
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{pascal.pdf}
	\caption{Proving Lemma~\ref{L11:pascal}: here $X \in A$ and $Y \in \bar A$; the $k$-element set $X'$ is highlighted.}
\end{figure}


\noindent The above lemma allows to obtain an explicit formula for the number $C_n^k$.

\begin{lemma}
	For every $n, k \in \N$, if $k \leq n$, then
	$$C_n^k = \dfrac{n!}{(n-k)! \cdot k!}.$$
\end{lemma}
\begin{proof}
	First, we notice that $1 = C_n^0 = \frac{n!}{n!} = \frac{n!}{(n - 0)! \cdot 0!}$ for every $n$. Let us assume that $k > 0$. We prove the formula by induction on $n$. If $n = 0$, then $k = 0$, which contradicts our assumption.
	
	Consider numbers $n + 1$ and $k$ such that $k \leq n + 1$, and assume that $C_n^{k'} = \dfrac{n!}{(n-k')! \cdot k'!}$ for whatever $k'$ with $k' \leq n$ (the inductive hypothesis).
	
	As $k > 0$, we have $k = k' + 1$ and $k' + 1 \leq n + 1$ for a suitable $k'$. Hence, $k' \leq n$, so the inductive hypothesis applies. By Pascal's Identity, we get:
	$$C_{n + 1}^{k' + 1} = C_n^{k' + 1} + C_n^{k'}.$$
	If $k' + 1 \leq n$, the IH is also applicable to the term $C_n^{k' + 1}$, whence
	\begin{multline*}
		C_{n+1}^k = C_{n + 1}^{k' + 1} = \dfrac{n!}{(n-k'-1)! \cdot (k'+1)!} + \dfrac{n!}{(n-k')! \cdot k'!} =
		\dfrac{n!\cdot (n - k') + n! \cdot (k' + 1)}{(n-k')! \cdot (k'+1)!} =\\
		\dfrac{n!\cdot (n + 1)}{((n+1)-(k'+1))! \cdot (k'+1)!} = \dfrac{(n + 1)!}{((n+1)-(k'+1))! \cdot (k'+1)!} = \dfrac{(n + 1)!}{((n+1)-k)! \cdot k!},
	\end{multline*}
	as it is required.
	
	Now consider the case when $k' + 1 > n$, whence $n + 1 \leq k' + 1 \leq n + 1$. Then $k' = n$, $k = n + 1$, and
	$$C_{n+1}^k =  C_{n+1}^{n+1} = 1 = \dfrac{(n + 1)!}{((n+1)-(n+1))! \cdot (n+1)!} = \dfrac{(n + 1)!}{((n+1)-k)! \cdot k!} .$$
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{pascal_triang.pdf}
	\caption{This \emph{Pascal's Triangle} arranges $C_n^k$ numbers. The equations $C_n^0 = 1$ and $C_n^k = 0$ for $k > 0$ describe the first column and row, respectively. All the other
		numbers are determined by Lemma~\ref{L11:pascal}.}
\end{figure}


\begin{rem}
	It is easy to see that for $k \leq n$, one has $C_n^k = \dfrac{|\mathrm{Inj}(k,n)|}{k!}$. So, there are $k!$ times more injections from $\ul{k}$ to $\ul{n}$ than there are $k$-sized subsets of $\ul{n}$. If proved independently, this gives another way to obtain the formula for $C_n^k$. Such a proof is usually presented as follows: we have $k!$ times more \emph{arrangements} than subsets, as each subset of size $k$ can be arranged in exactly $k!$ distinct ways ($k$-sized arrangements of $k$ elements are \emph{permutations}). One, however, needs \emph{equivalence relations} and \emph{classes} described below in order to make this argument rigorous.
\end{rem}

\noindent One easy property of numbers $C_n^k$ with a combinatorial proof is as follows.
\begin{lemma}
	For every $n, k \in \N$, if $k \leq n$, then $C_n^k = C_n^{n-k}$.
\end{lemma}
\begin{proof}
	It suffices to prove that $\mP_k(\ul{n}) \sim \mP_{n - k}(\ul{n})$. Indeed, consider a function $\phi$ such that $\phi(X) = \bar X$ for every $X \in  \mP_k(\ul{n})$. By the Rule of Sum, $|X| + |\bar X| = n$, hence $\phi \colon\mP_k(\ul{n}) \to \mP_{n - k}(\ul{n})$. If $\phi(X) = \phi(Y)$, then $X = \overline{\phi(X)} = \overline{\phi(Y)} = Y$, so $\phi$ is injective. On the other hand, $Z = \phi(\bar Z)$ for each $Z \in \mP_{n - k}(\ul{n})$, so $\phi$ is surjective.
\end{proof}
\begin{corr}\label{L11:bin_add}
	$C_{a + b}^a = C_{a + b}^b$.
\end{corr}

From high-school mathematics, we know the \emph{binomial formula} $(a + b)^2 = a^2 + 2ab + b^2$. Let us generalize this formula to an arbitrary natural exponent.

\begin{lemma}\label{L11:binom_1}
	For every numbers $x \in \R$ and $n \in \N$,
	$$(1 + x)^n = 1 + C_n^1 x + C_n^2 x^2 + \ldots + C_n^{n-1}x^{n-1} + x^n = \sum_{k = 0}^n C_n^k x^k.$$
\end{lemma}
\begin{proof}
	Induction on $n$. For $n = 0$, this is obvious. Assume that $(1 + x)^n = \sum_{k = 0}^n C_n^k x^k$. Then, applying the IH, re-indexing (we replace $k + 1$ with $k$), applying~\ref{L11:pascal} and the fact that $C_m^0 = C_m^m = 1$ for every $m$, we obtain:
	\begin{multline*}
		(1 + x)^{n + 1} = (1 + x)(1 + x)^n =  (1 + x) \cdot \sum_{k = 0}^n C_n^k x^k = \sum_{k = 0}^n C_n^k x^k  + \sum_{k = 0}^n C_n^k x^{k + 1} =\\
		\sum_{k = 0}^n C_n^k x^k  + \sum_{k = 1}^{n+1} C_n^{k - 1} x^{k} =
		C_n^0 x^0 + \sum_{k = 1}^n (C_n^k + C_n^{k-1}) x^k + C_n^n x^{n+1} =\\
		C_{n+1}^0 x^0 + \sum_{k = 1}^n C_{n+1}^k x^k + C_{n+1}^{n+1} x^{n+1} = \sum_{k = 1}^{n+1} C_{n+1}^k x^k,
	\end{multline*}
	as it is required.
\end{proof}

\begin{corr}[Binomial Theorem]
	For every numbers $a, b \in \R$ and $n \in \N$,
	$$(a + b)^n = \sum_{k = 0}^n C_n^k a^k b^{n-k}.$$
\end{corr}
\begin{proof}
	If $b = 0$, then the right-hand side equals $C_n^n a^n b^0 = a^n$, that is, the left-hand side.
	
	Suppose that $b \neq 0$. Then
	$$(a + b)^n = (b + a)^n = b^n \cdot \left (1 + \dfrac{a}{b} \right)^n = b^n \cdot \sum_{k = 0}^n C_n^k \left ( \dfrac{a}{b} \right)^k = \sum_{k = 0}^n C_n^k a^k b^{n-k}.$$
\end{proof}
\noindent The above theorem justifies the name \emph{binomial coefficients} for the numbers $C_n^k$.

\begin{exm}\label{L11:pow_2}
	For every $n \in \N$, we have $\sum_{k = 0}^n C_n^k = 2^n$.
	
	Indeed, it is enough to apply the Binomial Theorem to $a = 1$ and $b = 1$. Another way to verify this identity is a combinatorial one: both sides clearly denote the number of all subsets of the set $\ul{n}$ since $\mP(\ul{n}) = \mP_0(\ul{n}) \cup \mP_1(\ul{n}) \cup \ldots \cup \mP_n(\ul{n})$ and the Rule of Sum is applicable here.
\end{exm}

\begin{exc}
	For every $n \in \N$, prove that $\sum_{k = 0}^n (-1)^k C_n^k = 0$.
\end{exc}

\begin{exm}
	Yet another typical application of binomial coefficients (which many others can be reduced to) is as follows. Consider the equation
	$$x_1 + x_2 + \ldots + x_m = n,$$
	where $m \in \N_+$ and $n \in \N$. A \emph{solution} to this equation is just a tuple $(x_1,\ldots, x_m) \in \N^m$. How many solutions does the equation have?
	
	Let us encode \emph{every} $m$-tuple $\vec y = (y_1, y_2, \ldots,y_m)$ of naturals by a binary word (i.\,e., a tuple of the set $\ul{2}$ elements) $b(\vec y) = \underbrace{11\ldots1}_{y_1}0\underbrace{11\ldots1}_{y_2}0\ldots 0 \underbrace{11\ldots1}_{y_m}$. Clearly, this encoding is injective. The symbol $0$ occurs in $b(\vec y)$ just $m - 1$ times, while $1$ has exactly $y_1 + y_2 + \ldots + y_m$ many occurences. So, $\vec y$ is a solution to our equation iff $b(\vec y)$ is a binary word of length $n + m - 1$ with exactly $m - 1$ zeroes. Since $b$ is injective, this mapping provides a bijection from the set of solutions to the set of binary words of the said form. Let us count the latter.
	
	In its turn, each binary word may be bijectively encoded by the set of \emph{position} numbers for zeroes. This is an $(m-1)$-sized subset of a $(n + m - 1)$-element set (we choose $m-1$ positions among $n + m -1$ possible). Hence, there are $C_{n + m -1}^{m - 1}$ binary words of the form in question. Finally, in view of Corollary~\ref{L11:bin_add}, this gives us $C_{n + m -1}^{m - 1} = C_{n + m -1}^{n}$ distinct solutions to the original equation.
\end{exm}
\mcomm{It is important to highlight diverse bijections and `encodings' which reduce a problem to formal primitives. For a seminar class, we recommend to make them explicit in a few introductory problems and switch to the traditional ``intuitive'' style afterward (except for the most tricky arguments). Ideally, students should be able to translate intuitive combinatorial proofs into the set-theoretic formalism freely. The fact that most traditional combinatorial problems are stated informally makes this task quite hard.}
\begin{exm}
	What is the number of \emph{positive} integer solutions to the equation $x_1 + x_2 + \ldots + x_m = n$, i.\,e., those with all $x_i > 0$?
	
	Each natural number $x_i > 0$ can be uniquely expressed as $y_i + 1$ for some $y_i \geq 0$. Then $\vec x$ is a positive solution to the equation iff one has $(y_1 + 1) + (y_2 + 1) + \ldots + (y_m + 1) = n$. The latter equation has no natural solution when $n < m$ but is equivalent to $y_1 + y_2 + \ldots y_m = n - m$ otherwise. By the above, there are $C_{(n - m) + m - 1}^{m - 1} = C_{n - 1}^{m - 1}$ distinct solutions $\vec y$. As $\vec x$ bijectively corresponds to $\vec y$, there are exactly $C_{n - 1}^{m - 1}$ distinct positive solutions to the original equation when $n, m > 0$.
\end{exm}

\paragraph{Inclusion--Exclusion Principle.} From Corollary~\ref{L10:in-ex}, we know that $|A \cup B| = |A| + |B| - |A \cap B|$ for any two finite sets $A$ and $B$. It is not hard to extend this fact to three finite sets $A, B, C$. Indeed, since $(A \cup B) \cap C = (A \cap C) \cup (B \cap C)$ and $(A \cap C) \cap (B \cap C) = A \cap B \cap C$, we obtain
\begin{multline*}
	|(A \cup B) \cup C| = |A \cup B| + |C| - |(A \cup B) \cap C| =\\
	|A| + |B| - |A \cap B| + |C| - |(A \cap C) \cup (B \cap C)| =\\
	|A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |(A \cap C) \cap (B \cap C)| =\\
	|A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|.
\end{multline*}
It is possible to further generalize this formula, yet we will omit the proof (for its being somewhat clumsy), which is based on Corollary~\ref{L10:in-ex} and induction on $n$.
\begin{thm}[Inclusion--Exclusion Principle]\label{comb2:ex-in-gen}
	For arbitrary finite sets $A_1, A_2, \ldots, A_n$, it holds that
	\begin{multline*}
		|A_1 \cup A_2 \cup \ldots \cup A_n| = \sum\limits_{k = 1}^n (-1)^{k - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_k \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}| =\\
		\sum\limits_{1\leqslant i_1 \leqslant n} |A_{i_1}| - \sum\limits_{1\leqslant i_1 < i_2 \leqslant n} |A_{i_1} \cap A_{i_2}| + \sum\limits_{1\leqslant i_1 < i_2 < i_3 \leqslant n} |A_{i_1} \cap A_{i_2} \cap A_{i_3}| - \ldots\\
		+ (-1)^{n - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_n \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_n}|= \\[3pt]
		(|A_1| + |A_2| + \ldots + |A_n|) - (|A_1 \cap A_2| + |A_1 \cap A_3| +  \ldots + |A_2 \cap A_3| + \ldots + |A_{n -1} \cap A_n|) +\\[3pt]
		+ (|A_1 \cap A_2 \cap A_3| + |A_1 \cap A_2 \cap A_4| +  \ldots + |A_2 \cap A_3 \cap A_4| + \ldots + |A_{n -2} \cap A_{n -1} \cap A_n|) - \ldots\\[3pt]
		+ (-1)^{n - 1} |A_1 \cap A_2 \cap \ldots \cap A_n|.\\
	\end{multline*}
\end{thm}

\begin{exm}
	Let us count the number of \emph{surjections} between two finite sets. Likewise the ``just the size matters'' principle, it suffices to know $|\mathrm{Sur}(\ul{m},\ul{n})|$ for every $n, m \in \N$, where $\mathrm{Sur}(A,B)$ is the set of all surjections from a set $A$ to a set $B$.
	
	We know the total number of functions $\ul{m} \to \ul{n}$ to equal $n^m$. The idea is to count \emph{non-surjections} first and then subtract the number thereof from $n^m$ (by the Rule of Sum).  Let $A_{k + 1}$, for $k \in \ul{n}$, be the set of all functions $\ul{m} \to \ul{n}$ that \emph{do not} take the value $k$, that is, $A_{k + 1} = \{ f \in \ul{n}^{\ul{m}} \mid k \notin \rng f \} = (\ul{n} \setminus \{k\})^{\ul{m}}$. Clearly, a function must belong to at least one of $A_k$ in order to be a non-surjection. Then, by the Inclusion--Exclusion Principle,
	\begin{multline*}
		|\overline{\mathrm{Sur}(\ul{m},\ul{n})}| = |A_1 \cup A_2 \cup \ldots \cup  A_n| =\\
		\sum\limits_{s = 1}^n (-1)^{s - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_s \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}| =\\
		\sum\limits_{1\leqslant i_1 \leqslant n} |A_{i_1}| - \sum\limits_{1\leqslant i_1 < i_2 \leqslant n} |A_{i_1} \cap A_{i_2}| + \sum\limits_{1\leqslant i_1 < i_2 < i_3 \leqslant n} |A_{i_1} \cap A_{i_2} \cap A_{i_3}| - \ldots\\
		+ (-1)^{n - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_n \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_n}|.
	\end{multline*}
\end{exm}
What is the set $A_{i_1} \cap A_{i_2} \cap \ldots \cap  A_{i_s}$ for pairwise distinct indices $i_1, i_2, \ldots, i_s$? Clearly, it is the set of functions from $\ul{m}$ to $\ul{n}$ which take no value among $i_1, i_2, \ldots, i_s$, i.\,e., it the set $(\ul{n} \setminus \{ i_1, i_2, \ldots, i_s \})^{\ul{m}}$. According to Theorem~\ref{L10:num_func} and the Rule of Sum, this set has cardinality $(n - s)^m$ for whatever choice of $i_1, i_2, \ldots, i_s$.

Then
$$\sum\limits_{1\leqslant i_1 < i_2 < \ldots  < i_s \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}| = (n - s)^m \cdot \sum\limits_{1\leqslant i_1 < i_2 < \ldots  < i_s \leqslant n} 1 = C_n^s (n-s)^m$$
for there are $C_n^s$ many ways to choose a subset $\{ i_1, i_2, \ldots, i_s \}$ from $\ul{n}$ (and just one way to sort it in ascending order). Combined with the Inclusion--Exclusion Principle, this yields:
\begin{multline*}
	|\mathrm{Sur}(\ul{m},\ul{n})| = n^m - |A_1 \cup A_2 \cup \ldots \cup A_n| =\\
	n^m - \sum\limits_{s = 1}^{n} (-1)^{s - 1} C_n^s (n-s)^m = C_n^0 (n - 0)^m + \sum\limits_{s = 1}^{n} (-1)^{s} C_n^s (n-s)^m = \sum\limits_{s = 0}^{n} (-1)^{s} C_n^s (n-s)^m.
\end{multline*}

\begin{exm}
	From Theorem~\ref{L10:pigeon_dual}, we know that $|\mathrm{Sur}(\ul{m}, \ul{n})| = 0$ if $n > m$. Hence we obtain a \emph{purely arithmetical} non-trivial fact:
	$$\sum\limits_{s = 0}^{n} (-1)^{s} C_n^s (n-s)^m = 0$$
	when $n > m$. Consider another example of this kind. From Theorem~\ref{L10:fin_sur_in}, it follows that every surjection $\ul{n} \to \ul{n}$ is a bijection (and vice versa, of course). As the number of such bijections equals $n!$ (by Lemma~\ref{L11:num_bij}), we see that
	$$\sum\limits_{s = 0}^{n} (-1)^{s} C_n^s (n-s)^n = n!,$$
	which is another interesting arithmetical fact we have proved in a combinatorial manner (i.\,e., by counting the same quantity in two ways).
\end{exm}

\mcomm{Sometimes, mathematical modeling of a ``real-world'' problem is non-trivial (and much more so for genuine \emph{real-world}) and the ``answer'' may depend on the model essentially. While the example below is straightforward in any respect, we urge the students to pay attention to modeling details. This will be especially important for probabilistic problems in  sections below.}

\begin{exm}
	How many ways are there to place six distinct balls into five distinct boxes so that no box remains empty?
	
	First, we need a mathematical model for this `real-world' problem. We can assign consecutive naturals to balls and boxes (separately) and then identify balls and boxes with their respective numbers. This way we obtain the sets $\ul{6}$ and $\ul{5}$ as formal models for the said collections. What is a `way to place' a ball into a box? Essentially, it is an assignment of boxes to balls such that every ball has exactly one box assigned. Clearly, functions capture this idea exactly. So, a `way to place' is just a function from $\ul{6}$ to $\ul{5}$. Finally, we require no box to be empty, that is, every box must be assigned to a certain ball. In other words, our function must be surjective.
	
	The question is thus reduced to the following: how many surjections from $\ul{6}$ to $\ul{5}$ exist? We know the answer---it is
	$$|\mathrm{Sur}(\ul{6}, \ul{5})| = \sum\limits_{s = 0}^{5} (-1)^{s} C_5^s (5-s)^6 = C_5^0 \cdot 5^6 - C_5^1 \cdot 4^6 + C_5^2 \cdot 3^6 - C_5^3 \cdot 2^6 + C_5^4 \cdot 1^6 - C_5^5 \cdot 0^6 = 1800.$$
	
\end{exm}

How many ways are there to order numbers $1, 2, \ldots, n$ in such a way that no $k$ takes the $k$-th place? Clearly, this is just the number of bijections $f\colon\ul{n} \to \ul{n}$ with $f(k) \neq k$ for each $k \in \ul{n}$. Such bijections are called \emph{derangements} of the set $\ul{n}$.
\begin{exm}
	Let us count all the derangements of the set $\ul{n}$.
	
	As in the above, we count \emph{non-derangement} permutations first. Let
	$$A_{k + 1} = \{f\colon \ul{n} \to \ul{n} \mid f\mbox{ is bijective and } f(k) = k\}$$
	for every $k \in \ul{n}$. We need to calculate $|A_1 \cup A_2 \cup\ldots\cup A_n|$. By the Inclusion--Exclusion Principle,
	$$|A_1 \cup A_2 \cup \ldots \cup A_n| = \sum\limits_{s = 1}^{n} (-1)^{s - 1} \sum\limits_{0 \leqslant i_1 < i_2 < \ldots < i_s  \leq n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}|.$$
	Clearly, for any distinct $i_1, i_2, \ldots, i_s$, the set $A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}$ is the set of all bijections $\ul{n} \to \ul{n}$ with their values at $i_1, i_2,\ldots, i_s$ fixed. These bijections are in bijective correspondence with permutaions of the set $\ul{n}\setminus \{m_1,m_2,\ldots,m_s\}$. There are just $(n-s)!$ such permutations. Hence, the overall number of derangements equals
	$$n! - \sum\limits_{s = 1}^{n} (-1)^{s - 1} C_{n}^s (n-s)! = n! + \sum\limits_{s = 1}^{n} (-1)^{s} \dfrac{n!}{s!} = n!\sum\limits_{s = 0}^{n} (-1)^{s} \dfrac{1}{s!}.$$
\end{exm}
The share of derangements in the total number of bijections thus equals $\sum\limits_{s = 0}^{n} (-1)^{s} \dfrac{1}{s!}$. From Calculus, we know that for $n \to +\infty$, this sum converges to $e^{-1}$. In probabilistic terms (see Section~\ref{sect:prob}), this fact can be interpreted the following way: the more cards one has in an ordered deck, the closer to $e^{-1} = 0.3678\ldots$ is the probability that each card will change its position after a random shuffle. It might be thought somewhat counter-intuitive that this probability does not tend to $1$ nor to $0$.


\begin{exm}
	Our last example here is about computing values for Euler's totient function $\phi$. Let us recall that for every natural $m > 1$, $\phi(m)$ equals the number of the set $\ul{m}$ elements coprime with $m$.
	
	Assume that $m = p^{a_1}_1\ldots p_n^{a_n}$ for some pairwise distinct primes $p_i$, where each $a_i > 0$. Let $A_k = \{ x \in \ul{m} \mid p_k \dvd x \}$. Clearly, a number $x$ is \emph{not} coprime with $m$ iff they share some prime divisor $p_k$. So, $A_1 \cup A_2 \cup \ldots \cup A_n$ is the set of numbers that are not coprime with $m$ among the elements of $\ul{m}$. By the Inclusion--Exclusion Principle,
	\begin{multline*}
		\phi(m) = m - |A_1 \cup A_2 \cup \ldots \cup A_n| = \\
		m - \sum\limits_{1\leqslant i_1 \leqslant n} |A_{i_1}| + \sum\limits_{1\leqslant i_1 < i_2 \leqslant n} |A_{i_1} \cap A_{i_2}| - \sum\limits_{1\leqslant i_1 < i_2 < i_3 \leqslant n} |A_{i_1} \cap A_{i_2} \cap A_{i_3}| + \ldots\\
		+ (-1)^{n} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_n \leqslant n} |A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_n}|.
	\end{multline*}
	For each $x \in \ul{m}$, we have $x \in A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}$ iff  $p_{i_t} \dvd x$ for all $t$, which is equivalent to $p_{i_1} p_{i_2} \ldots p_{i_s} \dvd x$. The latter is equivalent to $x =  p_{i_1} p_{i_2} \ldots p_{i_s} \cdot l$, where $0 \leq l < \frac{m}{p_{i_1} p_{i_2} \ldots p_{i_s}}$. Thus, $|A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_s}| = m p^{-1}_{i_1} p^{-1}_{i_2} \ldots p^{-1}_{i_s}$ and
	\begin{multline*}
		\phi(m) = m \bigl(1  - (p^{-1}_{1} + p^{-1}_2 + \ldots + p^{-1}_n) + \sum\limits_{1\leqslant i_1 < i_2 \leqslant n} p^{-1}_{i_1} p^{-1}_{i_2} - \sum\limits_{1\leqslant i_1 < i_2 < i_3 \leqslant n} p^{-1}_{i_1} p^{-1}_{i_2} p^{-1}_{i_3} + \ldots\\
		+ (-1)^{n}  p^{-1}_{1} p^{-1}_{2} \ldots  p^{-1}_{n} \bigr) = m (1 - p^{-1}_1) (1 - p^{-1}_2) \ldots (1 - p^{-1}_n).
	\end{multline*}
	If you do not believe the last equation, you may first try it for small values of $n$ like $3$ and $4$, then prove it by induction on $n$.
\end{exm}
\begin{exm}
	As $12 = 2^2 \cdot 3$, we have $\phi(12) = 12 \cdot (1 - \frac{1}{2})\cdot (1 - \frac{1}{3}) = 12 \cdot \frac{1}{2} \cdot \frac{2}{3} = 4$. On the other hand, $1$, $5$, $7$, $11$ are the only numbers from $\ul{12}$ that are coprime with $12$.
\end{exm}

\section{Orders}\label{sect:orders}
\mcomm{The two following  sections contain the most common concepts concerning orders, equivalence relations, and partitions. While these topics are abstract, their usefulness for most mathematical courses outweighs the students' likely frustration. To make things more bearable, the Instructor may widely use previous sections as a source of concrete examples and, on the other hand, apply combinatorics to finite orders and equivalences. To attain the latter goal, we count partitions of a finite set and prove Dilworth's Theorem.}


Besides functions, there are two other important classes of special binary relations: those of \emph{orders} and of \emph{equivalences}. While being inherently abstract, these concepts admit natural set-theoretic definitions. A binary relation $R$ is called:
\begin{enumerate}
	\item \emph{reflexive for a set $Z$} iff $\forall x \in Z\, (x,x) \in R$;
	\item \emph{irreflexive} iff $\forall x\, (x,x) \notin R$;
	\item \emph{symmetric} iff $\forall x \forall y\; (x R y \ply y R x)$;
	\item \emph{antisymmetric} iff $\forall x \forall y\; \bigl( (x R y \wedge y R x) \ply x = y \bigr)$;
	\item \emph{transitive} iff $\forall x \forall y \forall z\; \bigl( (x R y \wedge y R z) \ply x R z \bigr)$.
\end{enumerate}
Clearly, the reflexivity property is relative to a parameter $Z$, while all the others are inherent in $R$. A relation $R$ on a set $A$ is just called \emph{reflexive} when it is reflexive for $A$.

In `arrow' terms, reflexivity endows each point of $A$ with a loop; irreflexivity means absence of any loops; symmetry secures a converse for every arrow; antisymmetry guarantees loops to be only possible arrows with a converse; finally, transitivity procures a one arrow `bypass' for every two arrow path.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{ord_refl.pdf}
	\caption{(a) Reflexivity: a loop at each point; (b) irreflexivity: no loops; (c) symmetry; (d) antisymmetry: no such cycle unless $x = y$; (e) transitivity.}
\end{figure}

\begin{exm}
	The relation $\id_A$ on a set $A$ is reflexive, symmetric, antisymmetric, and transitive. The relation $\void$ is irreflexive, symmetric, antisymmetric, and transitive. Indeed, say, the assumption $(x,y), (y,z) \in \void$ is always false and implies thus anything---including the statement $(x,z) \in \void$, whence transitivity of $\void$ follows.
	
	
	The relations ${<}$ and ${\leq}$ on the set $\N$ are transitive and antisymmetric (the assumptions $x < y$ and $y < x$ are inconsistent, so they imply $x = y$), while ${<}$ is irreflexive and ${\leq}$ is reflexive.
	
	The relation ${\sbs}$ on a set $\mP(A)$ is reflexive, antisymmetric, and transitive. The relations ${\sim}$ and ${\lesssim}$ on $\mP(A)$ are reflexive and transitive, whereas ${\sim}$ is symmetric. If the set $A$ has at least two distinct elements $a$ and $b$, then ${\lesssim}$ is neither symmetric ($\void \lesssim \{a\}$ but $\{a\} \not\lesssim \void$), nor antisymmetric ($\{a\} \lesssim \{b\}$ and $\{b\} \lesssim \{a\}$ but $\{a\} \neq \{b\}$).
\end{exm}

\noindent It is convenient to describe our properties in terms of algebraic operations.
\begin{lemma}
	A relation $R \sbs A^2$ is
	\begin{enumerate}
		\item $\mbox{reflexive}\ \iff \id_A \sbs R$;
		\item $\mbox{irreflexive}\ \iff \id_A \cap R = \void$;
		\item $\mbox{symmetric}\ \iff R \sbs R^{-1} \iff R = R^{-1} \iff R^{-1} \sbs R$;
		\item $\mbox{antisymmetric}\ \iff R \cap R^{-1} \sbs \id_A$;
		\item $\mbox{transitive}\ \iff R \circ R \sbs R$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let us check the last three statements. If $R$ is symmetric and $(x,y) \in R$, then, by definition, $(y,x) \in R$, whence $(x,y) \in R^{-1}$. So, $R \sbs R^{-1}$. But this implies $R^{-1} \sbs (R^{-1})^{-1} = R$ and $R = R^{-1}$, which, in turn, yields symmetry.
	
	Given $R \cap R^{-1} \sbs \id_A$, for each $x$ and $y$, from $xRy$ and $x R^{-1} y$, it follows that $x\, \id_A\, y$. Equivalently, from $xRy$ and $y R x$, it follows that $x = y$. The latter statement but means $R$ is antisymmetric.
	
	Let $R$ be transitive and $(x,y) \in R \circ R$. Then there exists $z$ such that $(x,z) \in R$ and $(z,y) \in R$. By transitivity, $(x,y) \in R$. For the other direction, let $R \circ R \sbs R$, $x R z$, and $z R y$. Then $(x,y) \in R \circ R$ and $x R y$. Hence, $R$ is transitive.
\end{proof}

\begin{exm}
	When is a relation $R \sbs A^2$ both symmetric and antisymmetric? For every such $A$, one has $R = R \cap R = R \cap R^{-1} \sbs \id_A$. Hence, $R \sbs \id_A$. Conversely, suppose $R \sbs \id_A$. Then $R \cap R^{-1} \sbs R \sbs \id_A$, whence $R$ is antisymmetric. Also, if $xRy$, then $x = y$, which gives $yRx$. Therefore $R$ is symmetric.
\end{exm}

\begin{exm}\label{ch0:comp_trans}
	If relations $P$ and $Q$ are transitive, then $P \cap Q$ is transitive too. Indeed, by Exercise~\ref{ch0:ex11} and Example~\ref{ch0:exm17}, we have
	\begin{multline*}
		(P \cap Q)\circ(P \cap Q) \sbs ((P \cap Q)\circ P) \cap ((P \cap Q) \circ Q) \sbs\\
		\sbs (P \circ P) \cap (Q \circ P) \cap (P \circ Q) \cap (Q \circ Q) \sbs\\
		\sbs (P \circ P) \cap  (Q \circ Q) \sbs P \cap Q.
	\end{multline*}
	We have applied transitivity of $P$ and $Q$ for the last step.
\end{exm}

\begin{exc}
	Prove that a relation $R \circ R^{-1}$ is always symmetric.
\end{exc}

\begin{exc}
	Let relations $P$ and $Q$ be symmetric. Prove that the relation $P \circ Q$ is symmetric iff $P \circ Q = Q \circ P$.
\end{exc}


\paragraph{Order relations.} A relation $R$ on a set $A$ is called a \emph{strict partial order} (or simply a \emph{strict order}) \emph{on} $A$, if $R$ is both irreflexive and transitive. Sometimes, they say \emph{`ordering'} instead of \emph{`order'}.
\begin{exm}
	For any set  $A$, the relation $\void$ is a strict order. The relations ${<}$ and ${>}$ on the set $\N$ are strict orderings, while ${\leq}$ is not for its being reflexive. The relation ${\subsetneq}$ on a set $\mP(A)$ is a strict ordering as well.
\end{exm}

\begin{exm}\label{ch0:induced0}
	Let $A$ be a set, $f\colon A \to \N$ and a relation $R \sbs A^2$ be such that $x R y \iff f(x) < f(y)$ for every $x, y \in A$ (think of $f(x)$ as the `price' of a `product' $x \in A$). Then the relation $R$ is a strict partial ordering. Evidently, one can substitute any strict ordering for the ordering ${<}$ on $\N$. The function $f\colon A \to B$ thus `translates' an order from $B$ onto $A$ (or, as they usually say, $f$ \emph{induces} an order on $A$).
\end{exm}

\begin{rem}\label{ch0:sord_asym}
	A strict order $R$ is always antisymmetric. Indeed, if $x R y$ and $y R x$, then $x R x$ due to transitivity. As $R$ is irreflexive, this leads us to a contradiction, which yields $x = y$. We have proved even more: every strict ordering is \emph{asymmetric}, that is, if $x R y$, then $y R x$ does not hold.
\end{rem}

A relation $R$ on a set $A$ is called a \emph{non-strict (partial) order(ing) on} $A$, if $R$ is reflexive, transitive, and antisymmetric.

\begin{exm}
	On any set $A$, the relation $\id_A$ is a non-strict order. The relations ${\leq}$ and ${\geq}$ on $\N$ are non-strict orders as well, while the irreflexive relation ${<}$ is not. The relation ${\sbs}$ on a set $\mP(A)$ and the divisibility relation ${\dvd}$ on $\N$ are non-strict orderings of the respective sets. Notice that the divisibility relation is \emph{not}  an ordering of $\Z$ for it is not antisymmetric: $1 \dvd -1$ and $-1 \dvd 1$ but $1 \neq -1$.
\end{exm}

\begin{exc}
	In the context of Example~\ref{ch0:induced0}, put $x Q y \iff f(x) \leq f(y)$ for all $x, y \in A$. Is it necessary for $Q$ to be a non-strict partial order on $A$?
\end{exc}

We see that for every $n,m \in \N$ it holds either $n \leq m$ or $m \leq n$, yet this is not the case for the ordering ${\dvd}$: indeed, $2 \ndvd 3$ and $3 \ndvd 2$. The elements $2$ and $3$ are thus called \emph{incomparable} in the sense of ${\dvd}$. It is this possibility which the expression `\emph{partial} order' implies.

\begin{exc}
	If $P$ and $Q$ are both strict (or non-strict) orders on $A$, then the relations $P^{-1}$ and $P \cap Q$ are such orderings as well.
\end{exc}

If $R$ is an ordering on $A$ (either strict or not), the pair $(A,R)$ is called a \emph{partially ordered set} (or \emph{poset}). If the relation $R$ is clear from the context, the set $A$ can be called a poset itself. Anyway, $A$ is known as the \emph{ground set} of the poset $(A, R)$.

\paragraph{Strict and non-strict orders.}
Strict and non-strict orderings of $A$ are closely interrelated. Namely, each strict order $P$ has a natural non-strict `counterpart' $\phi(P)$, whereas every non-strict $Q$ has a strict `counterpart' $\psi(Q)$.

We put $S(A) = \{ R \in \mP(A^2) \mid R\ \mbox{is a strict order} \}$
and similarly define the set $N(A)$ of all non-strict orders on $A$. Consider the functions $\phi\colon S(A) \to \mP(A^2)$ and $\psi\colon N(A) \to \mP(A^2)$ such that
$$\phi(P) = P \cup \id_A\quad \mbox{and}\quad \psi(Q) = Q \setminus \id_A$$
for each $P \in S(A)$ and $Q \in N(A)$. In other words, we let
$$
(x,y) \in \phi(P) \iff xPy \vee x = y\qquad \mbox{and}\qquad
(x,y) \in \psi(Q) \iff xQy \wedge x \neq y.
$$

\begin{thm} For every $P \in S(A)$ and $Q \in N(A)$, it holds that:
	\begin{enumerate}
		\item $\phi(P) \in N(A)$ and $\psi(\phi(P)) = P$;
		\item $\psi(Q) \in S(A)$ and $\phi(\psi(Q)) = Q$.
	\end{enumerate}
\end{thm}
\noindent The proof is straightforward but tedious a little.

As one can readily see, the functions $\psi\colon N(A) \to S(A)$ and $\phi\colon S(A) \to N(A)$ are inverse to each other. This implies the following
\begin{corr}
	The function $\phi\colon S(A) \to N(A)$ is bijective and $\psi = \phi^{-1}$.
\end{corr}
\begin{exc}
	Prove that $\phi(P^{-1}) = (\phi(P))^{-1}$ for every $P \in S(A)$.
\end{exc}

We see that strict and non-strict orderings, despite being different things, enjoy a natural bijection between them (that is, each set $A$ has `as many' strict orderings as it has non-strict ones). We are already familiar with the pair of ordering $({<},{\leq})$ on $\N$ or the pair $({\subsetneq},{\sbs})$ on an arbitrary set $\mP(A)$.

When considering a strict order, this allows us to have the respective non-strict order at our disposal, and vice versa. Particularly, we can freely mention a \emph{partial ordering} without specifying which version thereof we are speaking about. In our Course, all orderings are strict by default.

When denoting a strict order, they usually employ the symbol ${<}$ or the like. We shall suppose that in the pairs $({<},{\leq})$, $({\prec}, {\preceq})$, etc., the first symbol stands for the strict version of an ordering, while the second one does for the non-strict one, i.\,e., ${\leq} = \phi({<})$ and ${<} = \psi({\leq})$. Sometimes, they use symbols like ${\subsetneq}$ and ${\lneq}$ to denote a strict ordering, if things are to be clarified further.

It is likewise natural to identify our poset as a pair $(A, <)$ or $(A, \leq)$ with the triplet $(A, <, \leq)$.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{hasse.pdf}
	\caption{\label{fig:cube_3}(a) This diagram does \emph{not} depict a strict order for it lacks transitivity. (b) Nevertheless, one can easily restore the arrows required by transitivity, which are shown in color. In practice, they routinely use (a)-style diagrams to represent orders for the sake of clarity (so called \emph{Hasse diagrams}). The `colored' arrows are then implied (along with loops at each point in the case of a non-strict order).}
\end{figure}

\paragraph{Maxima and minima.} For any poset $(A, {<}, {\leq})$, an element $x \in A$ is called ($<$-)\emph{maximal} if
$$\forall y \in A\; \neg x < y.$$
In `arrow' terms, a maximal element is such that no arrow starts from it. We similarly define a (${<}$)-\emph{minimal} element $x$, so that
$$\forall y \in A\; \neg y < x.$$

The set of  ${<}$-maximal (which are exactly the same as ${\leq}$-maximal) elements of $A$ (also called \emph{maxima} (plural for \emph{maximum}) thereof) is denoted by $\max_{<} A$ or just by $\max A$.

\begin{exm}
	As expected, one gets $\min_{<} \N = \{0\}$ and $\max_{<} \N = \void$ but also $\max_{>} \N = \{0\}$ and $\min_{>} \N = \void$. Exactly so! Indeed, there exists no $y \in \N$ such that $0 > y$.
\end{exm}

\begin{exc}
	Let $R$ be an ordering on a set $A$. Prove that $\min_{R} A = \max_{R^{-1}} A$ and $\max_{R} A = \min_{R^{-1}} A$.
\end{exc}

\begin{exm}
	Consider a poset $(A, \void)$. One has $\min_{\void} A = A = \max_{\void} A$. Indeed, each element of $A$ is neither greater, nor lesser than any other. So, an ordering may have any number of minima or maxima.
\end{exm}

\begin{exm}
	Consider the set $A = \mP(\N) \setminus \{\void, \N\}$ as ordered by ${\sbs}$. What are the sets $\min A$ and $\max A$?
	
	It is easy to see that $x \subsetneq \{n\}$ is possible for no $x \in A$ nor $n \in \N$. Hence, each singleton $\{n\}$ is a minimum in $A$. On the contrary, if a set $y \in A$ has at least two distinct elements $n$ and $m$, then $\{m\} \subsetneq y$ and $y$ is thus not minimal. So, $\min A = \{ \{ n \} \mid n \in \N\}$. Similarly, $\max A = \{ \N \setminus \{ n \} \mid n \in \N\}$.
\end{exm} 

\begin{exc}
	What are the sets $\min_{\dvd} \N$ and $\max_{\dvd} \N$? And what if one restricts this ordering to the set $\N \setminus \{0,1\}$?
\end{exc}

Let $(A, <)$ be a poset. It is natural to make the notions of maximum and minimum relative to a set $B \sbs A$ by letting $\max_{<} B = \{ x \in B \mid \forall y \in B\, x \nless y \}$ and similarly for $\min_{<} B$.
%
%\begin{rem}
%С формальной точки зрения, можно также считать, что отображение $\id_B\colon B \to A$ индуцирует порядок $<_{B}$ на множестве $B$ (при этом ${<_{B}}$ есть ограничение ${<}$ на $B$), а затем рассмотреть максимальные элементы ч.\,у.\,м. $(B, <_B)$.
%\end{rem}

An element $x \in B$ is called a \emph{greatest element} of the poset's $(A, <)$ subset $B$ if $\forall y \in B\; y \leq x$; we likewise call $x \in B$ a \emph{least element} of $B$ if $\forall y \in B\; x \leq y$.

\begin{lemma}
	Let $(A, <)$ be a poset. If $x$ is a greatest element in $B \sbs A$, then $\max_{<} B = \{x\}$. Consequently, a greatest element of $B$ is unique, so it is indeed \emph{the} greatest element.
\end{lemma}
\begin{proof}
	Assume that $x \notin \max B$, so $x < y$ for some $y \in B$. On the other hand, we get $y \leq x$, which yields either $y = x$ or $y < x$ (since ${\leq} = \phi({<})$). In the former case, it is immediate that $y < y$; in the latter case, the same holds by transitivity. This contradicts the fact ${<}$ is irreflexive. Thus, $x \in \max B$.
	
	Now, suppose that $x' \in \max B$. Then $x' \nless x$ but $x' \leq x$. Hence, $x' = x$.
\end{proof}

%\begin{exc}
%Сформулируйте и докажите аналогичное утверждение для наименьших элементов.
%\end{exc}
%
\begin{exc}
	Suppose that $\max_{<} A = \{x\}$. Is $x$ always a greatest element of $(A, <)$?
\end{exc}

\begin{exm}
	Consider the set $A = \mP(\N) \setminus \{\void, \N\}$ as ordered by ${\sbs}$. As we know, this set has multiple maxima and minima, hence it has neither greatest nor least element. Now consider $B = \{ X \in A \mid \{1,2,3\} \sbs X \}$. Clearly, the element $\{1,2,3\}$ is the least one in $B$. On the other hand, it is easy to see that
	$$\max B = B \cap \max A = \{ \N \setminus \{ n \} \mid n \in \N \setminus \{1,2,3\} \}.$$
	In particular, $B$ does not have a greatest element.
\end{exm}

Let $(A, <)$ be a poset and $B \sbs A$. An element $x \in A$ is called an \emph{upper bound} of the set $B$, if $y \leq x$ for every $y \in B$. A \emph{lower bound} of $B$ is defined similarly.

\begin{exm}
	The upper bounds of the set $\{2,3,7\}$ in the poset $(\N, {\dvd})$ are all the natural multiples of $42 = \lcm(2,3,7)$. The only lower bound of that set is $1 = \gcd(2,3,7)$.
\end{exm}
\begin{exc}
	Let $(A, <)$ be a poset and $B,C \sbs A$. Let $B^\vartriangle$ be the set of upper bounds of $B$ and $B^\triangledown$ be the set of lower bounds of $B$. Prove that:
	\begin{enumerate}
		\item $(B \cup C)^\vartriangle = B^\vartriangle \cap C^\vartriangle$; $(B \cup C)^\triangledown = B^\triangledown \cap C^\triangledown$;
		\item if $B \sbs C$, then $C^\vartriangle \sbs B^\vartriangle$ and $C^\triangledown \sbs B^\triangledown$;
		\item $B \sbs B^{\vartriangle \triangledown} \cap B^{\triangledown \vartriangle}$;
		\item $B^\vartriangle = B^{\vartriangle \triangledown \vartriangle}$; $B^\triangledown = B^{\triangledown \vartriangle \triangledown}$.
	\end{enumerate}
\end{exc}

An element $x \in A$ is a \emph{supremum} of the set $B$ if $x$ is the least upper bound of $B$ (i.e., the least element of $B^\vartriangle$). Similarly, $x$ is an $\emph{infimum}$ of $B$ if it is the greatest lower bound thereof. As a least element (as well as a greatest) must be unique, the following notation does make sense:  $x = \inf B$ ($x = \sup B$, respectively), provided the infimum (supremum) exists.

\begin{rem}
	A set $B$ has a greatest element iff $\sup B \in B$, and $\sup B$ is that very element. Things are similar for $\inf B$.
\end{rem}
\begin{exc}
	How does $\sup B$ relate to $\inf B^\vartriangle$ (if both exist)?
\end{exc}

\begin{exm}
	For the natural ordering of real numbers and the set $B = \{ \frac{1}{n} \mid n \in \N_+ \}$, one has $\sup B = 1 \in B$ and $\inf B = 0 \notin B$.
\end{exm}

\begin{exm}
	Consider the order $P = \{(0,2), (0,3), (1,2), (1,3)\}$ on the set $A = \{0,1,2,3\}$. Letting $B = \{2,3\}$, we obtain $B^\triangledown = \{ 0, 1\}$; yet these lower bounds are incomparable, hence $B$ has no \emph{greatest} lower bound, although each lower bound is a \emph{maximal} one.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{min_max.pdf}
	\caption{The order $(A, <)$ shown above has two minimal elements $0, 1$ and one maximum $4$, which is the greatest element. The order has no least element. For the set $B = \{2,3\}$, we have $B^\triangledown = \{ 0, 1\}$ and $B^\vartriangle = \{ 4\}$.}
\end{figure}

\begin{exm}
	%Если $B \in X$, то $B \sbs \cup X \in \mP(A)$, поэтому $\cup X$ есть верхняя грань множества $X$. Пусть $C \in \mathcal \mP(A)$ "--- какая-нибудь верхняя грань множества $X$. По определению, тогда $B \sbs C$ для всех $B \in X$. Ясно, что в этом случае $\cup X \sbs C$, а значит, верхняя грань $\cup X$ наименьшая. Итак, $\sup X = \cup X$.
	%
	%Если $B \in X$, то $\cap X \sbs B$ (напомним, по нашим определениям $\cap \void \hm= \void$); следовательно, $\cap X$ есть нижняя грань $X$. Пусть $D \in \mP(A)$ есть некоторая нижняя грань $X$, т.\,е. $D \sbs B$ для всех $B \in X$. Если $X \neq \void$, то найдется $B_0 \in X$, для которого $D \sbs B_0$, а значит, $D \sbs \cup X$. Тогда, по определению пересечения, $D \sbs \cap X$ и, окончательно, $\inf X = \cap X$. Если же $X = \void$, любое $D \in \mP(A)$ является нижней гранью $X$. Наибольшим элементом $\mP(A)$ является $A$, а значит, $\inf \void = A$.
	
	Let us consider the poset $(\mP(A), {\sbs})$. It is easy to see that for $X = \{B_1, B_2\}$, one gets $\sup X = B_1 \cup B_2$ and $\inf X = B_1  \cap B_2$.
	Indeed, it is clear that $B_i \sbs B_1 \cup B_2$ for either $i$. On the other hand, if $B_1 \sbs C$ and $B_2 \sbs C$, then one obtains $B_1 \cup B_2 \sbs C$ by Example~\ref{L2:cup_sup}. The argument for $\inf X$ is similar.
	
	The union of sets $B_1$ and $B_2$ is thus their \emph{least} common \emph{super}set (i.\,e., ${\sbs}$-upper bound), and their intersection is their \emph{greatest} common \emph{sub}set (${\sbs}$-lower bound).
\end{exm}

\begin{exc}
	In the setting of the above example, prove that $\sup X = \cup X$ for any $X \sbs \mP(A)$.
\end{exc}

An important class of posets consists of so-called \emph{lattices}, i.\,e., of such posets $(A,{<})$ where for every $x, y \in A$, there exist both $\sup \{x, y\}$ and $\inf \{x, y\}$. For example, $(\mP(A), {\sbs})$ is a lattice.

\begin{exc}
	Prove that the poset $(\N,{\dvd})$ is a lattice.
\end{exc}
%
%\begin{exc}
%Докажите, что если в ч.\,у.\,м. $(A, {<})$ для каждого $X \sbs A$ существует $\sup X$, то для каждого $X \sbs A$ существует $\inf X$, и наоборот.
%\end{exc}
%

\paragraph{Linear orders.} An order ${<}$ on a set $A$ is called \emph{linear} (or \emph{total}\footnote{Clearly, a total order in this sense need \emph{not} be a total binary relation, whereas that the latter would mean $\forall x \in A\, \exists y\, x < y$. For this reason, we prefer `linear' to `total' in this context.}) if every two elements of $A$ are comparable to each other, i.\,e.,
$$\forall x, y \in A\ \ x \leq y \vee y \leq x.$$
The poset $(A,<)$ is said to be a \emph{linearly ordered set} if the order ${<}$ is linear.

\begin{exm}
	The natural orderings on $\N$, $\Z$, $\Q$, and $\R$ are linear, unlike ${\sbs}$ on $\mP (A)$ (if the set $A$ has at least two distinct elements) or ${\dvd}$ on $\N$.
\end{exm}

\begin{exc}
	Each linear order is a lattice.
\end{exc}

\begin{rem}
	If an order ${<}$ on a set $A$ is linear, then
	$$x \nless y \iff y \leq x$$
	for every $x, y \in A$. This implies that an element $x$ is greatest (least) in the set $B \sbs A$ iff $x$ is maximal (minimal) in $B$. In particular, a linearly ordered set can have no more than one maximum (minimum). It is thus justified to write $x = \max B$ when $x$ is maximal in some $B \sbs A$.
	
	\mcomm{It is important to show the students that `our' supremum is just a general version of what they may have come across in their Calculus course.}
	
	Furthermore, for a linearly ordered set, suprema (and infima) can be defined as minimal upper (maximal lower) bounds:
	\begin{multline*}
		x = \sup B \iff  x \in B^\vartriangle \wedge \forall z \in B^\vartriangle\ z \nless x \iff\\
		x \in B^\vartriangle \wedge \forall z < x\ z \notin B^\vartriangle \iff
		\forall y\in B\ y \leq x \wedge \forall z < x\, \exists y \in B\ z < y.
	\end{multline*}
\end{rem}
\noindent This form of the definition is routinely used in Calculus courses for subsets of $\R$.

%\begin{exc}
%Пусть $(B, <)$ л.\,у.\,м. и функция $f\colon A \to B$ индуцирует частичный порядок $P$ на $A$. Докажите, что отношение $\bar P$ транзитивно.
%\end{exc}

Let $(A,<)$ be a poset. A set $C \sbs A$ is called a \emph{chain} in $A$ if
$$\forall x, y \in C\;\; x \leq y \vee y \leq x.$$
In other words, a chain is a subset whose every two elements are comparable or, equivalently, such that the order becomes linear when restricted to it. On the contrary, the set $D \sbs A$ is called an \emph{antichain}, if no two of its (distinct) elements are comparable, i.\,e.,
$$\forall x, y \in D\;\; x \nless y \wedge y \nless x.$$
\begin{exm}
	In the poset $(\N, {\dvd})$, the set $\{2^n \mid n \in \N \}$ is a chain, while any set of prime numbers is an antichain. The set $\{ \N_{\geq k} \mid k \in \N \}$, where $\N_{\geq k} = \{ n \in \N\mid n \geq k \}$, is a chain in $(\mP(\N), {\sbs})$.
\end{exm}
\begin{exc}
	When is a poset's subset both a chain and an antichain? Find all chains and all antichains in a linearly ordered set.
\end{exc}
\begin{exc}
	In the poset $(\mP(\N), \sbs)$, find a non-empty chain that has no greatest element, nor least one.
\end{exc}

\paragraph{Order isomorphism.} Sometimes, two posets $\mathcal A = (A, <_A)$ and $\mathcal B = (B, <_B)$ are very similar to each other. For example, the natural numerical ordering $<$ aligns the sets $\{1,2,3\}$ and $\{2, 8, 15\}$ ``in the same way'': as $1 < 2 < 3$ and $2 < 8 < 15$, respectively, while this is not the case with infinite orders $(\N, <)$ and $(\Z, <)$, as the former is the only one that has a minimum: $0 < 1 < 2 < \ldots$, yet $\ldots < -2 < -1 < 0 < 1 < 2 < \ldots$.

In mathematics, one is usually interested in but the `form' of an ordering, not in its very elements, whose nature may be irrelevant given their order. Say, the orderings $1 < 2 < 3$ and $\mbox{Moon} < \mbox{Earth} < \mbox{Sun}$ are `essentially' the same. So, a `pure' order gets abstracted from a particular poset.

This is a very general situation. The notion of \emph{isomorphism} (literally, ``equality in form'') is employed to treat it formally. Let us restrict ourselves to the case of posets. Two posets $\mathcal A = (A, <_A)$ and $\mathcal B = (B, <_B)$ are called \emph{isomorphic} if there exists a bijection $\alpha\colon A \to B$ such that $x <_A y$ iff $\alpha(x) <_B \alpha(y)$ for every $x, y \in A$ (so, this $\alpha$ `respects' the order). Such a mapping $\alpha$ is called an \emph{isomorphism} from $\mathcal A$ to $\mathcal B$. We write $\mathcal A \stackrel{\alpha}{\cong} \mathcal B$ or, less explicitly, $\mathcal A \cong \mathcal B$ in this case.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{ord_iso.pdf}
	\caption{An order isomorphism: $\mathcal A \stackrel{\alpha}{\cong} \mathcal B$.}
\end{figure}

\begin{rem}
	In general, it is natural to define \emph{any} two pairs $(A, P)$ and $(B, Q)$, where $P \sbs A^2$ and $Q \sbs B^2$ (such pairs---not necessarily posets---are called \emph{structures}) to be \emph{isomorphic} if there exists a bijection $\alpha\colon A \to B$ such that $x P y$ iff $\alpha(x) Q \alpha(y)$ for every $x, y \in A$. A structure on the set $A$ may contain multiple relations $P_i \in A^{n_i}$ (not necessarily binary) and functions $f_i \colon A^{m_i} \to A$. It makes sense to think that diverse structures are \emph{the} object of mathematics.
\end{rem}
\begin{exc}
	Prove that if two structures $(A, P)$ and $(B, Q)$ are isomorphic and $(A, P)$ is a poset, then $(B, Q)$ is a poset as well.
\end{exc}

\begin{lemma}
	For every posets  $\mathcal A$, $\mathcal B$, and $\mathcal C$,
	\begin{enumerate}
		\item $\mathcal A \stackrel{\id_A}{\cong}  \mathcal A$;
		\item if $\mathcal A \stackrel{\alpha}{\cong}  \mathcal B$, then $\mathcal B \stackrel{\alpha^{-1}}{\cong}  \mathcal A$;
		\item if $\mathcal A \stackrel{\alpha}{\cong}  \mathcal B$ and $\mathcal B \stackrel{\beta}{\cong}  \mathcal C$, then $\mathcal A \stackrel{\beta \circ \alpha}{\cong} \mathcal C$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let us check the second statement. Clearly, $\alpha^{-1}$ is a bijection from $B$ to $A$. Assume that $u <_B v$ for arbitrary $u, v \in B$. As $\alpha$ is surjective, we have $u = \alpha(x)$ and $v = \alpha(y)$ for some $x, y \in A$. But $\alpha(x) <_B \alpha(y)$ implies $x <_A y$ since $\alpha$ is an isomorphism. On the other hand, $\alpha^{-1}(u) = \alpha^{-1}(\alpha(x)) = x$ and $\alpha^{-1}(v) = \alpha^{-1}(\alpha(y)) = y$, whence $\alpha^{-1}(u) <_A \alpha^{-1}(u)$ as required.
\end{proof}

\begin{exm}
	One has $(\Z, <) \cong (\Z, >)$. Indeed, $x < y$ is equivalent to $-x > -y$ and the mapping $x \mapsto -x$ is a bijection $\Z \to \Z$. Hence, this mapping is an isomorphism required. On the other hand, $(\N, <) \ncong (\Z, <)$\;\footnote{Clearly, we abuse the notation here since the symbol ${<}$ in the left-hand side stands for not the same order as it does in the right-hand side. Otherwise, we could use $<_{\N}$ and $<_{\Z}$, respectively, where ${<_{\N}} = {<_{\Z}} \cap \N^2$.}. Otherwise, there is an isomorphism $\alpha\colon \N \to \Z$. There exist elements $u \in \Z$ with $u < \alpha(0) \in \Z$ and $x \in \N$ such that $\alpha(x) = u$. From $\alpha(x) < \alpha(0)$, it follows that $x < 0$, which is impossible for any natural $x$. A contradiction.
\end{exm}
\begin{exc}
	Prove that $(\Q, <) \ncong (\Z, <)$ and $(\Q, <) \ncong (\R, <)$.
\end{exc}

\begin{rem}
	It is straightforward to check that
	$$(A, <_A) \stackrel{\alpha}{\cong} (B, <_B) \iff (A, \leq_A) \stackrel{\alpha}{\cong} (B, \leq_B).$$
	Each isomorphism thus `respects' the relation between strict and non-strict versions of an order.
\end{rem}

Two isomorphic posets are \emph{essentially} the same, i.\,e. they satisfy the same \emph{order} properties. We have already seen that two isomorphic posets both contain a least element or both do not, while isomorphisms map those least elements to each other. This is also applicable to \emph{every} property that is defined in order terms (the exact form of this statement is beyond the scope of our Course). Let us but take at look at some examples.
\begin{lemma}
	Suppose that $(A, <_A) \stackrel{\alpha}{\cong} (B, <_B)$. If $(A, <_A)$ is linearly ordered, then $(B, <_B)$ is linear as well. For every $X \sbs A$, it holds that $\max_{<_B} \alpha[X] = \alpha[ \max_{<_A} X ]$; furthermore, $\sup_{<_B} \alpha[X] = \alpha(\sup_{<_A} X)$ if $\sup_{<_A} X$ exists. Similar statements are true for minima and infima.
\end{lemma}
\begin{proof}
	We check just the claim about suprema. Let $\sup_{<_A} X$ exist. If $u \in \alpha[X]$, then $u = \alpha(x)$ for some $x \in X$. From $x \leq_A \sup_{<_A} X$, it follows that $u \leq_B \alpha(\sup_{<_A} X)$. On the other hand, let $v = \alpha(y)$ be an arbitrary upper bound of the set $\alpha[X]$. Then for every $x \in X$, we have $\alpha(x) \leq_B \alpha(y)$, whence $x \leq_A y$. Thus, $\sup_{<_A} X \leq_A y$ and $\alpha(\sup_{<_A} X) \leq_B v$.
\end{proof}

\begin{exc}
	Prove that for every poset $(A, \leq)$ there exists a set $S \sbs \mP(A)$ such that $(A, \leq) \cong (S, {\sbs})$. In other words, each ordering looks like the inclusion order on a suitable subset family.
\end{exc}

\section{Equivalences and Partitions}

A relation $R \sbs A^2$ is called an \emph{equivalence relation} (or just, an \emph{equivalence}) on the set $A$ if $R$ is reflexive, symmetric, and transitive.

\begin{exm}
	The relations $A^2$ and $\id_A$ are equivalences. Moreover, $\id_A \sbs E \sbs A^2$ for every equivalence $E$ on $A$.
	
	The relation $\equiv_m = \{ (x,y) \in \Z \mid  x \equiv y \pmod m \}$ is an equivalence. The relation ``$x$ is parallel to $y$'' is an equivalence on the set of lines in the plane if one defines each line to be parallel to itself. The relation ${\sim}$ of set equivalence is an equivalence indeed for any set (of sets).
\end{exm}

\begin{exm}
	Let $f\colon A \to B$. Then the relation $$\ker f = \{(x,y) \in A^2 \mid f(x) = f(y)\},$$ which is called the \emph{equivalence kernel} (or just the \emph{kernel}) of the function $f$, is indeed an equivalence on $A$. Clearly, $\ker f = \id_A$ iff $f$ is injective.
\end{exm}

%\begin{rem}
%Понятие эквивалентности обобщает понятие равенства (которое на каждом множестве $A$ совпадает с отношением $\id_A$). При этом некоторые свойства равенства эквивалентность может утратить. Например, если $R$ является эквивалентностью на множестве $A$ и $f\colon A^n \to A$, то из $x_i R y_i$ для $1 \lq i \lq n$, вообще говоря, не следует, что  $f(x_1,\ldots,x_n) R f(y_1,\ldots,y_n)$.
%
%Впрочем, многие важные эквивалентности таким свойством обладают. Они называются \emph{конгруэнциями}\index{Конгруэнция} (относительно $f$). Например, отношение $\equiv_m$ сравнимости по модулю $m$ является конгруэнцией относительно сложения и умножения  целых чисел: если $x \equiv_m x'$ и $y \equiv_m y'$, то $x + y \equiv_m x' + y'$ и $x  y \equiv_m x'  y'$ (проверьте!).
%\end{rem}
%
\begin{exm}
	Let us show that a relation $R \sbs A^2$ is an equivalence iff $(R \circ R^{-1}) \cup \id_A = R$.
	
	Suppose $R$ is an equivalence. Then $R = \id_A \circ R \sbs R \circ R \sbs R$, whence $R = R \circ R = R \circ R^{-1}$. So, $R = R \cup \id_A = (R \circ R^{-1}) \cup \id_A$.
	
	For the opposite direction, assume our equation. Then, clearly, $\id_A \sbs R$. One also gets $R^{-1} = (R \circ R^{-1})^{-1} \cup \id^{-1}_A = ((R^{-1})^{-1} \circ R^{-1}) \cup \id_A = R$. And finally, $R \circ R = R \circ R^{-1} \sbs R$.
\end{exm}

\paragraph{Quotient set.} Intuitively, given an equivalence on $A$, one can naturally `identify' equivalent elements thereof neglecting their `insignificant' differences. This procedure results in a new set of `classes' of elements from $A$. If one, say, identifies every two integers of the same parity (thus employing the equivalence ${\equiv_2}$), he gets exactly two classes: those of even and of odd numbers. If one identifies every two objects of the same color, he obtains a set of classes, which can rightly be called  `colors'. Such constructions are widespread in mathematics.

So, let $E$ be an equivalence on a set $A$ and $x \in A$. We call the set
$$[x]_E = \{ z \in A \mid x E z \}$$
the \emph{equivalence class} of $x$ w.\.r.\,t.\ (with respect to) $E$. Every element $y \in [x]_E$ is called a \emph{representative} of the class $[x]_E$. The set
$$A/E = \{ \sigma \in \mP(A) \mid \exists x \in A\; [x]_E = \sigma \}  =  \{[x]_E \mid x \in A\}$$
is called the \emph{quotient set} of the set $A$ by $E$.
\begin{rem}
	As a matter of fact, equivalence classes are just set images under $E$: namely, $[x]_E = E[\{x\}]$. This observation justifies the widespread notation $xE$ for $[x]_E$.
\end{rem}

\begin{exm}
	The set $A / A^2$ is just $\{A\}$ as all elements of $A$ are pairwise $A^2$-equivalent and go to the same class. The set $A / \id_A$ is the set of the singletons for all elements of $A$. Therefore $A / \id_A \sim A$.
	
	As we know, $x \equiv_m y$ iff the numbers $x$ and $y$ leave identical remainders after dividing by $m$. Hence, the class $[x]_{\equiv_m}$ consists of all integers with the same remainder as $x$ has. After dividing by $m$, the possible remainders are $0,1,\ldots,m - 1$. Then $\Z / {\equiv_m} \sim \{0,1,\ldots,m - 1\}$.
	
	Observe by the way that $\equiv_m$ equals $\ker r_m$, where  $r_m \colon x \mapsto \mbox{the remainder after dividing}\ x\ \mbox{by}\ m$. An exercise below shows this is a general situation.
	
	%Если мы проведем на плоскости $\Pi$ какую-либо прямую $l$ и рассмотрим отношение "<точки $x$ и $y$ лежат по одну сторону $l$ или обе на ней">, такое отношение $L$ будет, очевидно, эквивалентностью. Множество $\Pi / L$ состоит из трех элементов: двух полуплоскостей, на которые $l$ рассекает $\Pi$, и самой прямой $l$.
\end{exm}

%\begin{exm}
%Пусть $f\colon A \to B$. Что есть $A/\ker f$? Очевидно, $x \ker f \hm= \{z \in A \mid f(z) = f(x) \} = f^{-1}[\{ f(x) \}]$. Поэтому $\sigma \in A / \ker f$ тогда и только тогда, когда $\sigma = f^{-1}[\{y\}]$ для некоторого $y \in \rng f = f[A]$. Таким образом, $A / \ker f$ есть множество "<полных прообразов"> всевозможных значений функции $f$. Например, пусть $A = B = \R$ и $f = \cos$. Тогда
%$$\R / \ker \cos = \{ \{\pm\arccos \alpha + 2\pi k \mid k \in \Z  \} \mid \alpha \in [0,1] \}.$$
%\end{exm}

\begin{lemma}\label{ch0:eq_class}
	Let $E$ be an equivalence on $A$. Then for every $x,y\in A$ the following hold:
	\begin{enumerate}
		\item $x \in [x]_E$;
		\item $[x]_E \cap [y]_E \neq \void \iff xEy \iff [x]_E = [y]_E$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The first statement follows from $xEx$. For the second one, let us assume $z \in [x]_E \cap [y]_E$. Then $x E z$ and $y E z$, whence $z E y$, and $x E y$ by symmetry and transitivity. Let, in turn, $x E y$  and $z \in [x]_E$, i.\,e., $x E z$. Likewise we get $y E z$. So, $[x]_E \sbs [y]_E$. The converse inclusion is similar. Finally, suppose that $[x]_E = [y]_E$. By the first statement, obtain $x \in [x]_E \cap [y]_E \neq \void$.
\end{proof}

\begin{exc}
	For every set $A$ and equivalence $E \sbs A^2$, there exist a set $B$ and a surjection $f\colon A \to B$ such that $E = \ker f$.
\end{exc}

%\begin{exc}
%Пусть отношение $R \sbs A^2$ рефлексивно и транзитивно.  При всех $x, y \in A$ положим $x E y \iff x R y\ \mbox{и}\ y R x$. Проверьте, что $E$ есть эквивалентность на $A$. При всех $\sigma, \tau \in A / E$ положим $\sigma \lq \tau \iff \exists x \in \sigma\, \exists y \in \tau\: x R y$. Докажите, что ${\lq}$ есть нестрогий частичный порядок на множестве $A / E$.
%\end{exc}
%\begin{rem}
%Рефлексивные транзитивные отношения называются \emph{предпорядками}\index{Предпорядок}. Например, таково отношение ${\lesssim}$ на любом множестве $A$. Как мы знаем, $X \sim Y \iff X \lesssim Y\ \mbox{и}\ Y \lesssim X$. Поэтому на фактор-множестве $A / {\sim}$, согласно предыдущему упражнению, ${\lesssim}$ определяет весьма естественный порядок, называемый \emph{сравнением по мощности}\index{Мощность!сравнение}. В последующих частях курса мы покажем, опираясь на аксиому выбора, что такой порядок всегда линейный.
%\end{rem}

\paragraph{Partitions.}
As one can see, equivalence classes `cover' the whole set $A$, and every two distinct classes are disjoint. This way, each equivalence $E$ `partitions' the set $A$ into pairwise disjoint `pieces'. More formally, we call a set $\Sigma \sbs \mP(A)$ a \emph{partition} of the set $A$ if
$$\void \notin \Sigma, \quad \cup \Sigma = A\quad\mbox{and}\quad \forall \sigma, \tau \in \Sigma\; (\sigma \cap \tau \neq \void \ply \sigma = \tau).$$
\begin{exm}
	Lemma~\ref{ch0:eq_class} shows that each quotient set $A / E$ is a partition of $A$. Let $\R_{-}$ be the set of all negative reals. Then $\{\R_{-}, \{0\}, \R_{+}\}$ is a partition of $\R$. The set of all circles centered at $0$ of every possible radius $r \geq 0$ (the circle of radius $0$ equals its single center point) is a partition of the plane.
\end{exm}
\begin{exc}\label{L13:ex_part_empty}
	Describe all the partitions of the set $\void$.
\end{exc}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{part.pdf}
	\caption{A partition $\Sigma = \{\sigma_1, \sigma_2, \ldots, \sigma_9 \}$ of a set $A$. This $\Sigma$ is finite as it has just nine elements.}
\end{figure}

It turns out that not only each quotient set is a partition, but each partition is the quotient by a suitable equivalence, so that there exists quite a natural bijection between objects of these two kinds.

So, let $Eq(A)$ be the set of all equivalences on a set $A$, and $\Pi(A)$ be the set of all partitions of $A$. Consider the functions $\pi\colon Eq(A) \to \mP (\mP(A))$ and $\eps\colon \Pi(A) \to \mP(A^2)$ such that
$$\pi(E) = A / E \quad\mbox{and}\quad \eps(\Sigma) = \{(x,y) \in A^2 \mid \exists \sigma \in \Sigma\: (x \in \sigma \wedge y \in \sigma)\}$$
for every $E \in Eq(A)$ and every $\Sigma \in \Pi(A)$. In other words, the relation $\eps(\Sigma)$ is composed of all such pairs whose both coordinates belong to the same element (`piece') of the partition $\Sigma$.
\begin{thm}
	For every $E \in Eq(A)$ and $\Sigma \in \Pi(A)$ the following hold:
	\begin{enumerate}
		\item $\pi(E) \in \Pi(A)$ and $\eps(\pi(E)) = E$;
		\item $\eps(\Sigma) \in Eq(A)$ and $\pi(\eps(\Sigma)) = \Sigma$.
	\end{enumerate}
\end{thm}
%\begin{proof}
%Как мы уже заметили, $\pi(E)$ есть разбиение $A$ в силу леммы~\ref{ch0:eq_class}. Пусть $(x,y) \in \eps(A/E)$. Тогда существует $\sigma \in A / E$, т.\,ч. $x,y \in \sigma$, т.\,е. $x, y \in [z]_E$ для некоторого $z \in A$. Значит, $z E x$ и $z E y$, откуда $(x,y) \in E$. Обратно, пусть $x E y$. Тогда, согласно лемме~\ref{ch0:eq_class}, $y \in [y]_E = [x]_E$ и $x,y \in [x]_E \in A/E$. Следовательно, $(x,y) \in \eps(A/E)$. Итак, $\eps(\pi(E)) = E$.
%
%Проверим теперь, что $\eps(\Sigma)$ есть эквивалентность на $A$. Поскольку $\cup \Sigma = A$, для каждого $x \in A$ найдется $\sigma \in \Sigma$, т.\,ч. $x \in \sigma$; значит, $(x,x) \in \eps(\Sigma)$. Симметричность $\eps(\Sigma)$ очевидна. Допустим теперь, что $(x,y), (y,z) \in \eps(\Sigma)$. Тогда для некоторых $\sigma, \tau \in \Sigma$ имеем $x \in \sigma$, $y \in \sigma$, $y \in \tau$ и $z \in \tau$. Из $\sigma \cap \tau \neq \void$ получаем $\sigma = \tau$, откуда $x,z \in \sigma$ и $(x,z) \in \eps(\Sigma)$.
%
%Остается проверить, что $\pi(\eps(\Sigma)) = \Sigma$. Докажем сначала, что для всех $\sigma \in \Sigma$ и всех $x \in \sigma$ верно $\sigma = [x]_{\eps(\Sigma)}$. В самом деле, если $y \in \sigma$, получаем $x, y \in \sigma \in \Sigma$, откуда $(x,y) \in \eps(\Sigma)$, т.\,е. $y \in [x]_{\eps(\Sigma)}$. Имеем $\sigma \sbs [x]_{\eps(\Sigma)}$. Обратно, пусть $y \in [x]_{\eps(\Sigma)}$. Тогда $x, y \in \sigma'$ для некоторого $\sigma' \in \Sigma$. Из $x \in \sigma \cap \sigma' \neq \void$ следует, что $\sigma' = \sigma$, а значит, $y \in \sigma$. Получили $[x]_{\eps(\Sigma)} \sbs \sigma$.
%
%Допустим теперь, что $\tau \in A/ \eps(\Sigma)$. Тогда $\tau = [x]_{\eps(\Sigma)}$ для некоторого $x \in A$. 
%С другой стороны, $x \in \sigma$ для некоторого $\sigma \in \Sigma$ в силу $\cup \Sigma = A$. По доказанному $\sigma = [x]_{\eps(\Sigma)}$, а значит, $\tau = \sigma \in \Sigma$. Таким образом, $\pi(\eps(\Sigma)) \sbs \Sigma$.
%
%Обратно, пусть $\tau \in \Sigma$. Поскольку $\tau \neq \void$, можно выбрать $x \in \tau$. Имеем тогда $\tau = [x]_{\eps(\Sigma)} \in A / \eps(\Sigma)$. Итак, $\Sigma \sbs \pi(\eps(\Sigma))$.
%\end{proof}

\begin{corr}\label{L13:eq_part}
	The function $\pi\colon Eq(A) \to \Pi(A)$ is bijective and $\eps = \pi^{-1}$.
\end{corr}

\paragraph{Counting equivalences.}
Let $A$ be a finite set of size $n$. How many distinct equivalence relations on $A$ are possible? Due to Corollary~\ref{L13:eq_part}, it suffices to count all the partitions of $A$. The number $|Eq(A)| = |\Pi(A)|$ is denoted by $B_n$ and called the $n$-th \emph{Bell number}. In analogy to our ``just the size matters'' principle, it is clear that this number depends on $n$ but not on the exact nature of the set $A$ (as $A \sim A'$ implies $Eq(A) \sim Eq(A')$---check this!). So, formally one might define $B_n = |\Pi(\ul{n})|$.

\begin{thm}
	$B_0 = 1$ and $\forall n \in \N\ B_{n + 1} = \sum_{k = 0}^{n} C_n^k B_k$.
\end{thm}
\begin{proof}
	From Exercise~\ref{L13:ex_part_empty}, we know that $\void$ is the only possible partition of $\ul{0} = \void$ (the empty collection of non-empty `pieces'). Hence the first statement.
	
	Clearly, $n \in \ul{n+1}$. In order to specify a partition of $\ul{n+1}$, it is necessary and sufficient to determine the `piece' $X \sbs \ul{n+1}$ where $n$ belongs to and fix some partition of $\ul{n+1} \setminus X$. As $n \in X$, it remains to determine the subset $X' = X \setminus \{n\}$ of $\ul{n}$ and fix a partition of $\ul{n} \setminus X'$. The cardinality of $X'$ can be anything from $0$ to $n$. If it is fixed to be $k$, one has as many partitions of $\ul{n+1}$ as elements in $\mP_{k}(\ul{n}) \times \Pi(\ul{n} \setminus X')$ (this bijection is easy), which results in the number $C^k_n B_{n-k}$.
	
	For distinct values of $k$, our partitions must be distinct. So, making $k$ run over its range, we obtain $B_{n+1} = \sum_{k=0}^n C^k_n B_{n-k}$ by the rule of sum. Since $C^k_n = C^{n - k}_n$, we have $B_{n+1} = \sum_{k=0}^n C^{n-k}_n B_{n-k} = \sum_{k=0}^n C^k_n B_k$ after renaming $n - k$ into $k$.
\end{proof}
\begin{exm}
	We obtain one by one: $B_1 = C_0^0 B_0 = 1\cdot 1 = 1$; $B_2 = C_1^0 B_0 + C_1^1 B_1 = 1 + 1 = 2$; $B_3 = C_2^0 B_0 + C_2^1 B_1 + C_2^2 B_2 = 1 + 2 + 2 = 5$. Here are all the five possible partitions of $\ul{3}$: $\{\{0\}, \{1\}, \{2\} \}$; $\{\{0\}, \{1, 2\} \}$; $\{\{0, 1\}, \{2\} \}$; $\{\{0, 2\}, \{1\} \}$; $\{\{0, 1, 2\} \}$.
\end{exm}
\noindent Many combinatorial applications and interesting properties of Bell numbers are known today.

\paragraph{Dilworth's Theorem.} Now, let us explore some combinatorial properties of orderings. This time, they will not be about counting but rather will reflect some `extreme' possibilities for a finite poset structure.

Let $(A, <)$ be a poset. A partition $\Sigma$ of $A$ is called a \emph{partition into chains} if each set $C \in \Sigma$ is a chain in the poset.

\begin{exm}\label{dilworth_exm}
	Let the set $A = \{0, 1, 2, 3, 4\}$ be ordered by the relation ${<} = \{(0, 1),\ (0, 2),\ (0, 3),\linebreak (0, 4),\ (1,4),\ (2, 4),\ (3, 4)\}$. Then $\Sigma_1 = \{ \{2, 4\}, \{0\}, \{1\}, \{3\} \}$ and $\Sigma_2 = \{ \{0, 1 , 4\}, \{2\}, \{3\} \}$ are two possible partitions of $A$ into chains.
	
	What is the minimal possible size of such a partition? Clearly, the elements $1$, $2$, and $3$ form an antichain in the poset, so no two of them can share a `piece' in a partition into chains. Hence, no partition into less than three chains is possible.
	
	On the other hand, $\Sigma_2$ is a partition of the least size allowed. Thus, it is `optimal'.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{dilworth_exm.pdf}
	\caption{Example~\ref{dilworth_exm}.}
\end{figure}


\begin{lemma}\label{L13:dilw_triv}
	Let $(A, <)$ be a finite poset, $\Sigma$ be a partition thereof into chains, and $D$ be an antichain in this poset. Then $|D| \leq |\Sigma|$.
\end{lemma}
\begin{proof}
	Let us map each element $x \in D$ to the (only) `piece' $C_x$ of the partition $\Sigma$ that contains $x$. If $C_x = C_y$ for two distinct $x, y \in D$, then $x, y \in C_x$, so $x$ and $y$ are comparable to each other, which is not the case. Hence, the mapping $x \mapsto C_x$ from $D$ to $\Sigma$ is injective. By the Pigeonhole Principle, $|D| \leq |\Sigma|$.
\end{proof}

But given each antichain is of length $r$ or less, is it always possible to partition the poset into no more than $r$ chains? As a matter of fact, it is. At first, let us establish a simple auxiliary statement.

\begin{lemma}\label{L13:fin_min}
	If $(A, <)$ is a finite poset, then for each $x \in A$, there exist $u \in \min A$ and $v \in \max A$ such that $u \leq x \leq v$.
\end{lemma}
\begin{proof}
	It suffices to prove this only for maxima, as the argument is otherwise similar. Let $n = |A|$ and assume the contrary. Since $x \notin \max A$, there exists $x_1 > x$. But $x_1 \notin \max A$ by our assumption. Hence, there is some $x_2 > x_1$. Repeating this argument $n$ times, one gets a sequence $x = x_0 < x_1 < \ldots < x_n$. This can be viewed as a function $f\colon \ul{n + 1} \to A$, where $f(k) = x_k$. If $k \neq m$, then, w.\,l.\,o.\,g., $k < m$, whence $f(k) < f(m)$ by transitivity. Thus, $f(k) \neq f(m)$ and $f$ is an injection. We see that $\ul{n + 1} \lesssim A \sim \ul{n}$, which contradicts the Pigeonhole Principle.
\end{proof}

\begin{thm}[Dilworth]
	Let $(A, <)$ be a finite poset. If $D$ is an antichain in $(A, <)$ of the greatest possible length $|D|$, then there exists a partition $\Sigma$ of that poset into $|D|$ chains.
\end{thm}
\begin{proof}
	Let $n = |A|$. We prove the claim by (strong) induction on $n$. If $n = 0$, the only antichain is empty, which corresponds to the empty partition. Assume that $n > 0$ and the claim holds for every $n' < n$.
	
	As $A$ is non-empty, Lemma~\ref{L13:fin_min} provides us with two elements $m, M \in A$ such that $m \leq M$, $m \in \min A$, and $M \in \max A$ (these may coincide). Let $r$ be the greatest length of an antichain in $A$ and $A' = A \setminus \{ m, M \}$. Suppose that $D \sbs A'$ is a longest antichain in $A'$.
	
	Clearly, it is an antichain in $A$ as well, so $|D| \leq r$. If $|D| \leq r - 1$, there is a partition $\Sigma'$ of the set $A'$ into $|D|$ chains by the inductive hypothesis (for $|A'| <  n =|A|$). Then $\Sigma = \Sigma' \cup \{ \{ m, M \} \}$ is a partition of $A$ into exactly $|D| + 1 \leq r$ chains. On the other hand, $|\Sigma| \geq r$ by Lemma~\ref{L13:dilw_triv}. So, $\Sigma$ is just as desired.
	
	Otherwise, $|D| = r$. In this case, let us define two sets
	$$A_{-} = \{ x \in A \mid \exists y \in D\ x \leq y  \}\quad\mbox{and}\quad A_{+} = \{ x \in A \mid \exists y \in D\ y \leq x \}.$$
	It is easy to see that $A_{-} \cap A_{+} = D$. Clearly, $D \sbs A_{-} \cap A_{+}$. Conversely, let $x \in A_{-} \cap A_{+}$. Then there are elements $y_1, y_2 \in D$ such that $y_1 \leq x$ and $x \leq y_2$, which implies $y_1 \leq y_2$. As $D$ is an antichain, it must be that $y_1 = y_2$, whence $x = y_1 = y_2$ by antisymmetry. So, $x \in D$.
	
	On the other hand, let us check $A_{-} \cup A_{+} = A$. Clearly, $A_{-} \cup A_{+} \sbs A$. Now, assume that $x \in A$ but $x \notin A_{-}$ and $x \notin A_{+}$. This means that $x$ is incomparable with any element of $D$ and that $x \notin D$, so $D \cup \{ x \}$ is an antichain of length $r + 1$ in $A$, which is not possible.
	
	If $M \in A_{-}$, then $M \leq y$ for a certain $y \in D$. As $M$ is maximal, we get $M = y \in D \sbs A'$. But $M \notin A'$. So, $M \notin A_{-}$. Likewise we obtain $m \notin A_{+}$.
	
	We see that $|A_{-}| < n = |A|$, while $D \sbs A_{-} \sbs A$ is a longest possible antichain in $A_{-}$. By the inductive hypothesis, this gives us a partition $\Sigma_{-}$ of the set $A_{-}$ into $r$ chains. For every $y \in D$, we let $C_y^{-}$ be the only chain from $\Sigma_{-}$ containing $y$. As $D$ is an antichain, one has $C^{-}_y \cap D = \{ y \}$ and $C^{-}_y \neq C^{-}_z$ if $y \neq z$. So, the mapping $y \mapsto C^{-}_y$ from $D$ to $\Sigma_{-}$ is injective. That is, the set $\{ C^-_y  \mid y \in D \} \sbs \Sigma_{-}$ has cardinality $|D| = r$ (cf.~Remark~\ref{ch0:inj_img}). As $\Sigma_{-}$ has the same cardinality $r$, it should be that $\{ C^-_y  \mid y \in D \} = \Sigma_{-}$ by the Rule of Sum. (Alternatively, one can refer to Theorem~\ref{L10:fin_sur_in}, which forces $y \mapsto C^{-}_y$ to be a surjection.)
	
	By a similar argument, we obtain a partition $\Sigma_{+} = \{ C^+_y  \mid y \in D \}$ of the set $A_{+}$ whose cardinality is $r$. Now, consider the set $\Sigma = \{ C^-_y \cup C^+_y \mid y \in D \}$. Its elements  are disjoint (and distinct) for distinct $y$'s: indeed, if $y \neq z$,
	$$(C^-_y \cup C^+_y) \cap (C^-_z \cup C^+_z) = (C^-_y \cap C^-_z) \cup (C^-_y \cap C^+_z) \cup (C^+_y \cap C^-_z) \cup (C^+_y \cap C^+_z).$$
	Clearly, $C^-_y \cap C^-_z = \void$ as $\Sigma_-$ is a partition, and $C^-_y \cap C^+_z = \void$ since $x \in C^-_y \cap C^+_z$ implies $x \in A_- \cap A_+ \sbs D$, whence $x = y$ and $x = z$, despite $y \neq z$. On the other hand, each $x \in A$ is covered by some element of $\Sigma$ for $A = A_{-} \cup A_{+}$ and $\Sigma_-$, $\Sigma_+$ are partitions of $A_{-}$ and $A_{+}$, respectively. Finally, each set $C^-_y \cup C^+_y$ is a chain, since from $x \in C^-_y$ and $z \in C^+_y$, it follows that $x \leq y \leq z$.  As $\Sigma \sim D$, the set $\Sigma$ is a required partition of $A$.
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.8\textwidth]{dilworth.pdf}
	\caption{Proving Dilworth's Theorem.}
\end{figure}

In view of Lemma~\ref{L13:dilw_triv}, the partition $\Sigma$ just constructed is of the least possible size. So, combining the lemma and theorem, we obtain
\begin{thm}[Dilworth's Theorem, another form]
	Suppose that $(A, <)$ is a finite poset. Then the greatest possible length of an antichain in $(A, <)$ equals the least possible size of a partition of $A$ into chains.
\end{thm}

From Dilworth's Theorem, it is possible to extract some information about the \emph{lengths} of chains in the poset.
\begin{corr}\label{chain_size}
	Suppose that $(A, <)$ is a finite poset. If $|A| = n m + 1$, then there is either an antichain of length $n + 1$ or a chain of length $m + 1$ in $(A, <)$.
\end{corr}
\begin{proof}
	Assume the contrary. Then the longest antichain has at most $n$ elements (otherwise, any longer antichain could be cut to $n + 1$ elements). By Dilworth's Theorem, there exists a partition of $A$ into no more than $n$ chains. These chains are pairwise disjoint while each being of cardinality $m$ or less. By the Rule of Sum, the union of all those chains has at most $n m$ elements. On the other hand, that union equals $A$ (for a \emph{partition} it is) with $|A| > n m$. A contradiction.
\end{proof}

This result has the following concrete interpretation, which is interesting for its own sake.
\begin{thm}[Erd\H{o}s--Szekeres]
	A sequence of $nm + 1$ pairwise distinct integers has either an increasing subsequence of length $m + 1$ or a decreasing subsequence of length $n + 1$.
\end{thm}
\begin{exm} Among $7 = 2 \cdot 3 + 1 = 3 \cdot 2 + 1$ ordered integers: $-3, 1, -5, 8, 11, 0, 2$, one can spot both an increasing subsequence $-3, 1, 8, 11$ and a decreasing one $11, 0, 2$. Notice the lengths $4$ and $3$ are respectively greatest.
	
	Вy the way, this result does not seem to be that exciting when $n m$ is close to $n$ (or $m$). Say, in a sequence of $8 = 7\cdot1 + 1$ there should be either a decreasing subsequence of length $8$ (which would necessarily coincide with the whole sequence) or an increasing one of length $2$, which is always the case unless the whole sequence is decreasing.
\end{exm}
\begin{proof}
	Assume we are given a sequence $a_1, a_2, \ldots, a_s$ where $s = nm + 1$ and $i \neq j$ implies $a_i \neq a_j$. This can readily be considered as a set of pairs of the form $(i, a_i)$. We routinely identify $a_i$ with such a pair. Let us define an ordering $\prec$ on the set of pairs:
	$$a_i \prec a_j \iff i < j \wedge a_i < a_j.$$
	Clearly, this is indeed a strict ordering.  Consider a set $X = \{ (i_1, a_{i_1}),\ldots, (i_k, a_{i_k}) \}$ and assume it is a chain in this ordering. W.\,l.\,o.\,g., we may suppose that $i_1 < i_2 < \ldots < i_k$ (i.\,e., we may sort the pairs by their first coordinates, which are clearly pairwise distinct). By the assumption, if $u < v$, then $a_{i_u}$ and $a_{i_v}$ are comparable, that is, either $a_{i_u} \prec a_{i_v}$, which means $a_{i_u} < a_{i_v}$, or $a_{i_v} \prec a_{i_v}$ which implies the false statement $i_v < i_u$. So, $X$ is a chain iff $a_{i_1} < a_{i_2} < \ldots < a_{i_k}$.
	
	Likewise, $X$ is an antichain if for every $u < v$ the elements $a_{i_u}$ and $a_{i_v}$ are incomparable. As $i_u < i_v$, this is equivalent to $a_{i_u} \nless a_{i_v}$, that is, to $a_{i_u} > a_{i_v}$ since the numbers $a_{i_u}$ and $a_{i_v}$ are distinct. So, $X$ is an antichain iff $a_{i_1} > a_{i_2} > \ldots > a_{i_k}$.
	
	We see that ${\prec}$-chains correspond to increasing subsequences of the same length, whereas ${\prec}$-antichains correspond to decreasing subsequences of the same length. An application of Corollary~\ref{chain_size} finishes the proof.
\end{proof} 

It is easy to see that Corollary~\ref{chain_size} (hence, the Erd\H{o}s--Szekeres Theorem) may be also proved from the following dual of Dilworth's Theorem, which is but easier to prove.

\begin{thm}[Mirsky]
	Suppose that $(A, <)$ is a finite poset. Then the greatest possible length $m$ of a chain in $(A, <)$ equals the least possible size $n$ of a partition of $A$ into antichains.
\end{thm}
\begin{proof}
	It is obvious that $m \leq n$ (no two distinct comparable elements can share an antichain). The converse equality is obtained by induction on $|A|$. Assume that $C$ is a chain of length $m > 0$. By Lemma~\ref{L13:fin_min}, there exists a least element $x$ in $C$. Notice that $\min A$ is an antichain. Clearly, $C \cap \min A = \{ x \}$, for if $x \notin \min A$, there exists some $y < x$ so that $C \cup \{y\}$ is a chain longer than $C$. Let $A' = A \setminus \min A$. Clearly, $C' = C \setminus \{x\}$ is a longest possible chain in $A'$ (since \emph{each} chain of length $m$ in $A$ intersects $\min A$). As $|A'| < |A|$, one gets a partition $\Sigma'$ of $A'$ into $m - 1$ antichains by the IH. Then $\Sigma' \cup \{ \min A \}$ is a partition of $A$ into $m$ antichains, whence $n \leq m$.
\end{proof}

\section{Graphs: the Basics}
\mcomm{Our treatment of graphs is brief and mostly traditional. We try to employ the previously developed formalism as much as it is reasonably possible.}

\emph{Graphs} form a simple and popular formalism yet expressive enough for a great many areas in mathematics and applications. The basic idea is as follows: any two distinct `individuals' from some fixed set are either `connected by a link' or not. This may refer to, say, friendship (friends are connected), a railway network (some stations are connected), workers and skills (a worker is connected to each of his skills), etc. One should discern a pair of objects `immediately' connected by one `link' (we shall call them \emph{adjacent}) from a pair of objects connected by a sequence of `links' (like when there are some intermediate `stations'). 

There are two main features here: (1) if $x$ is adjacent to $y$, then $y$ is adjacent to $x$; (2) no $x$ is adjacent to itself. This gives rise to the following formal definition. A \emph{graph} $G$ is a pair $(V, E)$, where $V$ is an arbitrary non-empty set, whose elements are called \emph{vertices}, and $E \sbs V^2$ is a symmetric irreflexive binary relation on $V$, which is called the \emph{adjacency} relation (`a vertex $x$ is \emph{adjacent} to a vertex $y$'). This Course (as well as most of the applications) is restricted to \emph{finite graphs}, that is, we assume the set $V$ to be finite.

As $x E y$ is equivalent to $y E x$, it is reasonable to identify these two `links'. So, a $2$-element set $\{x, y\} \sbs V$ is called an \emph{edge} of the graph $G$ iff $x E y$ (equivalently, $y E x$). Abusing the notation, we will denote an edge $\{x, y\}$ as $x y$ or $y x$. The number $|V|$ is called the \emph{order} of the graph $G$ and the number of edges in $G$ is called the \emph{size} of $G$. Easily, the size of $G$ equals $|E| / 2$.

A graph $G = (V, E)$ is called an \emph{$(n,m)$-graph} if it is of order $n$ and of size $m$. For each $n$, there is a maximal possible number of edges $m$ but any combination of the two is otherwise possible.
\begin{lemma}\label{L14:l_ord_size}
	For every natural $n > 0$ and $m$, there exists an $(n,m)$-graph iff $0 \leq m \leq C_n^2$.
\end{lemma}
\begin{proof}
	Consider an arbitrary $(n,m)$-graph $G = (V, E)$. As each edge is a $2$-element subset of the set $V$ with $|V| = n$, the number $m$ of edges cannot exceed $|\mP_2(\ul{n})| = C_n^2$.
	
	For the other direction, let us consider the \emph{complete graph} $K_n = (\ul{n}, \ul{n}^2 \setminus \id_{\ul{n}})$ on $n$ vertices. In $K_n$, $E = \ul{n}^2 \setminus \id_{\ul{n}}$, so every two distinct vertices are adjacent. This means that the edge set of $K_n$ is just $\mP_2(\ul{n})$ and $K_n$ is an $(n, C_n^2)$-graph. If $0 \leq m < C_n^2$, one can easily obtain an $(n, m)$-graph by removing $C_n^2 - m$ edges from $K_n$. Removing an edge $u v$ from a graph $G = (V, E)$ means formally that we let $E' = E \setminus \{(u, v), (v, u)\}$ and get the graph $G' = (V, E')$. Clearly, $E'$ is still symmetric and irreflexive, hence $G'$ is a graph indeed.
\end{proof}

The set $N_G(x) = \{ y \in V \mid x E y \}$ of all vertices adjacent to $x$ in a graph $G = (V, E)$ is called the \emph{neighborhood} of the vertex $x$ in $G$ (hence, $y$ is a \emph{neighbor} of $x$). The number $d_G(x) = |N_G(x)|$ is called the \emph{degree} of $x$ in $G$. As usual, the subscript `$G$' is omitted when the graph is clear from the context. Since $N_G(x)\sbs V \setminus \{ x \}$, we have $0 \leq d(x) \leq n - 1$ for an $(n, m)$-graph $G$. Both bounds are tight (the upper one is reached, say, for $K_n$).

If one sort the degrees of all the vertices from $G$ in descending order, one obtains the \emph{degree sequence} $(d(v_1), d(v_2), \ldots, d(v_n))$ of the graph $G$, where $d(v_i) \geq d(v_{i + 1})$.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_degrees.pdf}
	\caption{(a) The complete graph $K_5$. (b) A graph with the degree sequence $(3, 2, 2, 2, 2, 1, 1, 1, 0)$. Each vertex' degree is shown. According to Lemma~\ref{L14:l_handshake}, we have  $7$ edges here and $3 + 2 + 2 + 2 + 2 + 1 + 1 + 1 + 0 = 2 \cdot 7$.}
\end{figure}

The overall number of handshakes made among a group of people equals half the sum of handshake numbers every person has partaken in.
\begin{lemma}[Handshake Lemma]\label{L14:l_handshake}
	For every $(n,m)$-graph $G = (V, E)$, it holds that $$\sum_{v \in V} d_G(v) = 2 m.$$
\end{lemma}
\begin{proof}
	By induction on $m$. If $m = 0$, then there are no edges in $G$ and $d_G(v) = 0$ for each $v \in V$, whence the required statement follows. Assume that $m > 0$ and the statement holds for \emph{every} $(n,m')$-graph $G'$ with $m' < m$. Let us remove an arbitrary edge $x y$ from $G$ obtaining thus some new graph $G' = (V', E')$. Clearly, $V' = V$, $d_{G'}(x) = d_G(x) - 1$,  $d_{G'}(x) = d_G(x) - 1$, $d_{G'}(v) = d_G(v)$ for every $v \in V \setminus \{x, y\}$, and $G'$ is a $(n,m-1)$-graph. By the IH, we get
	\begin{multline*}
		\sum_{v \in V} d_G(v) = \sum_{v \in V' \setminus \{x, y \}} d_{G'}(v) + (d_{G'}(x) + 1) + (d_{G'}(y) + 1) =\\
		\sum_{v \in V} d_{G'}(v) + 2 = 2(m-1) + 2 = 2m. 
	\end{multline*}
	
\end{proof}

\begin{exm}
	The lemma above provides one of the simplest necessary conditions for graph existence. Indeed, there is no graph with the degree sequence $(4,3,2,2,1,1)$ as the sum of those degrees is odd.
\end{exm}

\paragraph{Graph isomorphism.} Like most of the other mathematical \emph{structures} (orders, groups, etc.), graphs are usually considered \emph{up to isomorphism}. This means that the nature of vertices is irrelevant provided the `form' of the graph is fixed. Say, two triangles $\{0, 1, 2\}$ and $\{\pi, e, \mathrm{HSE}\}$ with edges $\{01, 12, 20 \}$ and $\{ \pi e, e\, \mathrm{HSE}, \mathrm{HSE}\, \pi \}$, respectively, are essentially the same.

Let us formalize this crucial idea. Two graphs $G = (V, E)$ and $G = (V', E')$ are said to be \emph{isomorphic} if there exists a function $\phi \colon V \to V'$ (which is then called an \emph{isomorhism}) such that: (1) $\phi$ is a bijection $V \to V'$; (2) for every $x, y \in V$, one has $x E y \iff \phi(x) E' \phi(y)$, that is, $\phi$ `respects' or `copies' the adjacency structure of the graphs. In such a case, we write $G \cong G'$ or, to be more specific, $G \stackrel{\phi}{\cong} G'$.

Loosely speaking, isomorphism is an equivalence relation. Indeed, it is straightforward to check the following
\begin{lemma} For every graphs $G, H, F$ and functions $\phi, \psi$, it holds that:
	\begin{enumerate}
		\item $G \stackrel{\id_{V_G}}{\cong} G$;
		\item if $G \stackrel{\phi}{\cong} H$, then $H \stackrel{\phi^{-1}}{\cong} G$;
		\item if $G \stackrel{\phi}{\cong} H$ and $H \stackrel{\psi}{\cong} F$, then $G \stackrel{\psi \circ \phi}{\cong} F$.
	\end{enumerate}
\end{lemma}

An isomorphism preserves every notion defined for a graph in terms of its adjacency relation. Let us give a few examples for this phenomenon.
\begin{lemma} If $G \cong H$, the graphs $G$ and $H$ have the same:
	\begin{enumerate}
		\item order;
		\item size;
		\item degree sequence.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let us check the last statement. Suppose that $G \stackrel{\phi}{\cong} H$. It suffices to prove that $d_G(v) = |N_G(v)| = |N_H(\phi(v))| = d_H(\phi(v))$ for every $v \in V_G$. As $v E_G u$ is equivalent to $\phi(v) E_H \phi(u)$, it is easy to see that $N_H(\phi(v)) = \phi[N_G(v)] \sim N_G(v)$, whence the required identity follows.
\end{proof}

\begin{exm}
	For the vertex set $\ul{4}$, let us consider two graphs: $G$ with edges $\{02, 13\}$ and $H$ with $\{01, 13\}$. These graphs do agree in both order and size but not in degree sequence, as it is $(1,1,1,1)$ for $G$ but $(2,1,1,0)$ for $H$. Hence, $G \ncong H$.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_iso.pdf}
	\caption{(a) Two isomorphic graphs. An isomorphism is shown as $n \mapsto n'$. (b) The graphs $O_4$, $P_3$, and $C_6$. These are defined up to isomorphism.}
\end{figure}

\begin{exc}
	Construct a pair of non-isomorphic graphs with the same degree sequence.
\end{exc}

As we have already said, graphs are usually considered `up to isomorphism', that is, ignoring their differences in vertex `names'. Let us see some important examples. We have already introduced the complete graph $K_n$ on $n$ vertices. The `complement' of this is the \emph{empty graph} $O_n \cong (\ul{n}, \void)$ that has no edges. (Notice that \emph{every} graph isomorphic to $(\ul{n}, \void)$ is denoted by $O_n$. We do not discern them from each other.)

The \emph{path} (or \emph{chain}) $P_n$ on $n > 0$ vertices $1, 2, \ldots, n$ has all the edges from $\{1 2, 2 3, 3 4, \ldots, (n - 1) n\}$. The \emph{cycle} $C_n$, $n > 1$, on the same vertices is defined by the edge set $\{1 2, 2 3, 3 4, \ldots, (n - 1) n, n 1\}$.

\paragraph{Subgraphs and complements.} It is sometimes obvious that one graph `contains' (an isomorphic copy of) another one. For example, in $K_4$ one can find $C_4^3$ copies of $K_3$. Let us formalize this idea. A \emph{graph} $G' = (V', E')$ is a \emph{subgraph} of a graph $G = (V, E)$ if $V' \sbs V$ and $E' \sbs E$. So, a subgraph of $G$ contains \emph{some} of the vertices and \emph{some} of the edges connecting those vertices in $G$.

A subgraph $G' = (V', E')$ of a graph $G = (V, E)$ is \emph{induced} (\emph{by the set $V'$}) if $G'$ contains \emph{all} the edges between the vertices of $V'$ in $G$, that is, $E' = E \cap (V' \times V')$. For example, in the $K_4$-graph with the edge set $\{12, 23, 34, 41, 13, 24\}$, the $K_3$-subgraph on the set $V' = \{1,2,3\}$ with the edges $12, 23, 31$ is induced (by the set $V'$), while the $C_4$-subgraph on the set $\{1,2,3,4\}$ with the edges $12, 23, 34, 41$ is not.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_sub.pdf}
	\caption{(a) A graph $G = (V, E)$. (b) Its subgraph $G' = (V', E')$ with vertices $V' = \{1, 3, 4, 5, 6\}$} is shown in red. (c) The subgraph $G'' = (V', E'')$ which is \emph{induced} by the set $V'$ is shown in blue.
\end{figure}

Another useful construction is \emph{complement}. The idea is as follows: keeping the vertex set unchanged, one removes every present edge from a graph but adds every edge which is possible but absent. As we have already mentioned, the complement of the complete graph $K_n$ is the empty graph $O_n$, since $K_n$ already contains all possible edges (i.\,e., every two \emph{distinct} vertices are adjacent). On the other hand, the complement of the $C_4$-graph with the edges $12, 23, 34, 41$ has just the edges $13$ and $24$.

Formally, the complement $\bar G = (V', E')$ of a graph $G = (V, E)$ is defined by $V' = V$ and $E' = (V^2 \setminus \id_{V}) \setminus E$, that is, $x E' y \iff x \neq y \wedge \neg x E y$ for every $x, y \in V$. Observe that the complement $\bar G$ of an $(n, m)$-graph $G$ is an $(n, C_n^2 - m)$-graph for $|E'| = |(V^2 \setminus \id_{V}) \setminus E| = n^2 - n - 2m = 2(C_n^2 - m)$.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_compl.pdf}
	\caption{The graph $C_4$ (in black) with its complement $P_2 \sqcup P_2$ (in red). The latter symbol denotes the union two disjoint `copies' of $P_2$ (these are disjoint indeed for they have no common vertex).}
\end{figure}

\paragraph{Paths and connectivity.} Let us fix a graph $G = (V, E)$. A vertex sequence $p = v_1 v_2 \ldots v_n$, $n > 0$, is called a \emph{path} in $G$ if $v_i E v_{i + 1}$ for each $i$ (formally, this \emph{sequence} may be interpreted as either an $n$-tuple or a function $\ul{n} \to V$). The path $p$ is \emph{simple} if $v_i \neq v_j$ when $i \neq j$. By definition, the \emph{length} $|p|$ of the path $p$ is $n - 1$ (that is, the number of \emph{edges} that link the vertices $v_i$ together). We say that vertices $x$ and $y$ are \emph{connected by the path} $p$ if $v_1 = x$ and $v_n = y$. In particular, each vertex $x$ is connected to itself by the single-vertex path $x$ of length $0$. We say that the path $p$ \emph{contains} an edge $xy$ if $v_i = x$ and $v_{i + 1} = y$ or $v_i = y$ and $v_{i + 1} = x$ for some $i$. We write $x \pth{q} y$, where $q = u_1 \ldots u_m$, for a path $x u_1 \ldots u_m y$. Permitting $m = 0$, we will use the same symbol when $x = y$, so $x \pth{q} x$ may stand for the path $x$ of length $0$.

\begin{lemma}\label{L14:simpl_path}
	If vertices $x$ and $y$ are connected by a path, then there is a \emph{simple} path that connects those vertices. Furthermore, a shortest such path is always simple.
\end{lemma}
\begin{proof}
	By the Least Number Principle, there exists a path $p$ of the \emph{least} possible length leading from $x$ to $y$. Assume that $p$ is not simple. Then there is a vertex $z$ with at least two distinct occurences in $p$, so that $p = x u_1 \ldots u_k z v_1 \ldots v_l z w_1 \ldots w_m y$, where $l \geq 1$. Clearly, the path $p' = x u_1 \ldots u_k z w_1 \ldots w_m y$ also connects $x$ to $y$, while $|p'| < |p|$. A contradiction.
\end{proof}

A path $c = v_1 \ldots v_n v_{n + 1}$ is called a \emph{cycle} in $G$ if $v_{n + 1} = v_1$ and $c$ has all distinct edges, that is, $\{ v_i, v_{i+1} \} \neq \{ v_j, v_{j + 1}\}$ whenever $i \neq j$. The latter condition means that we do not admit a path $u_1 u_2 u_3 u_2 u_1$ as a cycle (because otherwise every edge $x y$ would give rise to a cycle of arbitrarily great length: $x y x y \ldots x y x$, which would make the notion of cycle trivial). The cycle $c$ is \emph{simple} if all vertices $v_1, \ldots, v_n$ are pairwise distinct.

Let us write $x \sim_G y$ when the vertices $x$ and $y$ are connected in $G = (V, E)$ by some path. Clearly, ${\sim_G}$ is a binary relation on $V$.

\begin{lemma}
	The relation ${\sim_G}$ is an equivalence relation on the set $V$.
\end{lemma}
\begin{proof}
	One has $x \sim_G x$ as $x$ is connected to itself by the zero length path $x$. If $x v_1 \ldots v_n y$ is a path, then $y v_n \ldots v_1 x$ is a path as well, hence $x \sim_G y$ implies $y \sim_G x$. Given $x v_1 \ldots v_n y$ and $y u_1 \ldots u_m z$ are paths, one gets the path $x v_1 \ldots v_n y u_1 \ldots u_m z$ connecting $x$ to $z$, so from $x \sim_G y$ and $y \sim_G z$, it follows that $x \sim_G z$.
\end{proof}

Consider the quotient set $V / {\sim_G} = \{ [x]_{\sim_G} \mid x \in V \}$, where $[x]_{\sim_G} = \{ y \in V \mid x \sim_G y \}$. Each equivalence class $[x]_{\sim_G}$ consists of all the vertices connected to $x$. Notice that $N_G(x) \sbs [x]_{\sim_G}$ but not vice versa, generally. Such classes are called \emph{connected components} of the graph $G$. A graph is called \emph{connected} if it has exactly one connected component (i.\,e., $|V_G / {\sim_G}| = 1$ for our graph $G$) or, equivalently, every two vertices thereof are connected by a path (i.\,e., ${\sim_G}$-equivalent). Otherwise, a graph is called \emph{disconnected}.

\begin{exc}
	If $G \cong H$, then $|V_G / {\sim_G}| = |V_H / {\sim_H}|$. In particular, $G$ is connected iff $H$ is connected.
\end{exc}

\begin{lemma}[connected graph edge removal]\label{L14:l_conn_rem}
	Suppose that $G = (V, E)$ is a connected graph and $xy$ is an edge in $G$. We let $G'$ be the result of removing the edge $xy$ from $G$. Then the following hold:
	\begin{enumerate}
		\item if $xy$ belongs to a cycle in $G$, then $G'$ is still connected;
		\item if $xy$ belongs to no cycle in $G$, then $G'$ has exactly $2$ connected components; namely, $G' / {\sim_{G'}} = \{ [x]_{\sim_{G'}}, [y]_{\sim_{G'}} \}$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	For the first statement, consider some cycle $a \pth{q} x y \pth{q'} a$ and an arbitrary path $p = u \pth{r} v$ in $G$. By the definition of cycle, the path $y \pth{q'} a \pth{q} x$ does not contain $xy$. If $p$ does not contain the edge $xy$, it is still a path in $G'$. Suppose it does. By Lemma~\ref{L14:simpl_path} and symmetry of ${\sim_G}$, we may assume that $p$ is simple and $p = u \pth{r'} y x \pth{r''} v$, where neither $u \pth{r'} y$ nor $x \pth{r''} v$ contains $xy$. Then we have a path $u \pth{r'} y \pth{q'} a \pth{q} x \pth{r''} v$ in $G'$. Hence $u \sim_G v$ implies $u \sim_{G'} v$ as required.
	
	Now assume that $xy$ does not belong to a cycle. Let $A = [x]_{\sim_{G'}}$ and $B = [y]_{\sim_{G'}}$. It suffices to prove that $A$, $B$ are the only connected components; that is, $A \cup B = V$ and $A \cap B = \void$ (cf.~Lemma~\ref{ch0:eq_class}). For the first equation, consider a vertex $v \in V$. Since $G$ is connected, there are some paths connecting $x$ to $v$ and $y$ to $v$. Let us take \emph{shortest} such paths $p$ and $p'$, respectively, which must be simple by Lemma~\ref{L14:simpl_path}. W.\,l.\,o.\,g., $|p| \leq |p'|$.
	
	We claim that the path $p$ does not contain the edge $x y$. Otherwise, $p = x y \pth{q} v$, where $p'' = y \pth{q} v$ does not contain $x y$. Clearly, $|p''| < |p| \leq |p'|$, hence $p$ is \emph{not} the shortest path that connects $y$ to $v$. A contradiction. So, $p$ is a path in $G'$ and $v \in A$.
	
	Now, consider the second equation and assume, for the contrary, that $A \cap B \neq \void$. Then $x \sim_{G'} y$ by Lemma~\ref{ch0:eq_class}. Thus, there is a (simple) path $x \pth{p} y$ in $G'$ (hence, in $G$) that does not contain $x y$. Adding $x y$ to this path, we get a cycle $x \pth{p} y x$ in $G$, which is not possible.
\end{proof}

\begin{corr}\label{L14:c_conn_rem}
	Suppose that $G = (V, E)$ is a graph with exactly $k$ connected components and $xy$ is an edge in $G$. We let $G'$ be the result of removing the edge $xy$ from $G$. Then the following hold:
	\begin{enumerate}
		\item if $xy$ belongs to a cycle in $G$, then $G'$ has exactly $k$ connected components;
		\item if $xy$ belongs to no cycle in $G$, then $G'$ has exactly $k + 1$ connected components.
	\end{enumerate}
\end{corr}
\begin{proof}
	Clearly, $x$ and $y$ belong to the same component of $G$ (which is a connected graph). Applying the Lemma above to this component, we see that it is either still connected or split into two connected components. Hence, we either have $k$ or $k + 1$ connected components in $G'$.
\end{proof}

It is natural to expect a connected graph to have not too few edges: the more cities you have, the more roads you need to connect them with one another.
\begin{lemma}\label{L14:conn_edge_num}
	If an $(n,m)$-graph $G$ is connected, then $m \geq n - 1$.
\end{lemma}
\begin{proof}
	By induction on $m$. Let $m = 0$. If $G$ has at least two distinct vertices, these cannot not be connected by a path, hence, it should be $n = 1$, as required.
	
	Assume that $m > 0$ and that $m' \geq n - 1$ for every $m' < m$, every $n'$ and each connected $(n',m')$-graph (IH). Consider an arbitrary connected $(n, m)$-graph $G$ and remove an edge therefrom. By Lemma~\ref{L14:l_conn_rem}, this shall result in an $(n,m - 1)$-graph $H$ which is either connected or consists of two connected components that are an $(n',m')$-graph $G'$ and an $(n'', m'')$-graph $G''$.
	
	In the former case, we have $m - 1 \geq n - 1$ by the IH, whence $m \geq n - 1$. In the latter one, we get $n' + n'' = n$ and $m' + m'' = m - 1$. As $m', m'' < m$, obtain $m' \geqslant n' - 1$ and $m'' \geqslant n'' - 1$ by the IH. This implies $m - 1 \geq n - 2$, whence $m \geq n - 1$ again.
\end{proof}

\begin{exc}
	Prove that for every $n \in \N_+$, there exists a connected graph with exactly $n - 1$ edges; it is not thus possible to improve the bound set by Lemma~\ref{L14:conn_edge_num} in general.
\end{exc}

A connected graph $G$ is called \emph{minimally connected} if removing \emph{any} edge makes it disconnected. So, a minimally connected road system has no redundancy: block one road and some cities get unreachable from each other. The following statement will be of use below.

\begin{lemma}\label{L14:l_min_conn_edge}
	If an $(n,m)$-graph $G$ is minimally connected, then $m = n - 1$.
\end{lemma}
\begin{proof}
	By Lemma~\ref{L14:l_conn_rem}, we get two connected components if remove any edge from $G$. Clearly, each of these two is also minimally connected (otherwise some edge would be safe to remove from $G$). So, we may iteratively apply the same procedure to each of the components, while it still has at least one edge. After every removal of an edge, there is one more connected component than before it. At the procedure termination, we have $m$ removals done (each edge has been removed), which should result in a graph with $1 + m$ connected components.
	
	On the other hand, this graph has no edges, so every vertex is connected but to itself. Hence, there are exactly $n$ connected components. Therefore, $1 + m = n$ as desired.
\end{proof}

\begin{exc}
	Is it the case that an $(n, n - 1)$-graph is always connected?
\end{exc}

\section{Special Form Graphs}

\paragraph{Trees.} One of the most common graph classes in applications (say, in computer programming) is \emph{trees}. By definition, a graph is a \emph{tree} if it is both connected and \emph{acyclic} (that is, contains no cycle as a subgraph). There are other natural ways to define a tree as the following theorem shows.
\begin{thm}\label{L14:t_tree}
	Let $G$ be an $(n,m)$-graph. Then the following statements are equivalent:
	\begin{enumerate}
		\item $G$ is a tree;
		\item $G$ is minimally connected;
		\item $G$ is connected and $m = n - 1$;
		\item every two vertices of $G$ are connected by a unique simple path.
	\end{enumerate}
\end{thm}
\begin{proof}
	Assume that $G$ is a tree. Since $G$ has no cycle, by Lemma~\ref{L14:l_conn_rem}, it becomes disconnected if one removes any edge therefrom. As $G$ is connected, $G$ must be minimally connected.
	
	If $G$ is minimally connected, then $m = n - 1$ by Lemma~\ref{L14:l_min_conn_edge}.
	
	Assume that $G$ is connected and $m = n - 1$ but, for a contradiction, $G$ is not a tree. Then $G$ must have a cycle. Let us remove an arbitrary edge from that cycle. By Lemma~\ref{L14:l_conn_rem}, we obtain a connected $(n, m - 1)$ graph $G'$, for which it holds that $m - 1 \geq n - 1$ by Lemma~\ref{L14:conn_edge_num}. Then $m - 1 \geq m$. A contradiction.
	
	Thus, the first three statements are pairwise equivalent. Now, assume that every two vertices of $G$ are connected by a unique simple path, yet $G$ is not minimally connected. Let $x y$ be an edge which can be removed from $G$ so that the resulting graph $G'$ is still connected. By Lemma~\ref{L14:simpl_path}, there is a \emph{simple} path $x \pth{p} y$ in $G'$ (hence, in $G$). This path cannot contain $xy$, so $x \pth{p} y$ and $x y$ are two distinct simple paths between $x$ and $y$ in $G$. A contradiction.
	
	\begin{figure}[h]
		\centering
		\includegraphics*[width=0.60\textwidth]{graph_tree_def.pdf}
		\caption{Proving Theorem~\ref{L14:t_tree}.}
	\end{figure}
	
	
	Finally, we suppose that $G$ is a tree. Then every two vertices of $G$ are, clearly, connected by a simple path (in view of Lemma~\ref{L14:simpl_path}). For a contradiction, assume that for some vertices $x$ and $y$, there are two such distinct paths $x \pth{p} y$ and $x \pth{q} y$. As $p \neq q$, one can find the leftmost vertices that differ these paths from each other and then, the leftmost common vertex to the right of these. So, we get $x \pth{r} u v \pth{p'} z \pth{p''} y$ and $x \pth{r} u w \pth{q'} z \pth{q''} y$, where $v \neq w$ and the sequences $p'$ and $q'$ have no common vertex. It is clear that $u v \pth{p'} z \pth{s'} w u$ is a cycle in $G$, where $s' = a_m \ldots a_1$ if $q' = a_1 \ldots a_m$. A contradiction.
\end{proof}


Let us see an important example of a tree. Consider the set of all binary words of lengths $0, 1, \ldots, n$, that is, the set $\ul{2}^{\leq n} = \ul{2}^{\ul{0}} \cup \ul{2}^{\ul{1}} \cup \ldots \cup \ul{2}^{\ul{n}}$. (Recall that we identify finite sequences from $A^{\ul{n}}$ with $n$-tuples from $A^n$, and have that $A^1 = A$ and $A^0 = \{ \void \}$; in the context of words, the set $\void$ is called \emph{the empty word} and denoted by $\eps$.) It is natural to write a word $w \in \ul{2}^{m+1}$ as $w_0\ldots w_m$, where $w_i \in \ul{2}$. We say that the word $u v$ is the \emph{concatenation} of words $u = u_1 \ldots u_m$ and $v = v_1 \ldots v_n$ if $u v = u_1 \ldots u_m v_1 \ldots v_n$, while $u \eps = \eps u = u$.

Let us say that words $u$ and $v$ are \emph{adjacent} iff $u = v x$ or $v = u x$, where $x \in \ul{2}$. For example, the words $1101$ and $110$, as well as $110$ and $1100$ are adjacent. Clearly, this adjacency relation is irreflexive and symmetric, hence we have a graph on the set $\ul{2}^{\leq n}$. This graph is called the \emph{perfect binary tree} $T_n = (V, E)$. But is it indeed a tree? Let us check it!

Assume that $n > 0$, for $T_0$ is otherwise a sure tree. How many vertices does the graph $T_n$ have? Clearly, $|V| = |\ul{2}^{\leq n}| = 2^0 + 2^1 + \ldots + 2^n$ by the Rules of Sum and Product. Applying a well-known fact about the sum of a geometric progression (easily provable by induction), we obtain $|V| = 2^{n+1} - 1$. How many edges does $T_n$ have?

We see that there is just one vertex $\eps$ (the \emph{root} of the tree) of degree $2$ (as it is adjacent to $0$ and $1$), exactly $2^n$ vertices of degree $1$, which are the words of the maximal length $n$ (\emph{leafs} of the tree), each being adjacent to its longest proper prefix only: $v_1 \ldots v_{n - 1} E v_1 \ldots v_{n - 1} v_n$. Every other vertex $u_1 \ldots u_k$ is of degree $3$ since $N(u_1 \ldots u_k) = \{u_1 \ldots u_{k-1}, u_1 \ldots u_k 0, u_1 \ldots u_k 1\}$ when $1 \leq k < n$.

If $m$ is the size of $T_n$, we get $2 m = 1 \cdot 2 + 2^n \cdot 1 + (2^{n + 1} - 1 - 1 - 2^n) \cdot 3$ by the Handshake Lemma~\ref{L14:l_handshake}. This results in $m = 2^{n + 1} - 2 = |V| - 1$. On the other hand, the graph $T_n$ is connected since every vertex $v_1 \ldots v_k$ thereof is connected to $\eps$ by the path 
$\eps,\ v_1,\ v_1 v_2,\ v_1 v_2 v_3,\ \ldots, v_1 v_2 \ldots v_{k-1},\  v_1 v_2 \ldots v_{k-1} v_k$.

From Theorem~\ref{L14:t_tree}, it follows now that $T_n$ is indeed a tree.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_perf_tree.pdf}
	\caption{The perfect binary tree $T_3$.}
\end{figure}

\medskip

In fact, every connected graph is ``something more than a minimally connected graph'', that is, it contains a minimally connected subgraph (i.\,e., a tree). For a graph $G = (V, E)$, a tree $T = (V, E')$ is called a \emph{spanning tree} of $G$ if $E' \sbs E$. So, a spanning tree links all the vertices of $G$ together without any redundant edge.

\begin{thm}\label{L14:t_spanning}
	Every connected $(n,m)$-graph $G = (V, E)$ has a spanning tree $T$.
\end{thm} 
\begin{proof}
	By induction on $m$. If $m = 0$, as a vacuous truth, the graph is minimally connected for it is still connected after any edge removal (it does not change this way). So, put $T = G$.
	
	Suppose that $m > 0$. If $G$ is minimally connected, then it suffices to put $T = G$ again. Otherwise, there exists an edge $xy$ which can be safely removed, so that the resulting $(n, m - 1)$-graph $G' = (V, E \setminus \{(x,y), (y,x) \})$ is connected. By the IH, there is a spanning tree $T' = (V, E')$ in $G'$. As $E' \sbs E \setminus \{(x,y), (y,x) \} \sbs E$, this $T'$ is a spanning tree in $G$ as well.
\end{proof}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{graph_spanning.pdf}
	\caption{Two non-isomorphic spanning trees of a graph a shown in (a) red and (b) blue.}
\end{figure}


\paragraph{Bipartite graphs and colorings.} Sometimes, graphs are employed to model a situation where `vertices' represent objects of different kinds. For example, one can connect each worker to each of his skills (two kinds of vertices: workers and skills); or it is possible to link every point (from some finite geometric configuration) to every line it belongs to. In these natural examples, no two vertices of the same kind are adjacent. This restriction gives rise to a special class of graphs.

A graph $G = (V, E)$ is called \emph{bipartite} if there exist two non-empty sets $V_1$ and $V_2$ such that $V_1 \cup V_2 = V$, $V_1 \cap V_2 = \void$, and $x y \notin E$ for every $x, y \in V_i$ for each $i$. That is, there is a partition of the vertex set into two pieces neither of which has an `internal' edge.

\begin{exm}
	The cycle $C_4$ with edges $01, 12, 23, 30$ is bipartite as one may take $V_1 = \{0, 2\}$ and $V_2 = \{1, 3\}$. The cycle $C_3$ (as well as every graph that contains it as a subgraph, e.\,g. $K_n$ for $n \geq 3$) is not bipartite since every two distinct vertices of $C_3$ are adjacent, so cannot belong to the same `piece' $V_i$. Hence, $|V_i| \leq 1$ and $|V_1 \cup V_2| \leq 2 < 3$.
\end{exm}

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.6\textwidth]{graph_bipartite.pdf}
	\caption{(a) A bipartite graph on five vertices. The two `pieces' are shown in color. (b) The \emph{complete bipartite} graph $K_{3, 2}$. Each vertex of one `piece' is adjacent to every vertex of the other one.}
\end{figure}


More generally, a graph $G = (V, E)$ is called \emph{$k$-partite} if there exist non-empty sets $V_1,\ldots, V_k$ such that $V_1 \cup \ldots \cup  V_k = V$, $V_i \cap V_j = \void$ if $1 \leq i < j \leq k$, and $x y \notin E$ for every $x, y \in V_i$ for each $i$. Clearly, every $(n,m)$-graph is $n$-partite; empty graphs $O_n$ are only $1$-partite; and if a graph $G$ is $k$-partite with $|V_i| \geq 2$ for some $i$, then $G$ is $(k+1)$-partite as well (it is possible to split $V_i$ into two non-empty sets without any `internal' edges).

Another way to speak about vertex partitions is \emph{colorings}. A \emph{(proper) coloring} of a graph $G = (V, E)$ in $k$ colors is a function $c\colon V \to \ul{k}$ such that $c(x) = c(y)$ implies $(x, y) \notin E$ for every $x, y \in V$ (i.\,e., no two vertices of one color are adjacent). Clearly, for every $k$, a graph is $l$-partite for some $l \leq k$ iff there is a coloring thereof in $k$ colors. Indeed, put $V_{i+1} = c^{-1}[\{ i \}]$; notice that $c^{-1}[\{ i \}] = \void$ may hold for some $i$ (when color $i$ is not used), hence we have just $l \leq k$ non-empty `pieces' in general.

\medskip

Let us obtain a simple criterion for a graph to be bipartite.
\begin{thm}\label{L14:bipartite}
	For every graph $G = (V, E)$ such that $|V| \geq 2$, the following statements are equivalent:
	\begin{enumerate}
		\item $G$ is bipartite;
		\item $G$ has no cycle of odd length;
		\item $G$ has no simple cycle of odd length.
	\end{enumerate}
\end{thm}
\begin{proof}
	Suppose that $G$ is bipartite with a partition $\{V_1, V_2\}$. Assume that $G$ has an odd length cycle $x_1 x_2 \ldots x_{2n} x_{2n + 1} x_1$. W.\,l.\,o.\,g., $x_1 \in V_1$. As $x_1 E x_2$, we get $x_2 \in V_2$. By an easy induction, we obtain $x_{2n + 1} \in V_1$. But $x_{2n + 1} E x_1$. A contradiction.
	
	The third statement follows immediately from the second one.
	
	Suppose $G$ has no simple cycle of odd length. Let us define a partition $\{V_1, V_2\}$ of the set $V$ which makes $G$ bipartite. We claim that it suffices to obtain such a partition for every \emph{connected} graph $G$. Indeed, let $G$ consist of connected components $G_1, G_2, \ldots, G_k$, each of which contains no odd length cycle, hence, is bipartite with a partition $\{V^i_1, V^i_2\}$. Then we may put $V_1 = V^1_1 \cup \ldots \cup V^k_1$ and $V_2 = V^1_2 \cup \ldots \cup V^k_2$. The case when a few components are of order $1$ is somewhat special. Our theorem is not directly applicable to those. If there is at least one component $G_i$ of greater order, we may just add all the one-vertex components' vertices to $V^i_1$. If all the components are one-vertex, let us put one of them to $V_1$ and all the others to $V_2$. We still have $V_2  \neq \void$ then for $|V| \geq 2$.
	
	So, assume that $G$ is connected. For every $x, y \in G$, let $d(x,y)$ denote the length of a shortest path connecting $x$ to $y$ in $G$ (any such path is necessarily simple by Lemma~\ref{L14:simpl_path}). This quantity is well-defined since $G$ is connected; it is called the \emph{distance} between $x$ an $y$. Let us fix some vertex $z \in V$ and put $V_1 = \{ x \in V \mid d(z,x) \equiv 1 \pmod 2 \}$, $V_2 = \{ x \in V \mid d(z,x) \equiv 0 \pmod 2 \}$. Clearly, $V_1 \cup V_2 = V$ and $V_1 \cap V_2 = \void$. Let us check that each $V_i$ is non-empty. Indeed, $z$ is connected to $z$ by a zero-length path, hence $z \in V_2$. There must be a vertex $y$ which is adjacent to $z$ for $|V| \geq 2$ and $G$ is connected. Clearly, $d(z, y) = 1$ and $y \in V_1$. Now, we need to prove $\neg x E y$ when $x, y \in V_i$.
	
	For the contrary, assume that $x E y$ for certain $x, y \in V_i$, $x \neq y$. Consider some simple paths $z \pth{p} x$ and $z \pth{q} y$ of lengths $d(z,x)$ and $d(z,y)$, respectively. Clearly, $d(z,x) \equiv d(z,y) \pmod 2$. Let $w$ be the rightmost common vertex of these paths, so we have $z \pth{p'} w \pth{p''} x$ and $z \pth{q'} w \pth{q''} y$, where $p = \pth{p'} w \pth{p''}$ and $q = \pth{q'} w \pth{q''}$, while the paths $\pth{p''} x$ and $\pth{q''} y$ have no common vertex. We clam that $|z \pth{p'} w| = |z \pth{q'} w|$. Otherwise, w.\,l.\,o.\,g., $|z \pth{p'} w| < |z \pth{q'} w|$. Then the path $z \pth{p'} w \pth{q''} y$ is shorter than $z \pth{q} y$, which is supposedly a shortest path between $z$ and $y$. A contradiction.
	
	Then, we have
	$$
	|w \pth{p''} x| \equiv d(z,x) - |z \pth{p'} w| \equiv d(z,y) - |z \pth{q'} w| \equiv |w \pth{q''} y| \pmod 2.
	$$
	It is easy to see that $w \pth{p''} x y \pth{s''} w$, where $s'' = a_m \ldots a_1$ if $q'' = a_1 \ldots a_m$, is a simple cycle in $G$. The length of this cycle is congruent to $2 \cdot |w \pth{p''} x| + |x y| \equiv 0 + 1 \pmod 2$. But we have assumed no odd-length simple cycles in $G$. A contradiction.
\end{proof}

\begin{rem}\label{graph:part_algo}
	In fact, our proof for Theorem~\ref{L14:bipartite} suggests an algorithm to check whether a given graph is bipartite. First of all, we need to know distances between some of the vertices. Let us take an arbitrary vertex $z$ and label it with $0$. Clearly, $d(z,z) = 0$. Then we label each neighbor of $z$ with $1$. This comprises Step $1$. At Step $l + 1$, we label each yet unlabeled neighbor of every $l$-labeled vertex with the number $l + 1$. This procedure terminates when every labeled vertex has all its neighbors got labelled (which is inevitable). We claim that a vertex $x$ has label $l$ iff $d(z, x) = l$. By induction on $l$. The base case of $l = 0$ is evident. Assume our claim holds for all $l' < l$, $l > 0$, and consider an arbitrary vertex $x$. If $x$ is labelled by $l$, this has been done at Step $l$, that is, $x$ is a neighbor of a certain $y$ with label $l - 1$. By the IH, we have $d(z,y) = l - 1$ and $d(z, x)\geq l$ (otherwise, $x$ would have been labelled at an earlier step). As $y E x$, $d(z, x) \leq d(z, y) + 1$, hence $d(z, x) = l$. For the other direction, let $d(z,x) = l$. There is a path $z \ldots y x$ of length $l > 0$. Clearly, $d(z,y) = l - 1$. By the IH, $y$ must bear label $l - 1$ and $x$ must thus have been labeled with $l$ at Step $l$.
	
	Is it the case that every vertex of $G$ has a label on this procedure's termination? Not necessarily so for $G$ may be disconnected. But then we can take an arbitrary vertex $z'$ without a label (hence, from another connected component) and repeat similar steps. Then take a vertex $z''$ yet unlabeled (if any), etc. Clearly, these iterations shall terminate with no vertex unlabeled.
	
	From Theorem~\ref{L14:bipartite}, we know that it suffices to check each of the connected components (now indexed by $z, z', z'',\ldots$) for an odd-length cycle. Now, scan each component's edges for whether their respective ends bear labels of identical parity (that is, both even or both odd). If there exists any such edge $xy$, there is an odd-length cycle in $G$ as our proof shows, hence $G$ is not bipartite. Otherwise, the component we a looking at has a valid partition $(\{x \in V \mid d(z,x)\equiv 1 \pmod 2\}, \{x \in V \mid d(z,x)\equiv 0 \pmod 2\})$ (or is of order $1$). If no odd-length cycle is found in any component, the whole graph is bipartite; moreover, extracting a respective partition is easy and has been already discussed in the theorem's proof.
	
	Notice that labeling each vertex $x$ with just the \emph{remainder} after dividing $d(x,y)$ by $2$ suffices for our purpose.
\end{rem}
\begin{figure}[h]
	\centering
	\includegraphics*[width=0.8\textwidth]{graph_bipartite_algo.pdf}
	\caption{Applying the algorithm from Remark~\ref{graph:part_algo} to a graph $G$. This graph appears to have two connected components, of which just $[z']_{\sim_G}$ contains an odd-length cycle. This fact is witnessed by an edge whose ends bear labels of identical parity. Hence, $G$ is not bipartite, whereas its connected component $[z]_{\sim_G}$ is (the latter's vertex set may be partitioned into the `blue' and `red' parts).}
\end{figure}


\begin{exm}
	Every tree that has at least $2$ vertices is bipartite. Hence, \emph{every} tree is $2$-colorable, i.\,e., has a coloring in \emph{at most} $2$ colors.
	
	Let us compute how many ways to color a tree exist. By bipartiteness, there is at least one way to do so. As there are just two colors $0$ and $1$, we can \emph{invert} a coloring $c$ by putting $c'(x) = 1 - c(x)$ for each vertex $x$. Clearly, $c(x) = c(y)$ iff $c'(x) = c'(y)$, so $c'$ is a (proper) coloring as $c$ is. Hence, there are at least two colorings for any bipartite graph.
	
	Now, let us show that there are no more than two distinct colorings for a tree. It will be convenient to prove the following: \emph{every $(n,m)$-tree $T$ has exactly two coloring, each of which is the inversion of the other}---by induction on $m$.
	
	If $m = 0$, then $n = 1$ as $T$ is a tree. Clearly, there are just two colorings in this case. Assume that $m > 0$. Remove some edge $x y$ from $T$ to get a graph $T'$ with two connected components $T_1 = [x]_{\sim_{T'}}$ and $T_2 = [y]_{\sim_{T'}}$. Since these two are trees, by the IH, we have exactly two colorings $c_1$, $c'_1$ for $T_1$ and exactly two colorings $c_2$, $c'_2$ for $T_2$, where $c_1(x) = 0 = c_2(y)$ and $c'_1(x) = 1 = c'_2(y)$. On the other hand, every coloring $c$ of $T$ induces a pair of colorings for $T_1$ and for $T_2$ (distinct $c$'s yield distinct pairs). As $c(x) \neq c(y)$, these may be but $c_1$ and $c'_2$ or $c'_1$ and $c_2$, respectively. So, there are no more than two distinct colorings of $T$. As we have already noticed, at least two colorings for $T$ are guaranteed, each of which is the inversion of the other.
	
	Thus, we have exactly two distinct colorings for a tree. What about an arbitrary bipartite graph $G$? First, assume $G$ is connected. As we have already seen, there are at least two colorings of $G$. By Theorem~\ref{L14:t_spanning}, there exists a spanning tree $T$ in $G$. Every coloring of $G$ is a coloring of the tree $T$. By the above, there are no more than two such colorings.
	
	Finally, if a bipartite graph $G$ has $k$ connected components, one can color them independently, since there is no edge between two distinct components. Clearly, there are exactly $2^k$ distinct colorings of $G$ in this case.
\end{exm}

There is an interesting analogue of the Handshake Lemma for bipartite graphs.
\begin{lemma}
	For every bipartite $(n, m)$-graph $G = (V, E)$ with `parts' $V_1$ and $V_2$, the following holds:
	$$\sum_{v \in V_1} d_G(v) = \sum_{v \in V_2} d_G(v) = m.$$
\end{lemma}
\begin{proof}
	By induction on $m$. If $m = 0$, both sums equal $0 = m$. Assume that $m > 0$. Consider an edge $x_1 x_2$ of $G$.  W.\,l.\,o.\,g., we have $x_i \in V_i$. Let $G'$ be an $(n,m - 1)$-graph that one obtains on removing $x_1 x_2$ from $G$. Clearly, $$\sum_{v \in V_i} d_G(v) = \sum_{v \in V_i \setminus \{ x_i \} } d_G(v) + d_G(x_i) = \sum_{v \in V_i \setminus \{ x_i \} } d_{G'}(v) + d_{G'}(x_i) + 1 = \sum_{v \in V_i} d_{G'}(v) + 1$$
	for each $i \in \{1, 2\}$. By the IH, $\sum_{v \in V_i} d_{G'}(v) = m - 1$, whence $\sum_{v \in V_i} d_G(v) = m$.
\end{proof}

\begin{exm}
	Group A has $22$ members and Group B has $21$ members (the groups are disjoint). Each member of one group makes a handshake with some members of the other one (at most one per person). Everybody from Group A has made exactly $6$ handshakes. Is it possible that all the members of Group B have made an identical number of handshakes?
	
	Assume this is possible. An obvious model for this problem is the handshake graph $G = (V, E)$ where persons are treated as vertices, any two of which are adjacent iff they have made a handshake. This way, groups $A$ and $B$ form a partition of $V$ that makes $G$ bipartite. By our assumptions, $\sum_{v \in A} d_{G}(v) = 22\cdot 6$ and $\sum_{v \in B} d_{G}(v) = 21\cdot n$ for some $n$. By the above Lemma, $22 \cdot 6 = 21 \cdot n$, whence $7 \dvd 22 \cdot 6$, which is not the case. A contradiction.
\end{exm}

\paragraph{Digraphs.} Sometimes, the irreflexivity and symmetry assumptions we made in the definition of graph are too restrictive. In general, lifting them leads to arbitrary binary relations. In Section~\ref{sect:7}, we have seen binary relations depicted as \emph{diagrams}. Those diagrams consist of `vertices' and `arrows' and are much like graphs except that each link between two vertices has a `direction' (or `orientation') now. Two arrows between $x$ and $y$ are distinct iff they differ in their directions; `loops' from $x$ to $x$ are allowed. Still, we do not have multiple arrows from $x$ to $y$ (as the Reader remembers, an arrow from $x$ to $y$ in the diagram of a relation $R$ means that $(x,y) \in R$). Many ideas from the graph realm are fruitful in this more general case.

Formally, a \emph{directed graph} (or \emph{digraph}) is a pair $G = (V, E)$ where $V$ is a non-empty finite set (we keep this restriction) and $E \sbs V^2$ (i.\,e., $E$ is an arbitrary binary relation on $V$).

Let us transfer some graph notions to digraphs. We call $|V|$ the \emph{order} of a digraph $G = (V, E)$ and call $|E|$ the \emph{size} of $G$ (there is no need to identify `symmetric' pairs $(x,y)$ and $(y,x)$ when counting edges as those are directed). The neighborhood of a vertex $x$ is naturally split into two subsets $N^+(x) = \{ y \in V \mid x E y \} = E[\{ x\}]$ and $N^-(x) = \{ y \in V \mid y E x \} = E^{-1}[\{x\}]$. The number $d^+(x) = |N^+(x)|$ is called the \emph{outdegree} of the vertex $x$, while $d^-(x) = |N^-(x)|$ is the \emph{indegree} of $x$.

\begin{lemma}
	For every digraph $G = (V, E)$,
	$$\sum_{v \in V} d^+_G(v) = \sum_{v \in V} d^-_G(v) = |E|.$$  
\end{lemma}
This basically means that each edge has just one starting point (or `source') and just one ending point (or `target'). A formal proof (most naturally, by induction on $|E|$) is left to the reader.

%We say that a digraph $G = (V, E)$ is a \emph{tournament} if 

\section{Boolean Functions and Circuits}
\mcomm{The two following sections treat mostly traditional questions of closed sets (clones), their bases and Post's functional completeness theorem. In our view, the most complicated issue here is to define what expressing one function via some others means. Two popular approaches are: (1) to define `formulas' or `terms' and their values, or (2) to use ${\sbs}$-least clones. We have found both of them unsatisfactory for our audience. First, the students tend to mix functions and formulas since their intuition of functions is still heavily influenced by high-school `functions-as-algebraic-expressions' (despite all our set-theoretical efforts). Second, applying clones requires some machinery for structural induction proofs. The latter is far from obvious to the students and is not very intuitive without a direct reduction to induction on naturals (on `formula size', `construction length', etc.). In this Course, we do not however target teaching a general inductive definition formalism (a modicum of which is required anyway) to  our students.
	\medskip\\
	With this in view, we have taken a compromise approach. Instead of formulas, we use circuits. It is much harder to identify a circuit (something unfamiliar and fancy) with a function. In particular, we emphasize that circuits comprise a ``programming language''; most students know well that two distinct programs may compute identical functions. (The last but not the least, circuit formalism is an important prerequisite for many Computer Science courses.) Structural induction may be then reduced to that on circuit size.
	\medskip\\
	Nevertheless, we omit some formalities like discerning a variable from its value. And we make a natural concession to `formulas' when discussing DNF/CNF and Zhegalkin polynomials.
}

In Section~\ref{sect:1}, we saw \emph{compound statements} like \emph{$2 = 3$ and $4 < 5$}, whose truth value \emph{depends} on the truth values of their parts. So, a statement of the form \emph{$A$ and $B$}, built with the logical connective \emph{and}, is true iff both $A$ and $B$ are true. On the other hand, we can now express diverse dependencies formally as \emph{functions}. Studying logical connectives via functions is interesting from the algebraic point of view and is very important for both the computer science and practical computing (the latter stems from the fact that all objects computers work with are routinely encoded by binary words).

Let us recall the truth tables for popular logical connectives:
\begin{center}
	\begin{tabular}{| c c | c | c | c | c | c |}
		\hline
		$A$ & $B$ & not $A$  &$A$ and $B$&$A$ or $B$&if $A$ then $B$ &$A$ if and only if $B$\\
		\hline
		$0$&$0$&$1$&$0$&$0$&$1$&$1$\\
		$0$&$1$&$1$&$0$&$1$&$1$&$0$\\
		$1$&$0$&$0$&$0$&$1$&$0$&$0$\\
		$1$&$1$&$0$&$1$&$1$&$1$&$1$\\
		\hline
	\end{tabular}
\end{center}
As usual, $1$ stands for \emph{true} and $0$ stands for \emph{false}. We will use this table to formally define a few functions from $\ul{2}^2$ and from $\ul{2}$ to $\ul{2}$. We will denote these \emph{Boolean} functions by the symbols we used for logical connectives (and use the same names for both kinds of objects), while it is crucial to see the difference: a (binary) \emph{connective} (if seen as a function) takes two \emph{statements} and returns a \emph{statement}, whereas a (binary) \emph{Boolean function} takes two \emph{truth values} (i.\,e., elements of $\ul{2}$) and returns a \emph{truth value}.\footnote{It is generally unwise to identify statements with their truth values as Example~\ref{L1:exm_pi} shows: both the statements $0 = 0$ and $\pi < 3.14159265358979323847$ are true but proving this may require a good deal of work.} 
\begin{center}
	\begin{tabular}{| c c | c | c | c | c | c | c |}
		\hline
		$x$ & $y$ & $\neg x$  &$x \wedge y$&$x \vee y$&$x \to y$ &$x \leftrightarrow y$&$x + y$\\
		\hline
		$0$&$0$&$1$&$0$&$0$&$1$&$1$&$0$\\
		$0$&$1$&$1$&$0$&$1$&$1$&$0$&$1$\\
		$1$&$0$&$0$&$0$&$1$&$0$&$0$&$1$\\
		$1$&$1$&$0$&$1$&$1$&$1$&$1$&$0$\\
		\hline
	\end{tabular}
\end{center}
Now, it makes perfect sense to state that $x = x \wedge x$ for each $x \in \ul{2}$ etc., since we mean functions and their values.

Formally, every function $f\colon \ul{2}^n \to \ul{2}$, where $n \in \N$, is called a \emph{Boolean function of $n$ arguments}. How many such functions exist? As a degenerate case, for $n = 0$ one has $\ul{2}^0 = \{ \void \}$, so the sets $\{ (\void, 0) \}$ and $\{ (\void, 1) \}$ are the only Boolean functions of $0$ arguments. For technical reasons, we will ignore these functions and assume $n > 0$ in what follows.

In general, there are $|\ul{2}|^{|\ul{2}^n|} = 2^{2^n}$ distinct Boolean functions of $n$ arguments by Corollary~\ref{L10:num_func}. Let us denote by $\top$ the set of all Boolean functions of one or more arguments.

\begin{exc}
	Build a truth table for every Boolean function of one and of two arguments.
\end{exc}

\noindent The following useful equations can be easily checked by by truth tables for each function involved.
\begin{lemma}\label{L15:bool_eq1}
	For every $x,y,z \in \ul{2}$,
	\begin{enumerate}
		\item $x \wedge y = y \wedge x$; $x \vee y = y \vee x$; $x + y = y + x$;
		\item $(x \wedge y) \wedge z = x \wedge (y \wedge z)$; $(x \vee y) \vee z = x \vee (y \vee z)$; $(x + y) + z = x + (y + z)$;
		\item $x \wedge x = x$; $x \vee x = x$; $x + x = 0$;
		\item $x \wedge (x \vee y) = x$; $x \vee (x \wedge y) = x$;
		\item $\neg\neg x = x$;
		\item $x \wedge (y \vee z) = (x \wedge y) \vee (x\wedge z)$; $x \vee (y \wedge z) = (x\vee y) \wedge (x \vee z)$; $x \wedge (y + z) = (x\wedge y) + (x \wedge z)$;
		\item $\neg(x\wedge y) = \neg x \vee \neg y$; $\neg(x \vee y) = \neg x \wedge \neg y$;
		\item $x \wedge 0 = 0$; $x \wedge 1 = x$; $x\vee 0 = x$; $x \vee 1 = 1$; $x + 0 = x$; $x + 1 = \neg x$; $\neg 0 = 1$; $\neg 1 = 0$;
		\item $x \wedge \neg x = 0$; $x \vee \neg x = 1$;
		\item $x \to y = \neg x \vee y$; $\neg(x \to y) = x \wedge \neg y$; $0 \to x = 1$; $1 \to x = x$; $x \to 0 = \neg x$; $x \to 1 = 1$;
		\item $x \leftrightarrow y = (x \to y) \wedge (y \to x)$; $x + y = \neg (x \leftrightarrow y)$.
	\end{enumerate}
\end{lemma}

\begin{rem}\label{bool:ord_field}
	The function ${+}$ returns the remainder after dividing its arguments' sum by $2$. (The respective logical connective is \emph{exclusive or} (xor): \emph{either $A$ or $B$ but not both}.)
	It is also clear that the function ${\wedge}$ returns the product of its arguments (taking remainder afterwards is logical but does not change the result). We will sometimes denote conjunction by ${\cdot}$ for this reason. From Lemma~\ref{L15:bool_eq1}, one can see that the functions ${+}$ and ${\cdot}$ satisfy many properties of real number addition and multiplication respectively. E.\,g., for every $x$, there exist a number $y$ with $x + y = 0$ (take $y = x$) and, if $x\neq 0$, a number $z$ with $x \cdot z = 1$ (take $z = x$). In general, these functions over the set $\ul{2}$ comprise a \emph{field}. This particular field of two elements is denoted by $\mathrm{GF}(2)$ (\emph{Galois field} of two elements). This fact makes many results for real, rational, or complex numbers (which form fields in their turn) valid for the arithmetic of Boolean values $0$ and $1$ as well. In particular, basic results concerning systems of linear equations are still true.
	
	Another interpretation of Boolean functions stems from the natural ordering of $\ul{2}$ where $0 < 1$. W.\,r.\,t.\ this ordering, $x \wedge y = \inf \{x, y\}$ and $x \vee y = \sup \{x, y\}$ (in fact, $x \wedge y$ and $x \vee y$ are the least and the greatest elements of $\{x,y\}$, respectively), while $x \to y$ returns the truth value of the predicate ${\leq}$ itself: $x \to y = 1$ iff $x \leq y$.
\end{rem}


\paragraph{Boolean circuits.} As Lemma~\ref{L15:bool_eq1} suggests, one can express (or define) implication in terms of disjunction and negation: $x \to y = \neg x \vee y$. A natural question arises: when is one function  definable via some others? This question is of practical importance since we might want to compute a multitude of Boolean functions on hardware  capable of computing just some of them directly.

The first step is to rectify what `to define a function via others' means. To this end, we introduce a simple ``programming language'' where a function is `computed' via a few `primitive' functions. The most important operator of that language is a primitive function application with assigning the result to a variable. The program 
$$
\begin{array}{rcl}
	t_1 &=& \neg x\\
	t_2 &=& t_1 \vee y
\end{array}
$$
is intended to compute $x \to y$ using just $\neg$ and $\vee$ as primitive functions. Here $x$ and $y$ are considered input variables, whereas $t_1$ and $t_2$ are `local' or `temporary' variables intended to store the results of assignments. The last of them ($t_2$) is interpreted as the value our program `returns'. Notice the main idea: no more than one primitive function is applied at each step.\footnote{That is, each function must be applied to variables solely. Such a constraint is known as \emph{A-normal form}.} This is to make our program easy for analysis. And, of course, no temporary variable may be used before assigning some value to it!

In the assignment $t_2 = t_1 \vee y$, we have mixed input and temporary variables, which is not convenient technically. Otherwise, we might assign $x$ and $y$ to new temporary variables first and apply primitive functions to those exclusively:
$$
\begin{array}{rcl}
	t_1 &=& x\\
	t_2 &=& \neg t_1\\
	t_3 &=& y\\
	t_4 &=& t_2 \vee t_3.
\end{array}
$$
For technical reasons again, we want to allow assignments of one temporary variable to another one as well, like $t_i = t_j$, where $j < i$, of course.

Let us give a formal definition for our programs, which are known as \emph{Boolean circuits}. Let $P$ be as set of Boolean functions. We write $f^{(k)}$ for a Boolean function $f$ of $k$ arguments. Let $\vec x = (x_1, \ldots, x_n)$ be a tuple of pairwise distinct \emph{input variables}\footnote{Formally, these variables may be whatever objects (sets), but a natural intuition of `letters' as variables is adequate.}. A \emph{(Boolean) circuit $C$ over $P$ of $\vec x$} is a non-empty finite sequence (that is, a tuple, up to bijection) of assignments:
$$
\begin{array}{rcl}
	t_1 &=& R_1\\
	t_2 &=& R_2\\
	&\ldots&\\
	t_m &=& R_m,
\end{array}
$$
where the right-hand side $R_i$ of each assignment $t_i = R_i$ is either (a) an input variable $x_j$ or (b) a temporary variable $t_{i_1}$ with $i_1 < i$ or (c) an expression $f(t_{i_1},\ldots, t_{i_k})$ where $f^{(k)} \in P$ and $i_1,\ldots, i_k < i$. The variables $t_1,\ldots,t_m$ are assumed to be pairwise distinct. The number $m$ (that of assignments in $C$) is called the \emph{size} of the circuit $C$.

The Pedantic Reader might ask what an `assignment' and an `expression' mean. It is not hard to make these notions formal if one interprets $t_i  = R_i$ as the pair $(t_i, R_i)$ and $f(t_{i_1},\ldots, t_{i_k})$ as the pair $(f, (t_{i_1},\ldots, t_{i_k}))$. But the intended meaning of Boolean circuits renders such formalities unnecessary.

Indeed, we want to put a Boolean function into correspondence with a circuit---the function the circuit `computes'. We do it recursively, that is, assuming that a function already corresponds to every circuit `simpler' than the given one. The Reader might want to revise Section~\ref{sect:strings} for diverse recursive definitions, yet here we can use size as the measure of `simplicity' to make things easier. A formal theory of recursive definitions is still beyond the scope of this Course.

At first, we notice that for every circuit $C$ over $P$ of $\vec x$, its every prefix $C_i$:
$$
\begin{array}{rcl}
	t_1 &=& R_1\\
	t_2 &=& R_2\\
	&\ldots&\\
	t_i &=& R_i,
\end{array}
$$
where $i < m$, is again a circuit over $P$ of $\vec x$---yet of the smaller size $i$. Let $C_m = C$ as well.

Consider a circuit $C$ over $P$ of $\vec x$ and assume that for each circuit $C'$ over $P$ of $\vec x$ of any size $m' < m$, we have already defined a corresponding function $g_{C'} \colon \ul{2}^n \to \ul{2}$. Let us define a function $g_C \colon \ul{2}^n \to \ul{2}$ for $C$. Consider the last assignment $t_m = R_m$ in $C$.
\begin{itemize}
	\item If $R_m = x_j$, put\footnote{Here we have identified `variables' $(x_1, \ldots, x_n)$ with their possible values from $\ul{2}$. We are going to do so in what follows without any further comment.} $g_C(\vec x) = x_j$ for all $\vec x \in \ul{2}^n$. In other words, $g_C$ is a projector function in this case.
	\item If $R_m = t_{i_1}$, put $g_C = g_{C_{i_1}}$, where the latter function is already defined by assumption since the size $i_1$ of the prefix circuit $C_{i_1}$ is less than $m$.
	\item If $R_m = f(t_{i_1},\ldots, t_{i_k})$, put $g_C(\vec x) = f(g_{C_{i_1}}(\vec x),\ldots, g_{C_{i_k}}(\vec x))$ for all $\vec x \in \ul{2}^n$, which is well-defined as $i_1, \ldots, i_k < m$.
\end{itemize}
We have thus defined $g_C$ for every circuit $C$ over $P$ of $\vec x$ by recursion on its size. We may say that the circuit $C$ \emph{computes} the function $g_C$. 

\begin{exm}\label{bool:xor_circuits}
	Consider the following circuit $C$ over $\{ {\neg}, {\vee}, {\wedge} \}$ of $(x_1, x_2)$:
	$$
	\begin{array}{rclrclclcl}
		t_1 &=& x_1,\qquad &g_1(x_1, x_2) &=& x_1,\\
		t_2 &=& x_2,\qquad &g_2(x_1, x_2) &=& x_2,\\
		t_3 &=& \neg t_1,\qquad &g_3(x_1, x_2) &=&  \neg g_1(x_1, x_2) &=&  \neg x_1,\\
		t_4 &=& \neg t_2,\qquad &g_4(x_1, x_2) &=&  \neg g_2(x_1, x_2) &=&  \neg x_2,\\
		t_5 &=& t_1 \wedge t_4,\qquad &g_5(x_1, x_2) &=&  g_1(x_1, x_2) \wedge g_4(x_1, x_2) &=&  x_1 \wedge \neg x_2,\\
		t_6 &=& t_3 \wedge t_2,\qquad &g_6(x_1, x_2) &=&  g_3(x_1, x_2) \wedge g_2(x_1, x_2) &=&  \neg x_1 \wedge x_2,\\
		t_7 &=& t_5 \vee t_6,\qquad &g_7(x_1, x_2) &=&  g_5(x_1, x_2) \vee g_6(x_1, x_2) &=& (x_1 \wedge \neg x_2) \vee (\neg x_1 \wedge x_2) &=& x_1 + x_2.
	\end{array}
	$$
	At the end of each $i$-th line, we have placed an equation for the function $g_i = g_{C_i}$ which the respective prefix circuit $C_i$ computes. Clearly, $g_C = g_7 = {+}$.
	
	The circuit $C$ for ${+}$ has size $7$. It is however possible to compute ${+}$ over the same set $\{ {\neg}, {\vee}, {\wedge} \}$ of primitive functions in a more economical way. Indeed, consider another circuit $D$:
	$$
	\begin{array}{rclrclcl}
		t_1 &=& x_1,\qquad &g_1(x_1, x_2) &=& x_1,\\
		t_2 &=& x_2,\qquad &g_2(x_1, x_2) &=& x_2,\\
		t_3 &=& t_1 \vee t_2,\qquad &g_3(x_1, x_2) &=&  x_1 \vee x_2,\\
		t_4 &=& t_1 \wedge t_2,\qquad &g_4(x_1, x_2) &=& x_1 \wedge x_2,\\
		t_5 &=& \neg t_4,\qquad &g_5(x_1, x_2) &=&  \neg (x_1 \wedge x_2),\\
		t_6 &=& t_3 \wedge t_5,\qquad &g_6(x_1, x_2) &=&  (x_1 \vee x_2) \wedge \neg (x_1 \wedge x_2) &=& x_1 + x_2.\\
	\end{array}
	$$
	We still have $g_D = {+}$ but $\mathrm{size}(D) = 6 < \mathrm{size}(C)$.
\end{exm}

Another important measure of circuit complexity is \emph{depth}. Unlike size, one needs recursion (on size) in order to define it. Consider a circuit $C$ over $P$ of $\vec x$ and assume that for each circuit $C'$ over $P$ of $\vec x$ of any size $m' < m$, we have already defined a number $\mathrm{depth}(C') \in \N$. Consider the last assignment $t_m = R_m$ in $C$.
\begin{itemize}
	\item If $R_m = x_j$, put $\mathrm{depth}(C) = 1$.
	\item If $R_m = t_{i_1}$, put $\mathrm{depth}(C) = 1 + \mathrm{depth}(C_{i_1})$.
	\item If $R_m = f(t_{i_1},\ldots, t_{i_k})$, put $\mathrm{depth}(C) = 1 + \sup_{<}\, \{ \mathrm{depth}(C_{i_1}), \ldots, \mathrm{depth}(C_{i_k}) \}$.
\end{itemize}

\begin{exm}
	Let us compute the depths of circuits $C$ and $D$ from Example~\ref{bool:xor_circuits}. We obtain:
	$$
	\begin{array}{rclrclcl}
		t_1 &=& x_1,\qquad &\mathrm{depth}(C_{1}) &=& 1,\\
		t_2 &=& x_2,\qquad &\mathrm{depth}(C_{2}) &=& 1,\\
		t_3 &=& \neg t_1,\qquad &\mathrm{depth}(C_{3}) &=&  1 + \sup\, \{1\} &=& 2,\\
		t_4 &=& \neg t_2,\qquad &\mathrm{depth}(C_{4}) &=&  1 + \sup\, \{1\} &=& 2,\\
		t_5 &=& t_1 \wedge t_4,\qquad &\mathrm{depth}(C_{5}) &=&  1 + \sup\, \{1, 2\} &=& 3,\\
		t_6 &=& t_3 \wedge t_2,\qquad &\mathrm{depth}(C_{6}) &=&  1 + \sup\, \{1, 2\} &=& 3,\\
		t_7 &=& t_5 \vee t_6,\qquad &\mathrm{depth}(C_{7}) &=&  1 + \sup\, \{3, 3\} &=& 4,
	\end{array}
	$$
	so $\mathrm{depth}(C) = 4$, and
	$$
	\begin{array}{rclrclcl}
		t_1 &=& x_1,\qquad &\mathrm{depth}(D_{1}) &=& 1,\\
		t_2 &=& x_2,\qquad &\mathrm{depth}(D_{2}) &=& 1,\\
		t_3 &=& t_1 \vee t_2,\qquad &\mathrm{depth}(D_3) &=&  1 + \sup\, \{1, 1\} &=& 2,\\
		t_4 &=& t_1 \wedge t_2,\qquad &\mathrm{depth}(D_4) &=&  1 + \sup\, \{1, 1\} &=& 2,\\
		t_5 &=&\neg t_4,\qquad &\mathrm{depth}(D_5) &=&  1 + \sup\, \{2\} &=& 3,\\
		t_6 &=& t_3 \wedge t_5,\qquad &\mathrm{depth}(D_6) &=&  1 + \sup\, \{2, 3\} &=& 4,\\
	\end{array}
	$$
	whence $\mathrm{depth}(D) = 4 = \mathrm{depth}(C)$.
\end{exm}

There is another representation for Boolean circuits in terms of digraphs. This is especially vivid and useful when every function $f^{(k)}$ in $P$ is \emph{symmetric}, that is, $f(x_{\sigma(1)}, \ldots, x_{\sigma(k)}) = f(x_1, \ldots, x_k)$ for every permutation $\sigma$ of the set $\{1,\ldots, k\}$ and every $\vec x \in \ul{2}^k$. For example, $\wedge$ is symmetric as $x_1 \wedge x_2 = x_2 \wedge x_1$ always holds, while $\to$ is not symmetric since $0 \to 1 \neq 1 \to 0$.

We will not give any formal definition for this graphical representation but be content with analyzing the two circuits from Example~\ref{bool:xor_circuits}.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.65\textwidth]{circuits_exm.pdf}
	\caption{The circuits (a) $C$ and (b) $D$ from Example~\ref{bool:xor_circuits}. For each node (or prefix circuit), its depth is shown in red. Clearly, the depth is the length of a \emph{longest} path from an input to that node. In general, labeling nodes with temporary variables is optional.}
\end{figure}

This graphical representation may be also seen as a scheme of a real electric circuit, as a logical primitive of today's sophisticated electronics. Assume that each arrow represents a conductor and at each input and each node's output two states are discernible: `current' (identified with $1 \in \ul{2}$) or `no current' (identified with $0$). Assume that every primitive function node is a device cleverly engineered to transform its input states according to the respective function (a \emph{logic gate}). Say, a negation gate outputs `current' iff there is `no current' at its input, thus doing a rudimentary computation. Assume we have `current' at some of the inputs according to a tuple $\vec x \in \ul{2}^n$. Then there is `current' at the circuit's $C$ output iff $g_C(\vec x) = 1$.

As circuits comprise a ``programming language'', one can easily see that size and depth correspond to the well-known program performance measures: \emph{space} and \emph{time}, respectively. The first of these analogies seems quite clear. For the second one, imagine that each logic gate requires some time $T$ to do its current-transforming job (as a matter of fact, there are such delays in real-world devices). How long does one have to wait until the correct state of the output is established? Clearly, this time is determined by the longest path for the current to flow from the inputs to the output and can be estimated as $T \cdot \mathrm{depth}(C)$.

\paragraph{Programming with circuits.} In many programming languages, one can call a subroutine (also know as `function', `procedure' or `method') from a program to do some specific task. Typically, each subroutine has its own local variables which are not visible from the calling program nor other subroutines. This way, one can write a subroutine independently of the caller and reuse it freely; in particular, one can safely reuse variables in different subroutines.

Yet our circuit formalism lacks anything like that. What can we do then? Assume that a circuit $C$ of $(x_1, \ldots, x_n)$ is given:
$$
\begin{array}{rcl}
	t'_1 &=& R_1\\
	&\ldots&\\
	t'_m &=& R_m,\\
\end{array}
$$
and we want to `call' it from another circuit $D$ in order to apply the function $g_C$ to some temporary variables in $D$:
$$t = g_C(t_1, \ldots t_n).$$
Unless $g_C$ is a primitive function, we have to somehow insert $C$ into $D$:
$$
\begin{array}{rcl}
	&\ldots&\\
	t'_1 &=& R_1\\
	&\ldots&\\
	t'_m &=& R_m\\
	t &=& t'_m\\
	&\ldots&\\
\end{array}
$$
Two problems arise here. First, the temporary variables $t'_i$ may occur elsewhere in $D$, which makes this sequence of assignments formally incorrect (what if, say, $t$ is the same variable as $t'_1$?). Second, the terms $R_i$ depend on $x_1, \ldots, x_n$ as inputs whereas we want to feed $t_1, \ldots, t_n$ to $C$ in their stead.

Clearly, both these problems are purely formal. It is well possible to solve them in a perfectly formal manner, but this is too tedious. We shall describe a solution informally.

For the first problem, one can replace all occurrences of $t'_i$ in $C$ with a new variable $t''_i$ which is `fresh', i.\,e., has no occurrence in $D$ (and surely $t''_i$ differs from $t''_j$ when $i \neq j$). For the second problem, we replace all occurrences of $x_j$ in the terms $R_1, \ldots, R_m$ with $t_j$. (Notice that an assignment $t'_i = x_j$ will be replaced by $t''_i = t_j$. In the definition of circuit, we have provisioned an assignment with a temporary variable as its right-hand side for this very reason.) In what follows, we shall hide all these formalities behind assignments like
$$t = C(t_1, \ldots, t_n).$$

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{circuits_subst.pdf}
	\caption{Everything looks simpler in this diagram of the `caller' circuit $D$. The `subroutine' circuit $C$ is depicted here as a `black box'. Nodes $t_1, \ldots, t_n$ are connected to its respective inputs $x_1, \ldots, x_n$. The output of $C$ is connected to $t$ for any subsequent usage.}
\end{figure}

Now, let us do some practical programming with circuits. With this in view, we shall generalize circuit computations slightly. Since every prefix circuit computes a function, it is possible to label more temporary variables as outputs (not just the last one)---this way, a circuit may compute a tuple of Boolean functions or equivalently, a function of the form $\ul{2}^n \to \ul{2}^m$ (cf.~the identity $A^C \times B^C \sim (A \times B)^C$ from Theorem~\ref{ch0:bi_power}).

Let us build a circuit for adding two one-digit binary numbers. A binary (notation for a) natural number is a tuple of zeroes and ones, but adding two one-digit numbers may result in a two-digit number for $add(1, 1) = 10$. Assuming $0 = 00$ and $1 = 01$ (that is, introducing a `leading zero'), one can always consider the result as a two-digit number. So, we may let $add\colon \ul{2}^1 \times \ul{2}^1 \to \ul{2}^2$ or $add\colon \ul{2}^2 \to \ul{2}^2$, and $add(x, y) = z_2 z_1$ for $x, y, z_i \in \ul{2}$.

Our circuit $C_{add}$ shall thus have two inputs $x, y$ and two outputs $z_1, z_2$. It is convenient to make it a circuit over the set $\{{+}, {\wedge}\}$. Here it is:
$$
\begin{array}{rcl}
	t_1 &=& x\\
	t_2 &=& y\\
	z_1 &=& t_1 + t_2\\
	z_2 &=& t_1 \wedge t_2.\\
\end{array}
$$

Now, we will implement addition for two-digit (or longer) numbers. The main idea is to modify the circuit $C_{add}$ slightly: we may rightly call the bit $z_2$ the \emph{overflow} for it is $1$ iff one digit is not enough to represent the sum. When adding the \emph{second} bits of two-digit numbers, we must take the overflow resulting from the first bits into account; on the other hand, we may produce a new overflow. So, the modified bit addition $add_1$ is a function from $\ul{2}^3$ to $\ul{2}^2$ such that, say, $add_1(1, 1, 1) = 11$. The respective \emph{bit adder} circuit $A$ will take three inputs $x$, $y$, and $o$, where $o$ is the incoming overflow, and produce still two outputs $z_2$ (the outgoing overflow) and $z_1$:
$$
\begin{array}{rcl}
	t_1 &=& x\\
	t_2 &=& y\\
	t_3 &=& o\\
	z_2 &=& \mathrm{maj}(t_1, t_2, t_3)\\
	t_4 &=& t_1 + t_2\\
	z_1 &=& t_4 + t_3.\\
\end{array}
$$
For the sake of clarity, we have constructed a circuit over the set $\{{+}, {\mathrm{maj}} \}$, where the important \emph{majority function}  $\mathrm{maj}$ returns $1$ iff there are more `$1$'s than `$0$'s among its argument values, e.\,g., $\mathrm{maj}(1,0,1) = 1$ but $\mathrm{maj}(0,1,0) = 0$. Indeed, we have an overflow iff there are at least two input `$1$'s. We shall see later that $\mathrm{maj}(x,y,z) = (x \wedge y) + (x \wedge z) + (y \wedge z)$ and the bit adder can be easily built over the set $\{{+}, {\wedge}\}$ as well.

Given the bit adder $A$, we can construct adders $Add_n$ for $n$-digit numbers \emph{recursively}. For $n = 1$, we just have to fix $o = 0$ obtaining the circuit $Add_1$ of $(x_1, y_1)$:
$$
\begin{array}{rcl}
	t_1 &=& x_1\\
	t_2 &=& x_2\\
	o &=& t_2 + t_2\\
	(z_2, z_1) &=& A(t_1, t_2, o).\\
\end{array}
$$
The last line is similar to `calling' one circuit from another which we have already discussed. Here we have but two outputs of $A$ assigned to $z_2$ and $z_1$.

Assume that a circuit $Add_n$ of $(x_1,\ldots, x_n, y_1, \ldots, y_n)$ for adding two $n$-digit numbers $x_n \ldots x_1$ and $y_n \ldots y_1$ with outputs $(z_{n+1}, z_n, \ldots, z_1)$ has been already built ($z_{n+1}$ is the outgoing overflow or, equivalently, the most significant bit of the result). One can then obtain $Add_{n+1}$ this way:
$$
\begin{array}{rcl}
	t_1 &=& x_1\\
	s_1 &=& y_1\\
	&\ldots&\\
	t_n &=& x_n\\
	s_n &=& y_n\\
	(o, z_n, \ldots, z_1) &=& Add_n(t_1, \ldots, t_n, s_1, \ldots, s_n)\\
	t &=& x_{n+1}\\
	s &=& y_{n+1}\\
	(z_{n+2}, z_{n+1}) &=& A(t, s, o),\\
\end{array}
$$
where $(z_{n+2}, z_{n+1}, z_n, \ldots, z_1)$ are outputs.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.75\textwidth]{circuits_adder.pdf}
	\caption{Constructing adders (a) $Add_1$ and (b) $Add_{n+1}$ recursively.}
\end{figure}


This construction's recursive structure allows simple inductive proofs for diverse properties of the circuit. In particular, one can easily prove that $Add_n$ indeed computes the sum of two binary numbers by induction on $n$.

\paragraph{Closures.} Now, we can make it precise how one function may be expressed via some others. Let $Q$ be a set of Boolean functions. The \emph{closure} of $Q$ is the set
$$[Q] = \{ f \in \top \mid f = g_C\ \mbox{for some circuit $C$ over $Q$}\}.$$
The set $[Q]$ thus consists of all such functions that one can compute with a circuit over $Q$. From Example~\ref{bool:xor_circuits}, it follows that ${+} \in [\{ {\neg}, {\vee}, {\wedge} \}]$. To make our notation prettier, we will write $[f_1, \ldots, f_n]$ instead of $[\{ f_1, \ldots, f_n \}]$, so ${+} \in [ {\neg}, {\vee}, {\wedge} ]$.

\begin{exm}
	We know that ${\to} \in [{\neg}, {\vee}]$, but we have ${\to} \in [{\neg}, {\wedge}]$ as well, since $x \to y = \neg (x \wedge \neg y)$ (it is easy to make a suitable circuit at this point; we shall skip further explanations in similar cases).
	
	On the other hand, $x \wedge y = \neg (\neg x \vee \neg y)$ and $x \vee y = \neg (\neg x \wedge \neg y)$. Does not that mean we can always `emulate' $\wedge$ using just $\vee$ (and vice versa) if $\neg$ is also available? As we shall see, it does. In particular, we shall have $[{\neg}, {\wedge}] = [{\neg}, {\vee}]$.
	
	Let us consider extreme cases. Clearly, $[\top] = \top$, for $[\top] \sbs \top$ by definition, and every function $f^{(n)}$ can be computed via itself with a trivial circuit:
	\begin{equation}\label{bool:triv_circ}
		\begin{array}{rcl}
			t_1 &=& x_1\\
			t_2 &=& x_2\\
			&\ldots&\\
			t_n &=& x_n\\
			t_{n+1} &=& f(t_1, \ldots, t_n).
		\end{array}
	\end{equation}
	But what is $[\void]$? Can we compute a function using no primitives at all? Revisiting the definition, we see that every circuit $C$ over $\void$ has only variables in its assignments' right-hand sides. If the last assignment is of the form $t_i = x_j$, we have $g_C(\vec x) = x_j$ by definition, that is, $g_C$ is a \emph{projector function}, which returns one of its inputs. If the last assignment is $t_i = t_{i_1}$ with $i_1 < i$, we need induction on size to prove that $g_C$ is still a projector. Indeed, $g_{C_{i_1}}$ is a projector by the IH, while $g_C = g_{C_{i_1}}$ by definition. On the other hand, every projector is easily computable via a trivial circuit of the form $t_1 = x_j$.
	
	Thus, $[\void]$ equals the set of projectors $\bot = \{ f\in \top \mid \exists j \forall \vec x\, f(\vec x)  = x_j \}$.
\end{exm}

Now, we are going to establish a few important properties of the closure operation. In order to prove them and many other interesting statements alike, we will apply the following `structural induction' method.
\begin{lemma}[Structural induction for Boolean functions]\label{bool:struct}
	Let $Q$ be as set of Boolean functions and $\phi$ be a unary predicate over Boolean functions. Suppose all of the following hold:
	\begin{itemize}
		\item for each $f \in Q$, $\phi(f)$;
		\item for each projector function $p \in \bot$, $\phi(p)$;
		\item if $\phi(h), \phi(g_1), \ldots, \phi(g_m)$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$, then $\phi(f)$.
	\end{itemize}
	Then we have $\phi(f)$ for every $f \in [Q]$.
\end{lemma}
This statement asserts some properties that each function from $Q$ enjoys to hold for all functions in $[Q]$ as well. Such a property must be ``nice enough'', i.\,e., it must hold for projectors and respect the \emph{superposition} (sometimes called `composition') operation: when $f$ is obtained from some $h, g_1, \ldots, g_m$ by the equation $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$. The lemma does not explicitly mention circuits but its proof shall in fact mimic the way we define the function $g_C$ for a circuit $C$. (It is very instructive to try to find a superposition operation there.)
\begin{proof}
	For every function $f \in [Q]$, the exists a circuit over $Q$ such that $f = g_C$. There may be many such circuits, but their \emph{sizes} form a non-empty set of naturals. By the Least Number Principle, there is the \emph{least} such size (and a respective circuit). We shall call this number the \emph{size} of $f$ (w.\,r.\,t.\ $Q$) and denote it by $\sz(f)$.
	
	We will prove $\phi(f)$ by induction on $\sz(f)$. That is, we shall assume as the IH that for each function $f' \in [Q]$ with $\sz(f') < \sz(f)$ it holds that $\phi(f')$, and try to derive $\phi(f)$ therefrom.
	
	Let $s = \sz(f)$ and let $C$ be a circuit over $Q$ computing $f$. What is the last assignment in $C$? If it is of the form $t_s = x_j$, then $f = g_C$ is a projector, whence $\phi(f)$ by assumption. If it is of the form $t_s = t_i$ with $i < s$, we have $f = g_C = g_{C_i}$. The function $g_{C_i}$ is computed by the prefix circuit $C_i$, where $\sz(C_i) < s$.  The circuit $C_i$ is not necessarily a shortest among those computing $g_{C_i}$, but surely $\sz(g_{C_i}) \leq \sz(C_i) < s$. By the IH, we get $\phi(g_{C_i})$ and $\phi(f)$.
	
	The only remaining case is $t_s = h(t_{i_1},\ldots, t_{i_k})$, where $h \in Q$ and $i_1, \ldots, i_k < s$. Then $f(\vec x) = g_C(\vec x) = h(g_{C_{i_1}}(\vec x), \ldots, g_{C_{i_k}}(\vec x))$ for all $\vec x$. As in the above, we have $\phi(g_{C_{i_1}}), \ldots, \phi(g_{C_{i_k}})$ by the IH. From the assumptions, we obtain $\phi(h)$ and $\phi(f)$ finally.
\end{proof}
\mcomm{This lemma reduces the inductive definition for clones to our chosen function `representation', be it a circuit, a formula or whatever. It states that $[Q]$ (as defined in representation terms) is included into every clone containing $Q$. On the other hand, the following theorem implies that $[Q]$ is indeed a clone (that is, $\bot \sbs Q$ and $[Q]$ is closed under superposition) and contains $Q$. So, $[Q]$ is the ${\sbs}$-least clone containing $Q$; this provides an abstraction layer allowing to mention no circuits in most of the subsequent arguments. We do not do so, nevertheless. If the Instructor likes this approach, he might want to add an explicit lemma stating $[Q]$ to be a clone.}
\begin{thm}[Closure properties]\label{bool:closure}
	For every $P, Q \sbs \top$, it holds that:
	\begin{enumerate}
		\item $Q \sbs [Q]$;
		\item if $P \sbs Q$, then $[P] \sbs [Q]$;
		\item $[[Q]] = [Q]$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Given $f \in Q$, the trivial circuit~(\ref{bool:triv_circ}) over $Q$ computes $f$, so $f \in [Q]$.
	
	Since $g \in [P]$, there exists a circuit $C$ over $P$ with $g_C = g$. Obviously, $C$ is a circuit over $Q$ as well when $P \sbs Q$. Hence, $f \in [Q]$.
	
	From the first claim, it clearly follows that $[Q] \sbs [[Q]]$. It remains to prove that $[[Q]] \sbs [Q]$. We may apply Lemma~\ref{bool:struct} here. The predicate $\phi(f)$ in question is just $f \in [Q]$. Clearly, this property holds for every function in $[Q]$ and every projector (consider a circuit of the form $t_1 = x_j$).
	
	Finally, we have to check that superposition preserves $\phi$. Suppose that $h, g_1, \ldots, g_m \in [Q]$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x \in \ul{2}^n$. We need $f \in [Q]$. Let $D, C_1, \ldots, C_m$ be some circuits over $Q$ which compute the functions $h, g_1, \ldots, g_m$, respectively. Consider the circuit $C$ over $Q$:
	$$
	\begin{array}{rcl}
		t_1 &=& x_1\\
		t_2 &=& x_2\\
		&\ldots&\\
		t_n &=& x_n\\
		s_1 &=& C_1(t_1, \ldots, t_n)\\
		&\ldots&\\
		s_m &=& C_m(t_1, \ldots, t_n)\\
		r &=& D(s_1,\ldots,s_m).\\
	\end{array}
	$$
	It is easy to see that $C$ computes just the function $f$. Hence, $f \in [Q]$.
\end{proof}

\begin{exm}
	Now, we can easily prove $[{\neg}, {\wedge}] = [{\neg}, {\vee}]$ in a perfectly formal manner. As we have already seen, $\wedge \in [{\neg}, {\vee}]$. Then $\{ {\neg}, {\wedge} \} \sbs [{\neg}, {\vee}]$, whence $[{\neg}, {\wedge}] \sbs [[{\neg}, {\vee}]]$. But $[[{\neg}, {\vee}]] = [{\neg}, {\vee}]$, so $[{\neg}, {\wedge}] \sbs [{\neg}, {\vee}]$. The other inclusion is similar. Likewise obtain $[{\neg}, {\wedge}, {\vee}] = [{\neg}, {\wedge}] = [{\neg}, {\vee}]$.
\end{exm}

A set $Q \sbs \top$ is called \emph{closed} if $[Q] = Q$ (which is equivalent to $[Q] \sbs Q$ in view of Theorem~\ref{bool:closure}).\footnote{Such closed sets are also known as \emph{clones} on the set $\ul{2}$. The definition may vary slightly.} This essentially means that $Q$ is large enough to contain everything one can compute over $Q$ with a Boolean circuit. We already know that $\top$ is closed. The set of projectors $\bot = [\void]$ is closed as well, for $[\bot] = [[\void]] = [\void] = \bot$. In fact, every closure set $[P]$ is closed for $[[P]] = [P]$ by Theorem~\ref{bool:closure}. Thus, sets of the form $[P]$ and closed sets are exactly the same.

\begin{exm}\label{bool:clone_cap}
	If $P$ and $Q$ are closed, then $P \cap Q$ is closed as well. Indeed, $P \cap Q \sbs P$ implies $[P \cap Q] \sbs [P] = P$ and similarly for $Q$, whence $[P \cap Q] \sbs P \cap Q$.
\end{exm}

So, the set $[{\neg}, {\vee}]$ is closed, while the set $\{ {\neg}, {\vee} \}$ is not since the latter does not contain the function ${\to}$ yet the former does. But can we have a better description for the set $[{\neg}, {\vee}]$? Which functions does it contain? (Like we know that $[\void]$ consists of all projectors.) Which closed sets exist? On the other hand, given a closed set $Q$, can we find some `minimal' $P$ with $[P] = Q$? Clearly, $P = Q$ shall work, but we may have something more frugal, like in $[\void] = \bot = [\bot]$.

By the way, we say that $P$ is \emph{complete in $Q$} iff $[P] = Q$. Clearly, this implies $P \sbs Q$. A $\sbs$-minimal complete subset $P$ of $Q$ is called a \emph{basis} of $Q$, that is, $P$ is a basis of $Q$ iff $[P] = Q$ but $[P'] \neq Q$ for any $P' \subsetneq P$. E.\,g., $\bot$ is complete in $\bot$ but is not a basis thereof, while $\void$ is.

\begin{exc}
	Can a finite set be closed?
\end{exc}

Questions like those we presented had been thoroughly studied by the middle of the 20th century. Below, we shall present some of the most important results in this area.

\begin{rem}
	The three closure properties from Theorem~\ref{bool:closure} are widespread in mathematics and are found in many contexts that have nothing in common with Boolean functions (at first glance, at least). To give but one example, consider \emph{linear span}. For a subset $S$ of a vector space $V$ (say, over the field $\R$), its linear span $\langle S \rangle$ may be defined as the set of all possible finite sums of the form $\alpha_1 v_1 + \ldots + \alpha_k v_k$, where $v_i \in S$ and $\alpha_i \in \R$, including the `empty' sum with $k = 0$ which is identified with $\vec 0 \in V$.\footnote{This empty sum is only important when $S = \void$. In this case, $\langle \void \rangle = \{\vec 0\}$. Otherwise, one has $\vec 0 = v - v \in \langle S \rangle$ for whatever $v \in S$.} (Such sums are known as \emph{linear combinations}.)
	
	It is easy to check that $S \sbs \langle S \rangle$, $S \sbs T$ implies $\langle S \rangle \sbs \langle T \rangle$, and $\langle \langle S \rangle \rangle = \langle S \rangle$. Closed sets $S \sbs V$, with $\langle S \rangle = S$, are exactly subspaces of the vector space $V$, and a basis of a subspace $S$ may be equivalently defined as its ${\sbs}$-minimal complete subset.
	
	One can also prove that $\langle S \rangle$ is just the intersection of all subspaces of $V$ that include $S$ or, equivalently, the ${\sbs}$-least such subspace of $V$.
\end{rem}

\mcomm{The Instructor might wish to give more examples of this kind like topological or deductive closures.}

\paragraph{Normal forms for Boolean functions.} As a matter of fact, one needs very few well-known functions in $Q$ in order to obtain $[Q] = \top$. Moreover, every Boolean function can be computed by a circuit of a very special form. We shall however step aside from circuit formalism in favor of slightly more popular `normal form expressions'.

We do not want to define `expressions' formally; for every thing formal, we will still be able to emulate those with circuits. Given a variable $x_i$, the expressions $x_i$ and $\neg x_i$ are called \emph{literals}. Every expression of the form $l_1 \wedge l_2 \wedge \ldots \wedge l_k$, where each $l_i$ is a literal and $k \ge 1$, is known as an \emph{elementary conjunction}. Notice the lack of parentheses here; they do not matter due to the associativity property from Lemma~\ref{L15:bool_eq1}. Similarly, every expression of the form $l_1 \vee l_2 \vee \ldots \vee l_k$, where $k \ge 1$, is called an \emph{elementary disjunction}.\footnote{Elementary conjunctions are also known as \emph{minterms} (especially, when every variable $x_i$, $1 \leq i \leq n$, occurs just once therein). Elementary disjunctions are called \emph{maxterms} then.} A \emph{disjunctive normal form} (DNF) is a disjunction $c_1 \vee c_2 \vee \ldots \vee c_n$, $n \geq 1$, of elementary conjunctions $c_i$. Similarly, a \emph{conjunctive normal form} (CNF) is a conjunction $d_1 \wedge d_2 \wedge \ldots \wedge d_n$, $n \geq 1$, of elementary disjunctions $d_i$.

\begin{exm}
	Every literal is an elementary conjunction and disjunction as well. Every elementary conjunction or disjunction is both a DNF and a CNF. So, $\neg x_1 \wedge x_2 \wedge x_1$ is both a CNF (of three elementary disjunctions) and a DNF (of one elementary conjunction). The expression $(\neg x_1 \wedge x_2 \wedge x_3) \vee x_3 \vee (x_5 \vee \neg x_2)$ is a DNF but not a CNF.
\end{exm}

It is not hard to formally define, say, a DNF as a circuit of the form 
$$
\begin{array}{rcl}
	t_1 &=& R_1\\
	&\ldots&\\
	t_m &=& R_m\\
\end{array}
$$
such that in the $R_i$ terms, every occurrence of $\neg$ precedes every occurrence of $\wedge$ and every occurrence of $\wedge$ precedes every occurrence of $\vee$ (multiple negations are not a problem, of course, for $\neg \neg x = x$). Clearly, a function `equals' a DNF-expression iff it is computable by such a circuit. 

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.75\textwidth]{circuits_dnf.pdf}
	\caption{A DNF as a Boolean circuit. Each conjunction-labeled box computes the conjunction of its inputs (it is quite clear how to construct such a circuit). Similarly for disjunctions. The long dotted line hides a connection of literals to conjunction-boxes, which specifies a particular DNF and may be tricky.}
\end{figure}

The most noteworthy fact about DNF is that \emph{every} Boolean function equals one of such expressions. The same is true for CNF. Thus, for each $f \in \top$, an equation of the form $f(\vec x) = D(\vec x) = C(\vec x)$ holds for a suitable DNF $D$ and CNF $C$ for all $\vec x$. In circuit terms, every function $f$ is computable by a suitable DNF (or CNF) circuit.
\begin{thm}\label{bool:dnf}
	For every Boolean function $f^{(n)}$, there exists a DNF $D$ of variables $x_1, \ldots x_n$ such that $f(\vec x) = D(\vec x)$ for all $\vec x \in \ul{2}^n$.
\end{thm}
\begin{proof}
	For an arbitrary value $\sigma \in \ul{2}$ and a variable $x$, let
	$$
	\begin{array}{rcl}
		x^\sigma&=&\begin{cases}
			x&\text{if $\sigma = 1$;}\\
			\neg x&\text{if $\sigma = 0$.}
		\end{cases}
	\end{array}
	$$
	Clearly, $x^\sigma$ is just a shorthand notation for the literal $x$ or $\neg x$. Considering all possible values for $x, \sigma \in \ul{2}$, it is easy to notice that $x^\sigma = 1$ iff $x = \sigma$.
	
	For every $\vec\sigma = (\sigma_1,\ldots, \sigma_n) \in \ul{2}^n$, consider the elementary conjunction $\Phi_{\vec \sigma} = x^{\sigma_1}_1 \wedge \ldots \wedge x^{\sigma_n}_n$. E.\,g., one has $\Phi_{1001} = x^1_1 \wedge x^0_2 \wedge x^0_3 \wedge x^1_4  = x_1 \wedge \neg x_2 \wedge \neg x_3 \wedge x_4$.
	
	Let us show that $\Phi_{\vec \sigma}(\vec x) = 1$ iff $\vec x = \vec \sigma$. Indeed, the former equation means $x^{\sigma_i}_i = 1$ for each $i$, that is, $x_i = \sigma_i$ and $\vec x = \vec \sigma$ finally. A tuple $\vec \sigma \in \ul{2}^n$ is thus `encoded' by the elementary conjunction $\Phi_{\vec\sigma}$.
	
	Let $U$ be the set $\{ \vec \sigma \in \ul{2}^n \mid f(\vec\sigma) = 1 \}$ of all tuples where the function $f^{(n)}$ takes the value $1$. If $U = \void$, we have $f(\vec x) = x_1 \wedge \neg x_1$, which is a DNF we need. Otherwise, let $U = \{\vec\sigma^1, \ldots,  \vec\sigma^k\}$. Consider the expression $D =  \Phi_{\vec \sigma^1} \vee \ldots \vee \Phi_{\vec \sigma^k}$. Clearly, $D$ is a DNF.
	
	Furthermore, 
	$$
	\begin{array}{rcl}
		D(\vec x) = 1 &\iff& \exists j\: \Phi_{\vec\sigma^j}(\vec x) = 1\\
		&\iff& \exists j\: \vec x = \vec\sigma^j\\
		&\iff& \vec x \in U\\
		&\iff& f(\vec x) = 1.
	\end{array}
	$$
	So, $D$ is a required DNF.
\end{proof}

\begin{rem}
	A similar theorem holds for CNF. One can prove it by taking a `dual' of the argument above. First of all, consider literals $x^{\neg \sigma}$. Clearly, $x^{\neg \sigma} = 0$ iff $x \neq \neg \sigma$ iff $x = \sigma$. Then take the elementary disjunction $\Psi_{\vec\sigma} = x^{\neg \sigma_1}_1 \vee \ldots \vee x^{\neg \sigma_n}_n$ for each $\sigma \in \ul{2}^n$. This disjunction equals $0$ iff $\vec x = \vec \sigma$; so, we can `encode' a tuple with an elementary disjunction as well. Say, we have $\Psi_{1001} = x^0_1 \vee x^1_2 \vee x^1_3 \vee x^0_4  = \neg x_1 \vee x_2 \vee x_3 \vee \neg x_4$.
	
	Finally, consider the set $Z = \{ \vec \sigma \in \ul{2}^n \mid f(\vec\sigma) = 0 \}$. If $Z = \{\vec\sigma^1, \ldots,  \vec\sigma^k\} \neq \void$, the CNF $C =  \Psi_{\vec \sigma^1} \wedge \ldots \wedge \Psi_{\vec \sigma^k}$ is an expression we are interested in. If $Z = \void$, just take $C = x_1 \vee \neg x_1$.
\end{rem}

\begin{exm}
	Our proof for Theorem~\ref{bool:dnf} provides an \emph{algorithm} to obtain a DNF for any given function $f$ (provided one can somehow compute its values).
	
	For example, consider a Boolean function $f^{(3)}$ with $U = \{ 000, 010, 101 \}$ and, respectively, $Z = \{ 001, 011, 100, 110, 111 \}$. Applying the algorithm gives the DNF $D = (\neg x_1 \wedge \neg x_2 \wedge \neg x_3) \vee (\neg x_1 \wedge x_2 \wedge \neg x_3) \vee (x_1 \wedge \neg x_2 \wedge x_3)$.
	
	The `dual' algorithm returns the CNF $C = (x_1 \vee x_2 \vee \neg x_3) \wedge (x_1 \vee \neg x_2 \vee \neg x_3) \wedge (\neg x_1 \vee x_2 \vee x_3) \wedge (\neg x_1 \vee \neg x_2 \vee x_3) \wedge (\neg x_1 \vee \neg x_2 \vee \neg x_3)$.
	
	Neither $D$ nor $C$ is of the least possible length among the expressions for $f$ of its kind. Say,  $D' = (\neg x_1 \wedge \neg x_2) \vee (x_1 \wedge \neg x_2 \wedge x_3)$ is a shorter DNF for $f$. 
	
	This algorithm is not particularly effective since it generally requires computing all the $2^n$ values of the function $f^{(n)}$. No essentially better algorithm for this task is currently known.
\end{exm}
As each DNF or CNF can be computed with a circuit over $\{{\neg}, {\wedge}, {\vee} \}$, so every Boolean function can, and we obtain
\begin{corr}\label{bool:dnf_compl}
	$\top = [{\neg}, {\wedge}, {\vee}] = [{\neg}, {\wedge}] = [{\neg}, {\vee}]$.
\end{corr}
This way, we have found very simple function sets being complete in $\top$. In fact, just one function is enough! Consider the function ${\dvd}$ that may be defined by the equation $x \dvd y = \neg (x \wedge y)$. This function is known as \emph{Sheffer stroke}. As $\neg x = \neg (x \wedge x) = x \dvd x$ and $x \wedge y = \neg (x \dvd y) = (x \dvd y) \dvd (x \dvd y)$, we get $\{{\neg}, {\wedge} \} \sbs [\,{\dvd}\, ]$, whence $\top = [{\wedge}, {\neg}] \sbs [[\,{\dvd}\,]] = [\,{\dvd}\,] \sbs \top$ by Theorem~\ref{bool:closure}. So, $\top = [\,{\dvd}\,]$.

Regarding a Boolean circuit as a real electric circuit, this means that ${\dvd}$-function gates alone suffice to compute any Boolean function. In electronics, they are called \emph{NAND gates} (``not-and'').

\begin{rem}\label{bool:func_var}
	There is another instructive proof for $\top \sbs [{\neg}, {\wedge}, {\vee}]$. First of all, we prove the equation
	$$f(y, \vec x) = (y \wedge f(1, \vec x)) \vee (\neg y \wedge f(0, \vec x))$$
	for every function $f^{(n+1)}$ and $y \in \ul{2}$, $\vec{x} \in \ul{2}^{n}$ (this equation is known as an \emph{expansion of $f$ in the first argument} (there are other expansions of diverse types)). Indeed, if $y = 1$, it turns into $f(1, \vec x) = (1 \wedge f(1, \vec x)) \vee (\neg 1 \wedge f(0, \vec x)) = (1 \wedge f(1, \vec x)) \vee 0$, which is clearly true. The case when $y = 0$ results in $f(0, \vec x) = 0 \vee (1 \wedge f(0, \vec x))$, that surely holds.
	
	Now, we are to employ induction on the number $n$ of a function's arguments. If $n = 1$, there are just four Boolean functions: $0^{(1)}$, $1^{(1)}$, $\neg$, $\id_{\ul{2}}$ (where \emph{constants} $0^{(1)}$ and $1^{(1)}$ are defined by the equations $0(x) = 0$ and $1(x) = 1$ for each $x \in \ul{2}$, respectively), each of which belongs to $[{\neg}, {\wedge}, {\vee}]$ as $0(x) = x \wedge \neg x$, $1(x) = x \vee \neg x$, while $\id_{\ul{2}}$ is a projector.
	
	Assume that every Boolean function of $n$ arguments belongs to $[{\neg}, {\wedge}, {\vee}]$. Consider a function $f^{(n+1)}$. By the above formula, get
	$$f(y, \vec x) = (y \wedge g_1(\vec x)) \vee (\neg y \wedge g_2(\vec x)),$$
	where $g_1$ and $g_2$ with $g_1(\vec x) = f(1, \vec x)$ and $g_2(\vec x) = f(0, \vec x)$ are functions of $n$ arguments and, hence, belong to $[{\neg}, {\wedge}, {\vee}]$. It is now obvious how one can construct a circuit for $f$ over $[{\neg}, {\wedge}, {\vee}]$ given such circuits for $g_1$, $g_2$.
	
	Actually, the above  expansion formula suggests a direct proof for Theorem~\ref{bool:dnf}. Indeed, one can expand a function in more than one argument, say, in the first and second ones:
	\begin{multline*}
		f(\vec x) = (x_1 \wedge x_2 \wedge f(1, 1, x_3, \ldots x_n)) \vee (x_1 \wedge \neg x_2 \wedge f(1, 0, x_3, \ldots x_n)) \vee\phantom{y}\\
		(\neg x_1 \wedge x_2 \wedge f(0, 1, x_3, \ldots x_n)) \vee (\neg x_1 \wedge \neg x_2 \wedge f(0, 0, x_3, \ldots x_n)).
	\end{multline*}
	Continuing this way, one can make $f$-terms in the right-hand side constants, which effectively turns the right-hand side into a DNF.
\end{rem}
\begin{exc}
	Prove Theorem~\ref{bool:dnf} and the similar statement for CNF applying suitable expansion formulas.
\end{exc}

\medskip

Another important function set is $\{ {\wedge}, {+}\}$, which we have already come across when building a binary adder. Is this set complete in $\top$? In fact, it is not but it lacks not much to be such. Moreover, there exists a very natural `normal form' related to these functions. According to Remark~\ref{bool:ord_field}, we will identify $\wedge$ with multiplication operation $\cdot$. Let us introduce more special form `expressions' (easily formalizable as circuits, of course).

Let $\vec x = (x_1, x_2, \ldots, x_n)$ be a fixed tuple of \emph{pairwise distinct} variables. A product of the form $x_{i_1} x_{i_2} \ldots x_{i_k} = x_{i_1} \cdot x_{i_2} \cdot \ldots \cdot x_{i_k} = x_{i_1} \wedge x_{i_2} \wedge \ldots \wedge x_{i_k}$ where $i_1 < i_2 < \ldots < i_k$ is called a \emph{monomial of degree $k$}. If $k = 0$, we identify the resulting `empty' product with $1 \in \ul{2}$. Consider a product $x_1 x_2 x_1 x_1 x_2$. Formally, it is not a monomial but it `equals' one (in value) for $x_2 x_1 = x_1 x_2$ and $x_i x_i = x_i$ by Lemma~\ref{L15:bool_eq1}. That is why we may require all the variables in a monomial to be arranged in ascending order. Let $a \in \ul{2}$ and $x_{i_1} x_{i_2} \ldots x_{i_k}$ be a monomial. By $a x_{i_1} x_{i_2} \ldots x_{i_k}$ we denote $x_{i_1} x_{i_2} \ldots x_{i_k}$ if $a = 1$, and $0$ otherwise. We shall call $a$ the \emph{coefficient for} $x_{i_1} x_{i_2} \ldots x_{i_k}$ then.

How many monomials over $\vec x$ exist? Clearly, one can encode each monomial with the set $\{i_1, i_2, \ldots, i_k\}$, $i_1 < i_2 < \ldots < i_k$, and encode each set in its turn with the binary word $$\underbrace{00\ldots0}_{i_1 - 1}1\underbrace{00\ldots0}_{i_2 - i_1 - 1} 1 \underbrace{00\ldots0}_{i_3 - i_2 - i_1 - 1}1 \ldots \underbrace{00\ldots0}_{i_k - i_{k-1} -\ldots - i_1 - 1} 1 \underbrace{00\ldots0}_{n - i_k},$$
that is, the word of length $n$ with `$1$'s at positions number $i_1, i_2, \ldots,$ and $i_k$ exactly. It is easy to see that this encoding is indeed a bijection between the set of monomials and $\ul{2}^n$. For $n = 5$, e.\,g., the word $01101$ encodes the monomial $x_2 x_3 x_5$ while $00000$ encodes the `empty' monomial $1$. So, we have $2^n$ monomials overall.
Furthermore, this encoding makes it convenient to name coefficients according to their respective monomials. Say, the notation $a_{01101}$ stands for the coefficient for $x_2 x_3 x_5$. One more natural step in this direction is to identify a binary word $\vec \sigma$ with its meaning $b(\vec \sigma)$ as a natural number. Then we may use $a_{13}$ instead of $a_{01101}$, $a_{31}$ for $a_{11111}$, and $a_0$ for $a_{00000}$.

With all these notations fixed, we may introduce our normal form finally. It is a \emph{Zhegalkin polynomial}, that is, a sum of the form
$$P(\vec x) = a_{00\ldots0}\, 1 + a_{10\ldots0}\, x_1 + a_{010\ldots0}\, x_2 + \ldots + a_{0\ldots01}\, x_n + a_{110\ldots0}\, x_1 x_2 + \ldots + a_{11\ldots 1}\, x_1 x_2\ldots x_n.$$
The order of summands does not matter for computing the value $P(\vec x)$ at $\vec x \in \ul{2}^n$, so we do not want to fix it formally. It is easy to see that this value can be computed by a natural circuit of $\vec x$ over the set $\{ {\cdot}, {+}, 1^{(1)} \}$, where $1^{(1)}$ is the \emph{constant $1$} unary function defined by the equation $1(x) = 1$ for each $x \in \ul{2}$. Formally, one might \emph{define} a polynomial as such a circuit, but this would make coefficients $a_{\vec\sigma}$ less highlighted, though they are quite important here as we shall see in a moment.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.75\textwidth]{circuits_poly.pdf}
	\caption{A Zhegalkin polynomial as a Boolean circuit. The red arrow is present iff $a_{00\ldots0} = 1$. Otherwise, a circuit over $\{{\cdot}, {+}\}$ would suffice.}
\end{figure}

\begin{thm}\label{bool:zhegalkin}
	For every Boolean function $f^{(n)}$, there exists a unique tuple $(a_{00\ldots0}, a_{010\ldots0}, \ldots, a_{11\ldots1}) \in \ul{2}^{2^n}$ such that
	\begin{equation}\label{bool:eq_poly}
		f(\vec x) = a_{00\ldots0}\, 1 + a_{10\ldots0}\, x_1 + a_{010\ldots0}\, x_2 + \ldots + a_{0\ldots01}\, x_n + a_{110\ldots0}\, x_1 x_2 + \ldots + a_{11\ldots 1}\, x_1 x_2\ldots x_n
	\end{equation}
	for all $\vec x \in \ul{2}^n$.
\end{thm}
Thus, unlike DNF and CNF, the polynomial representation of a Boolean function is essentially unique.
\begin{proof}
	There are finitely many tuples $\vec \sigma \in \ul{2}^n$, so by substituting all possible values $\vec \sigma$ for $\vec x$, we see that equation~(\ref{bool:eq_poly}) is equivalent to the following system of simultaneous equations:
	\begin{equation}\label{bool:eq_sle}
		\begin{array}{l}
			\begin{cases}
				f(000\ldots0) &= a_{00\ldots0}\\
				f(100\ldots0) &= a_{00\ldots0} + a_{10\ldots0}\\
				f(010\ldots0) &= a_{00\ldots0} + a_{010\ldots0}\\
				&\ldots\\
				f(110\ldots0) &= a_{00\ldots0} + a_{10\ldots0} + a_{010\ldots0} + a_{110\ldots0}\\
				&\ldots\\
				f(111\ldots1) &= a_{00\ldots0} + a_{10\ldots0} + a_{010\ldots0} + \ldots + a_{0\ldots01} + a_{110\ldots0} + \ldots + a_{11\ldots 1}.\\
			\end{cases}
		\end{array}
	\end{equation}
	What is the main idea behind this? Assume that a binary tuple $\vec \sigma \in \ul{2}^n$ has `$1$'s at positions $\{i_1, i_2, \ldots, i_k\}$ exactly---in other words, $\vec \sigma$ encodes the monomial $x_{i_1} x_{i_2} \ldots x_{i_k}$. This monomial takes $1$ at $\vec\sigma$ (so we have $a_{\vec \sigma}$ in the right-hand side) as well as every monomial encoded by a subset of $\{i_1, i_2, \ldots, i_k\}$ does. That is, if $\vec \tau \in \ul{2}^n$ has `$1$'s at \emph{some} positions from $\{i_1, i_2, \ldots, i_k\}$ but nowhere else, the summand $a_{\vec \tau}$ is present in the right-hand side, and vice versa. Let us denote this set of tuples $\vec \tau$ by $I(\vec \sigma)$. So, we have $2^k = |I(\vec \sigma)|$ summands in the right-hand side when $\vec \sigma$ contains just $k$ unities. For example, if $\vec \sigma = 01101$, we get $a_{\vec \tau}$ in the right-hand side iff $\vec\tau \in I(\vec{\sigma}) = \{01101, 00101, 01001, 01100, 01000, 00100, 00001, 00000\}$. It is clear that $b(\vec \tau) \leq b(\vec \sigma)$ for each $\vec \tau \in I(\vec \sigma)$ since $b(\vec \sigma) = 2^{n - i_1} + 2^{n - i_2} + \ldots 2^{n - i_k}$ while $b(\vec \tau)$ includes just some of these summands.
	
	Now, let us have a look at system~(\ref{bool:eq_sle}). It is a system of $2^n$ linear equations in $2^n$ variables $a_{\vec \sigma}$ with coefficients from $\ul{2}$. A tuple $(a_{00\ldots0}, a_{010\ldots0}, \ldots, a_{11\ldots1}) \in \ul{2}^{2^n}$ is a solution to the system iff this tuple satisfies our required constraint~(\ref{bool:eq_poly}).
	
	We know from Remark~\ref{bool:ord_field} that the theory of solving such systems is basically the same as what the Reader should know for real coefficients from his Linear Algebra course. According to that theory, a system of linear equations has a unique solution iff the determinant of its matrix is not zero. Let us prove this is the case.
	
	Reordering equations or variables preserves the absolute value of the determinant, so let us arrange the equations from~(\ref{bool:eq_sle}), which are of the form $f(\vec \sigma) = \sum_{\vec\tau \in I(\vec \sigma)} a_{\vec\tau} = a_{00\ldots0} + \ldots + a_{\vec \sigma}$, as $b(\vec \sigma)$ ascends. Identifying $\vec \sigma$ and $b(\vec \sigma)$, we obtain the system
	$$
	\begin{array}{l}
		\begin{cases}
			f(0) &= a_0\\
			f(1) &= a_0 + a_1\\
			f(2) &= a_0 +\phantom{a_1 +} a_2\\
			f(3) &= a_0 + a_1 + a_2 + a_3\\
			f(4) &= a_0 +\phantom{a_1 + a_2 + a_3 +} a_4\\
			&\ldots\\
			f(2^{n} - 1) &= a_0 + a_1 + a_2 + \ldots + a_{2^{n} - 1}.\\
		\end{cases}
	\end{array}
	$$
	The key observation here is that the equation for $f(b(\vec \sigma))$ clearly takes the $b(\vec \sigma)$-th place and, in the right-hand side, it contains a sum of certain numbers $a_i$ with $i \leq b(\vec \sigma)$ whereas $a_{b(\vec \sigma)}$ is always present. So, the matrix of the latter system is a lower triangular one with all unities at its main diagonal. The determinant of the matrix is thus $1$, so it is non-zero for the original system as well.\footnote{In fact, the determinant of the original system~(\ref{bool:eq_sle}) is $1$ for $-1 = 1$ in the field $\mathrm{GF}(2)$ (as $1 + 1 = 0$).}
\end{proof}

\begin{exm}
	The proof above effectively reduces finding a polynomial for a function to solving a system of linear equations over the field $\mathrm{GF}(2)$. Given the values of $f$, this procedure may be highly efficient. Even without much optimization (but see Remark~\ref{bool:coeff}), solving a small-sized system is really easy.
	
	Consider the Boolean function $f^{(3)}$ defined by the table
	\begin{center}
		\begin{tabular}{c c c | c }
			$x$&$y$&$z$&$f(x,y,z)$\\
			\hline
			$0$&$0$&$0$&$1$\\
			$0$&$0$&$1$&$1$\\
			$0$&$1$&$0$&$0$\\
			$0$&$1$&$1$&$1$\\
			
			$1$&$0$&$0$&$0$\\
			$1$&$0$&$1$&$0$\\
			$1$&$1$&$0$&$0$\\
			$1$&$1$&$1$&$1$\
		\end{tabular}
	\end{center}
	Let us find such a tuple $\vec a = (a_{000}, a_{100}, \ldots, a_{111}) \in \ul{2}^8$ that 
	$$f(x,y,z) = a_{000} + a_{100} x + a_{010} y + a_{001} z + a_{110} x y + a_{101} x z + a_{011} y z + a_{111} x y z$$
	for each $(x,y,z) \in \ul{2}^3$. Applying this equation to every possible value of $(x,y,z)$, obtain the system
	$$
	\begin{cases}
		\begin{array}{lcccr}
			a_{000}  &=& 1 &=& f(0,0,0)\\
			a_{000} + a_{100} &=& 0 &=& f(1,0,0)\\
			a_{000} + a_{010} &=& 0 &=& f(0,1,0)\\
			a_{000} + a_{001} &=& 1 &=& f(0,0,1)\\
			a_{000} + a_{100} + a_{010} + a_{110} &=& 0 &=& f(1,1,0)\\
			a_{000} + a_{100} + a_{001} + a_{101} &=& 0 &=& f(1,0,1)\\
			a_{000} + a_{010} + a_{001} + a_{011} &=& 1 &=& f(0,1,1)\\
			a_{000} + a_{100} + a_{010} + a_{001} + a_{110} + a_{101} + a_{011} + a_{111} &=& 1 &=& f(1,1,1).\\
		\end{array}
	\end{cases}
	$$
	This is easily solvable by standard Gaussian elimination. Say, one can add rows 1, 2, 3, and 5 together to obtain
	$a_{110} = a_{000} + a_{000} + a_{100} + a_{000} + a_{010} + a_{000} + a_{100} + a_{010} + a_{110} = 1 + 0 + 0 + 0 = 1$.
	Finally, $\vec a = (1, 1, 1, 0, 1, 0, 1, 0)$ and
	$$f(x,y,z) = 1 + x + y + xy + yz.$$
	You may want to check this polynomial values to match the above table.
\end{exm}

\begin{corr}\label{bool:poly_compl}
	$\top = [\,{\cdot}, {+}, 1^{(1)}]$.
\end{corr}
Indeed, given the function $1^{(1)}$, one can compute the value $1 \in \ul{2}$ for the polynomial as $1(x_1)$. Notice that the constant $0^{(1)}$ is not needed for Zhegalkin polynomials since it can be obtained by letting all the coefficients equal zero (so the polynomial is `empty' and does not contain any term).
\begin{exm}
	It is worth memorizing Zhegalkin polynomials for these important functions:
	$$
	\begin{array}{rcl}
		\neg x &=& 1 + x;\\
		x \vee y &=& x + y + xy;\\
		x \to y &=& 1 + x + xy;\\
		x \leftrightarrow y &=& 1 + x + y;\\
		\mathrm{maj}(x, y, z) &=& xy + xz + yz.\\
	\end{array}
	$$
	The first two equations here suffice to prove Corollary~\ref{bool:poly_compl}. Indeed, we have $\top = [{\neg}, {\vee}]$ by Corollary~\ref{bool:dnf_compl}; then, clearly, ${\neg} \in [{+}, 1^{(1)}]$ and ${\vee} \in [\,{\cdot}, {+}, 1^{(1)}]$, whence $\top = [{\neg}, {\vee}] \sbs [[\,{\cdot}, {+}, 1^{(1)}]] = [\,{\cdot}, {+}, 1^{(1)}]$ by Theorem~\ref{bool:closure}. Yet this does not make Zhegalkin polynomials redundant because they are still important for their unicity property. 
\end{exm}

\begin{rem}\label{bool:coeff}
	It is not hard to solve system~(\ref{bool:eq_sle}) explicitly. Recall that $I(\vec \sigma)$ is the set of all tuples $\vec \tau$ that may be obtained from $\vec \sigma$ by changing some `$1$'s to `$0$'s. If $\vec \sigma$ has `$1$'s at positions $i_1, \ldots, i_k$ exactly, we get $|I(\vec\sigma)| = 2^k$. System~(\ref{bool:eq_sle}) contains the equations $f(\vec \sigma) = \sum_{\vec\tau \in I(\vec \sigma)} a_{\vec\tau}$ for each $\vec \sigma \in \ul{2}^n$.
	
	Now, let us consider the sum
	$$\sum_{\vec\tau \in I(\vec \sigma)} f(\vec\tau) = \sum_{\vec\tau \in I(\vec \sigma)} \sum_{\vec\rho \in I(\vec \tau)} a_{\vec\rho} = \sum_{\vec\rho \in I(\vec \sigma)} c_{\vec \rho} \cdot a_{\vec \rho},$$
	where the coefficient $c_{\vec \rho} \in \ul{2}$ shows how many occurrences $a_{\vec \rho}$ has in the sum ($c_{\vec \rho} = 0$ if this number $c'_{\vec \rho} \in \N$ is even, and $c_{\vec \rho} = 1$ otherwise). To get this formula, we have applied the fact that $I(\vec \tau) \sbs I(\vec \sigma)$ when $\vec \tau \in I(\vec \sigma)$.
	
	What is $c'_{\vec \rho}$ for a fixed $\vec \rho \in I(\vec \sigma)$? Obviously, it is the number of such tuples $\vec \tau \in I(\vec \sigma)$ that $\vec \rho \in I(\vec \tau)$. As $\vec \sigma$ has `$1$'s just at the positions from $I_1 = \{i_1, \ldots, i_k\}$, the `position set' for $\vec\rho$ is some $I_2 = \{ i_{t_1}, \ldots, i_{t_s} \} \sbs I_1$. If $I_3$ is the `position set' for $\vec \tau$, our requirement on $\vec \tau$ is clearly equivalent to $I_2 \sbs I_3 \sbs I_1$, that is, $I_3 = I_2 \cup X$ with $X \sbs I_1 \setminus I_2$. Hence, $c'_{\vec\rho} =  |\mP(I_1 \setminus I_2)| = 2^{k - s}$. This number is even whenever $s < k$, so $c_{\vec \rho} = 0$ then. If $s = k$, we have $I_2 = I_1$, i.\,e., $\vec \rho = \vec \sigma$, and $c'_{\vec \rho} = 2^0 = 1 = c_{\vec \rho}$.
	
	Finally, $$\sum_{\vec\tau \in I(\vec \sigma)} f(\vec\tau) = a_{\vec \sigma},$$ which is the explicit solution to~(\ref{bool:eq_sle}). For example, $a_{00101} = f(00000) + f(00001) + f(00100) + f(00101)$. Such sums share some parts for distinct $a_{\vec \sigma}$. Therefore, further computational optimizations are possible.
\end{rem}

\begin{rem}
	One can prove Theorem~\ref{bool:zhegalkin} without any reference to linear algebra. Indeed, for each $\vec a = (a_{00\ldots0}, a_{010\ldots0}, \ldots, a_{11\ldots1}) \in \ul{2}^{2^n}$, the right-hand side polynomial $P_{\vec a}(\vec x)$ of equation~(\ref{bool:eq_poly}) represents a Boolean function we may denote by $\phi(\vec a)$. Hence, $\phi$ is a function from $\ul{2}^{2^n}$ to $\ul{2}^{\ul{2}^n}$, between two finite sets of equal size.
	
	Let us show that $\phi$ is injective. Assume that $\phi(\vec a) = \phi(\vec b)$ but $\vec a \neq \vec b$, whence $P_{\vec a}(\vec x)$ and $P_{\vec b}(\vec x)$ differ in at least one coefficient, so  $Q(\vec x) = P_{\vec a}(\vec x)+ P_{\vec b}(\vec x) = P_{\vec c}(\vec x) = P_{\vec a + \vec b}(\vec x)$ has at least one non-zero coefficient (as one might expect, $\vec a + \vec b = (a_0 + b_0, \ldots, a_{2^n - 1} + b_{2^n - 1})$). On the other hand, the value of $P_{\vec a}(\vec x)$ equals $\phi(\vec a)(\vec x)$ at each $\vec x \in \ul{2}^n$, so the values of $P_{\vec a}(\vec x)$ and $P_{\vec b}(\vec x)$ are always identical; hence $Q(\vec x) = 0$ for each $\vec x \in \ul{2}^n$.
	
	At least one coefficient $c_{\vec \sigma}$ in $Q$ is $1$. Let us take a tuple $\vec \sigma$ with $c_{\vec \sigma} = 1$ that contains the least possible number $k$ of `$1$'s (which is know as the \emph{weight} $||\vec \sigma|| \in \N$ of $\vec\sigma$). This tuple has `$1$'s at positions $i_1, i_2, \ldots, i_k$ exactly and encodes the monomial $x_{i_1} x_{i_2} \ldots x_{i_k}$. What is the value $Q(\vec \sigma)$? The monomial $x_{i_1} x_{i_2} \ldots x_{i_k}$ (in particular, $1$ if $k = 0$) contributes $1$ to the sum; any monomial of lesser weight contributes $0$ as its coefficient is zero; any monomial of greater or equal weight (yet other than $\vec \sigma$) contains a variable $x_j$ with $j \notin \{i_1, i_2, \ldots, i_k \}$, so it contributes $0$ as well. Thus, $Q(\vec \sigma) = 1$, which is impossible. A contradiction.
	
	By Theorem~\ref{L10:fin_sur_in}, $\phi$ is surjective, that is, a tuple $\vec a$ with $P_{\vec a}(\vec x) = \phi(\vec a)(\vec x) = f(\vec x)$ exists for every Boolean function $f$. Such $\vec a$ is unique by injectivity of $\phi$.
	
	The main drawback of this proof is that it gives no explicit algorithm for computing $\vec a$.
\end{rem}

\section{Closed sets}
Up to now, we have seen $\top$ and $\bot$ as our only closed set examples with an explicit characterization (that is, other than ``$[Q]$ for a given $Q$''). It is known that there are countably many closed sets of Boolean functions; for each set, a finite basis and quite a neat characterization are known as well. Let us consider a few particularly important closed sets.

\paragraph{Constant-preserving functions.}
The set $P_0 = \{ f \in \top \mid f(\vec 0) = 0\}$, where $\vec 0 = 00\ldots0$ (the length is just how many arguments $f$ has), consists of all functions that \emph{preserve constant $0$}. Similarly, $P_1 = \{ f \in \top \mid f(\vec 1) = 1\}$,  where $\vec 1 = 11\ldots1$, is the set of functions \emph{preserving constant $1$}. The set $P = P_0 \cap P_1$ is called that of \emph{constant-preserving functions}. If $P_0$ and $P_1$ are closed, then $P$ must be closed as well by Example~\ref{bool:clone_cap}.

\begin{exm}
	We have ${\wedge}, {\vee} \in P_0 \cap P_1$; ${+} \in P_0 \setminus P_1$; ${\to} \in P_1 \setminus P_0$; and $\neg \notin P_0 \cup P_1$.
\end{exm}

The set $P_0$ (and similarly $P_1$) is closed indeed. Let us prove $[P_0] \sbs P_0$, i.\,e., every function $f \in [P_0]$ satisfies the property $f \in P_0$. By Lemma~\ref{bool:struct}, it suffice to check that $P_0 \sbs P_0$ (trivial); $\bot \sbs P_0$, which is also clear as $p(00\ldots0) = 0$ for every projector $p$; and that superposition preserves this property. The latter means that from $h, g_1, \ldots,g_m \in P_0$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$, it follows that $f \in P_0$. Indeed,
$$f(\vec 0) = h(g_1(\vec 0),\ldots, g_m(\vec 0)) = h(0, \ldots, 0) = 0.$$

\begin{exm}
	It is not possible to express ${\to}$ via ${\wedge}$ and ${\vee}$, i.\,e., ${\to} \notin [{\wedge}, {\vee}]$. Indeed, one gets ${\wedge}, {\vee} \in P_0$, whence $[{\wedge}, {\vee}] \sbs [P_0] = P_0$ but ${\to} \notin P_0$. One may say that the set $P_0$ (or the property to preserve $0$) is an \emph{obstacle} (or \emph{invariant}) preventing ${\to}$ from being computable over $\{{\wedge}, {\vee}\}$: every such function must preserve $0$, whereas ${\to}$ does not. So, when checking a function for being expressible via some others, one either finds a circuit to prove it is, or finds an obstacle to prove it is not. 
\end{exm}
\begin{exc}
	Prove that there are exactly $2^{2^n - 1}$ functions of $n$ arguments in $P_i$, $i \in \ul{2}$.
\end{exc}

\paragraph{Monotonicity.}
Another interesting set is that of \emph{monotonic} functions. Let $\vec \sigma, \vec \tau \in \ul{2}^n$. We put $\vec \sigma  \leq \vec \tau$ iff $\sigma_i \leq \tau_i$ for each $i$ from $1$ to $n$ ($x_i$ and $y_i$ are compared w.\,r.\,t.\ the `natural' order where $0 \leq 1$). For example, $(1,0,1,0,0) \leq (1,1,1,0,1)$, whereas $(1,0,1,0,0)$ and $(1,1,0,0,0)$ are incomparable. It is easy to see that ${\leq}$ is a non-strict partial order on the set $\ul{2}^n$. (Formally, one may define such a relation separately for each $n$.) From Section~\ref{sect:orders}, we know that the relation $<$ with $\vec \sigma < \vec \tau \iff \vec \sigma \leq \vec \tau \wedge \sigma \neq \tau$ is the strict partial `counterpart' order for ${\leq}$. Clearly, $\vec \sigma < \vec \tau$ iff there is one or more indices $i_1, \ldots, i_k$ such that $\sigma_j < \tau_j$ when $j = i_s$ for some $s \leq k$ and $\sigma_j = \tau_j$ otherwise. In the case of $(1,0,1,0,0) < (1,1,1,0,1)$, one has $\{i_1, i_2\} = \{2, 5\}$. See Figure~\ref{fig:cube_3} for a diagram of the poset $(\ul{2}^3, {\leq})$ (each tuple $\vec \sigma$ is identified with its natural number value $b(\vec\sigma)$ there). A Boolean function $f$ is called \emph{monotonic} iff $f(\vec \sigma) \leq f(\vec \tau)$ whenever $\vec \sigma \leq \vec \tau$. The set of all monotonic functions is denoted by $M$.

\begin{exm}
	We have ${\wedge}, {\vee}, 0^{(1)}, 1^{(1)} \in M$ but ${+}, {\neg} \notin M$. In particular, $1 + 1 = 0 < 1 = 0 + 1$ despite $(1,1) > (0,1)$.
\end{exm}

Why is the set $M$ closed? Let us apply Lemma~\ref{bool:struct} again. Clearly, $M \sbs M$ and $\bot \sbs M$. Assume that $h, g_1, \ldots,g_m \in M$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$. Let $\vec \sigma \leq \vec \tau$. Then we have $g_j(\vec \sigma) \leq g_j(\vec \tau)$ for each $j$, whence $(g_1(\vec \sigma),\ldots, g_m(\vec \sigma)) \leq (g_1(\vec \tau),\ldots, g_m(\vec \tau))$ and $f(\vec \sigma) = h(g_1(\vec \sigma),\ldots, g_m(\vec \sigma)) \leq h(g_1(\vec \tau),\ldots, g_m(\vec \tau)) = f(\vec \tau)$ finally.

\begin{exm}
	Let $f(x,y,z) = x + y + z$. This function $f$ is not computable via $\{{\wedge}, {\vee}, 0^{(1)}, 1^{(1)}\}$. Indeed, the latter functions are monotonic, while $f$ is not: $f(001) = 1 > 0 = f(011)$ but $001 < 011$.
\end{exm}

\begin{rem}
	In the above, we have defined the set $I(\vec \sigma)$ for each tuple $\vec \sigma \in \ul{2}^n$ so that $\vec \tau \in I(\vec\sigma)$ iff $\vec \tau$  may be obtained from $\vec \sigma$ by changing some `$1$'s to `$0$'s. This set has been important for finding a Zhegalkin polynomial for a function. It is clear now that $\vec \tau \in I(\vec \sigma)$ iff $\vec \tau \leq \vec \sigma$.\footnote{The set $I(\vec \sigma)$ is then called the \emph{principal ideal generated by $\vec \sigma$} in the poset $(\ul{2}^n, {\leq})$.} Hence, the formula for Zhegalkin coefficients from Remark~\ref{bool:coeff} turns into $a_{\vec \sigma} = \sum_{\vec\tau \leq \vec \sigma} f(\vec\tau)$, while $f(\vec \sigma) = \sum_{\vec\tau \leq \vec \sigma} a_{\vec\tau}$ by system~(\ref{bool:eq_sle}). This nice symmetry is an example of the so-called \emph{M\"obius Inversion} for finite posets.
\end{rem}
\begin{exc}
	Prove that $(\ul{2}^n, {\leq}) \cong (\mP(\ul{n}), {\sbs})$.
\end{exc}

\paragraph{Duality.}
One more closed set we need is the set $S$ of \emph{self-dual} functions. For $\vec \sigma = (\sigma_1, \ldots, \sigma_n) \in \ul{2}^n$, we put $\neg\vec\sigma = (\neg\sigma_1, \ldots, \neg\sigma_n)$. For example, $\neg (1,0,1) = (0,1,0)$. The function $f^*$ defined by the equality $\neg f^*(\vec \sigma) = f(\neg\vec\sigma)$ for each $\vec \sigma \in \ul{2}^n$ is called the \emph{dual} of a function $f^{(n)}$. Equivalently, one has $f^*(\vec \sigma) = \neg f(\neg\vec\sigma)$ for each $\vec \sigma$. A function $f$ is \emph{self-dual} iff $f^* = f$ or, equivalently, $\neg f(\vec \sigma) = f(\neg\vec\sigma)$ for all $\vec \sigma$. In other words, $f$ is self-dual iff its value is always inverted when all its arguments are. Notice that a function $f$ is \emph{not} self-dual iff there exists a tuple $\vec\sigma$ with $f(\vec \sigma) = f(\neg\vec \sigma)$.

\begin{exc}
	Prove that $(f^*)^* = f$.
\end{exc}

\begin{exm}
	One has $x \wedge y = \neg (\neg x \vee \neg y )$, so $\wedge = \vee^*$ (whence ${\wedge^*}= ({\vee^*})^* = \vee$). As ${\wedge} \neq {\vee}$, neither function is self-dual. Nor ${+}$ is self-dual for $0 + 0 = 1 + 1$. In fact, $x +^* y = \neg(\neg x + \neg y) = 1 + x + 1 + y + 1 = 1 + x + y = x \leftrightarrow y$, so ${+^*} = {\leftrightarrow}$. On the other hand, the functions ${\neg}$ and $\mathrm {maj}$ are self-dual: $\neg \neg x = \neg \neg x$ and $\mathrm {maj}(\neg x, \neg y, \neg z) = \mathrm {maj}(x + 1, y + 1, z + 1) = (x + 1)\cdot(y + 1) + (x + 1)\cdot(z + 1) + (y + 1)\cdot(z + 1) = xy + xz + yz + x + x + y + y + z + z + 1 + 1 + 1 = xy + xz + yz + 1 = \neg \mathrm {maj}(x,y,z)$.
\end{exm}

\begin{exc}
	Prove that there exist exactly $2^{2^{n-1}}$ self-dual functions of $n$ arguments.
\end{exc}

\noindent Let us check that $[S] = S$. At first, we need a useful
\begin{lemma}\label{bool:dual_super}
	Suppose that $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$. Then $f^*(\vec x) = h^*(g^*_1(\vec x),\ldots, g^*_m(\vec x))$ for all $\vec x$, i.\,e.,
	the superposition of some functions' duals equals the dual of their superposition.
\end{lemma}
\begin{proof}
	$$
	f^*(\vec x) = \neg f (\neg\vec x) =  \neg h(g_1(\neg \vec x),\ldots,g_m(\neg \vec x)) =\\
	\neg h(\neg g^*_1(\vec x),\ldots,\neg g^*_m(\vec x)) = h^*(g^*_1(\vec x),\ldots,g^*_m(\vec x)).
	$$
\end{proof}
Now, apply Lemma~\ref{bool:struct}. If $p \in \bot$, we get $\neg p(\vec \sigma) = \neg \sigma_j = p(\neg\vec\sigma)$, whence $p \in S$. Assume that $h, g_1, \ldots,g_m \in S$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$. Then $f^*(\vec x) = h^*(g^*_1(\vec x),\ldots,g^*_m(\vec x)) = h(g_1(\vec x),\ldots, g_m(\vec x)) = f(\vec x)$ by Lemma~\ref{bool:dual_super}. So, $f \in S$.

\begin{exm}
	We have ${\to} \notin [\mathrm{maj}, {\neg}]$. Indeed, $[\mathrm{maj}, {\neg}] \sbs [S] = S$, though ${\to} \notin S$ for $0 \to 0 = 1 \to 1$.
\end{exm}

Let $C$ be a circuit over a set $Q$ computing a function $f$. What if we change every gate in $C$ to its dual to obtain a circuit $C^*$? It seems clear that $C^*$ computes $f^*$. But let us elaborate on this point. For a set $Q$ of Boolean functions, we put $Q^* = \{ f^* \in \top \mid f \in Q \}$. The set $Q^*$ is thus \emph{dual} to $Q$.
\begin{lemma}\label{bool:dual_closure} For any Boolean function sets $P$ and $Q$,
	\begin{enumerate}
		\item $(Q^*)^* = Q$;
		\item if $P \sbs Q$, then $P^* \sbs Q^*$;
		\item $[Q]^* = [Q^*]$;
		\item if $Q$ is a basis of $P$, then $Q^*$ is a basis of $P^*$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	The first and second claims are obvious. For the third one, let us first prove $[Q] \sbs [Q^*]^*$ for an arbitrary $Q$.
	
	We can do it by applying Lemma~\ref{bool:struct}. One has $Q^* \sbs [Q^*]$ by Theorem~\ref{bool:closure}, whence $Q = (Q^*)^* \sbs [Q^*]^*$ by the previous claims. For projectors, we get $\bot = [\void] \sbs [Q^*]$ by Theorem~\ref{bool:closure}. As we know, projectors are self-dual; hence $\bot = \bot^* \sbs [Q^*]^*$ by the second claim. Assume now that $h, g_1, \ldots, g_m \in [Q^*]^*$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$. By Lemma~\ref{bool:dual_super}, $f^*(\vec x) = h^*(g^*_1(\vec x),\ldots, g^*_m(\vec x))$ for each $\vec x$. On the other hand, $h^*, g^*_1, \ldots, g^*_m \in ([Q^*]^*)^* = [Q^*]$. Then clearly $f^* \in [[Q^*]] = [Q^*]$ (see the proof of Theorem~\ref{bool:closure} if in doubt), whence $f = (f^*)^* \in [Q^*]^*$.
	
	Finally, $[Q]^* \sbs ([Q^*]^*)^* = [Q^*]$ by the previous claims, and $[Q^*] \sbs [(Q^*)^*]^* = [Q]^*$ (as $Q$ is arbitrary, we may substitute $Q^*$ for $Q$).
	
	For the last claim, assume that $[Q] = P$ but $[R] \subsetneq P$ for any $R \subsetneq Q$. Then $[Q^*] = [Q]^* = P^*$, so $Q^*$ is complete in $P^*$. Suppose that $P^* = [R]$ for some $R \subsetneq Q^*$. Hence $[R^*] = [R]^* = (P^*)^* = P$, whereas $R^* \subsetneq (Q^*)^* = Q$ clearly. The contradiction shows that $Q^*$ is a basis of $P^*$.
\end{proof}

\begin{exc}
	Prove that $S^* = S$.
\end{exc}
\begin{exm}\label{bool:const_dual}
	Consider the sets $P^*_0$ and $P^*_1$. If $f \in P_0$, one has $f^*(\vec 1) = \neg f (\neg \vec 1) = \neg f (\vec 0) = \neg 0 = 1$, that is, $f^* \in P_1$. So, $P^*_0 \sbs P_1$. One can likewise prove $P^*_1 \sbs P_0$. Then $P_0 = (P^*_0)^* \sbs P^*_1 \sbs P_0$, whence $P^*_1 = P_0$, and similarly, $P^*_0 = P_1$.
\end{exm}

\paragraph{Linear functions.}
The set $L$ of \emph{linear} functions may be defined as $[{+}, 1^{(1)}]$. On the one hand, $L$  is clearly closed by Theorem~\ref{bool:closure}. On the other hand, this characterization is not nice as we see no interesting property of the \emph{functions} from $L$ themselves (but rather of their circuits). It is therefore easy to prove a function belongs to $L$---just give a suitable circuit, while it seems somewhat hard to prove it does not: \emph{why} cannot we have a circuit?

But things are much better here in view of Theorem~\ref{bool:zhegalkin}. Indeed, for each function $f^{(n)}$, its Zhegalkin coefficients $a_{\vec\sigma}$ are unique. Let us prove that $f \in L$ iff $a_{\vec\sigma} = 0$ for each tuple $\vec\sigma$ with $||\vec\sigma|| \geq 2$ (that is, with two or more `$1$'s) or, equivalently, the equation
\begin{equation}\label{bool:eq_lin}
	f(\vec x) = a_{00\ldots0}\, 1 + a_{10\ldots0}\, x_1 + a_{010\ldots0}\, x_2 + \ldots + a_{0\ldots01}\, x_n
\end{equation}
holds for each $\vec x$. If it indeed holds, one can easily construct a circuit for $f$ over $\{{+}, 1^{(1)}\}$, so $f \in [{+}, 1^{(1)}] = L$. For the other direction, we will prove that for each $f^{(n)} \in [{+}, 1^{(1)}]$, there exists a tuple $(b_0, b_1, \ldots, b_n) \in \ul{2}^{n + 1}$  such that
\begin{equation}\label{bool:eq_lin1}
	f(\vec x) = b_0\, 1 + b_1\, x_1 + b_2\, x_2 + \ldots + b_n\, x_n.
\end{equation}
By structural induction on $f$ (Lemma~\ref{bool:struct}). When $f \in \{{+}, 1^{(1)}\} \cup \bot$, this is obvious. Assume that $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$ and
$$
\begin{array}{rcl}
	h(\vec y) &=& c_0\, 1 + c_1\, y_1 + \ldots + c_m\, y_m\\
	g_1(\vec x) &=& d^1_0\, 1 + d^1_1\, x_1 + \ldots + d^1_n\, x_n\\ 
	&\ldots&\\
	g_m(\vec x) &=& d^m_0\, 1 + d^m_1\, x_1 + \ldots + d^m_n\, x_n.\\ 
\end{array}
$$
Then
\begin{multline*}
	f(\vec x) = c_0\, 1 + c_1\, g_1(\vec x) + \ldots + c_m\, g_m(\vec x) = (c_0 + c_1 d^1_0 + \ldots + c_m d^m_0) \cdot 1 +\phantom{1}\\
	(c_1 d^1_1 + \ldots + c_m d^m_1) \cdot x_1 + \ldots + (c_1 d^1_n + \ldots + c_m d^m_n) \cdot x_n,
\end{multline*}
whence the coefficients $b_i$ are clear.

Equation~(\ref{bool:eq_lin1}) gives a Zhegalkin polynomial for $f$. Since its coefficients are uniquely determined, there must be $b_0 = a_{00\ldots0}$, $b_1 = a_{10\ldots0}$, $b_2 = a_{010\ldots0}$, \dots, $b_n = a_{0\ldots01}$. Hence, the required equation~(\ref{bool:eq_lin}) holds for $f$.

\begin{exm}
	In practice, this means that checking $f \in L$ boils down to computing Zhegalkin coefficients for $f$. The functions $\neg$ and $\leftrightarrow$ are linear as $\neg x = 1 + x$ and $x \leftrightarrow y = 1 + x + y$, whereas $\wedge$ and $\mathrm{maj}$ are not: $x \wedge y = x y$ and $\mathrm{maj}(x,y,z) = xy + xz + yz$. Notice that just one coefficient $a_{\vec\sigma} = 1$ with $||\vec\sigma|| \geq 2$ is sufficient to prove $f \notin L$.
\end{exm}

\begin{exm}
	The function $\wedge$ is not expressible via $\{{+}, 1^{(1)}\}$ for it is not linear.
\end{exm}

\begin{exc}
	Prove that $L^* = L$.
\end{exc}

\begin{exc}
	There is a description of $L$ which does not mention circuits, Zhegalkin polynomials nor any other Boolean function representation. Let $(\vec x; a/i)$ stand of the tuple $(x_1,\ldots,x_{i-1},a, x_i,\ldots, x_n)$. Then a function $f^{(n)} \in L$ iff for each $i \leq n$,
	from $\exists \vec a\; f(\vec a; 0/i) = f(\vec a; 1/i)$, it follows that $\forall \vec b\; f(\vec b; 0/i) = f(\vec b; 1/i)$. In other words, linearity means that if $f$ is \emph{not always dependent} on its $i$-th argument, then $f$ is \emph{always independent} of it. In view of this description, functions from $L$ are also known as \emph{affine} functions.
\end{exc}

\medskip

\begin{exm}\label{bool:in_clones}
	The following table summarizes our results on whether important functions belong to $P_0$, $P_1$, $M$, $S$, or $L$. We put `+' into the respective cell if a function belongs to a set, and put `--' otherwise. 
	\begin{center}
		{\large
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				&$P_0$ & $P_1$ & $M$ & $S$ & $L$\\
				\hline
				$0^{(1)}$&+&--&+&--&+\\
				\hline
				$1^{(1)}$&--&+&+&--&+\\
				\hline
				$\neg$&--&--&--&+&+\\
				\hline
				$\wedge$&+&+&+&--&--\\
				\hline
				$\vee$&+&+&+&--&--\\
				\hline
				$+$&+&--&--&--&+\\
				\hline
				$\to$&--&+&--&--&--\\
				\hline
				$\leftrightarrow$&--&+&--&--&+\\
				\hline
				$\mathrm{maj}$&+&+&+&+&--\\
				\hline
				$\dvd$&--&--&--&--&--\\
				\hline
			\end{tabular}
		}
	\end{center}
	Notice that the Sheffer stroke ${\dvd}$ belongs to none of $P_0, P_1, M, S, L$. It is no coincidence that $[\,{\dvd}\,] = \top$, as we shall learn from Post's Criterion below.
\end{exm}

\paragraph{Finding bases.}
As we have already mentioned, every closed set of Boolean functions has a finite basis. Let us find these for some of the sets.

\begin{exm}
	For every function $f$, we have $f(00\ldots0) = a_{00\ldots0}$ by equation~(\ref{bool:eq_poly}). That is, $f \in P_0$ iff $a_{00\ldots0} = 0$. The latter implies that the polynomial for $f$ is a circuit over $\{{\wedge}, {+}\}$, so $P_0 \sbs [{\wedge}, {+}]$. As $\{{\wedge}, {+}\} \sbs P_0$, we have $[{\wedge}, {+}] = P_0$ finally. Furthermore, ${\wedge} \in P_1$ but ${+} \notin P_1$; hence ${+} \notin [{\wedge}]$ and $P_0 \not\sbs [{\wedge}]$. Likewise, ${+} \in L$ but ${\wedge} \notin L$, whence $P_0 \not\sbs [{+}]$. Therefore, the set $\{{\wedge}, {+}\}$ is a basis for $P_0$.
	
	By Lemma~\ref{bool:dnf_compl} and Example~\ref{bool:const_dual}, the set $\{{\wedge^*}, {+^*}\} = \{{\vee}, {\leftrightarrow}\}$ is a basis for $P_1 = P^*_0$.
\end{exm}

A function $f^{(n)}$ is called \emph{conjunctive} iff $f(\vec \sigma \wedge \vec\tau) = f(\vec\sigma) \wedge f(\vec\tau)$, where $\vec \sigma \wedge \vec\tau = (\sigma_1 \wedge \tau_1, \ldots, \sigma_n \wedge \tau_n)$, for every $\vec\sigma, \vec\tau \in \ul{2}^n$. Let us denote the set of all conjunctive functions by $\bigwedge$. Clearly, ${\wedge}, 0^{(1)}, 1^{(1)} \in {\bigwedge}$, while ${\vee} \notin {\bigwedge}$. Indeed, for $\vec \sigma = (1, 0)$ and $\vec \tau = (0, 1)$, we get
$$\begin{array}{rcccl}
	(\sigma_1 \wedge \tau_1) \vee (\sigma_2 \wedge \tau_2) &=& (1 \wedge 0) \vee (0 \wedge 1) &=& 0;\\
	(\sigma_1 \vee \sigma_2) \wedge (\tau_1 \vee \tau_2) &=& (1 \vee 0) \wedge (0 \vee 1) &=& 1.\\
\end{array}$$
The set $\bigwedge$ is closed. We routinely apply structural induction (Lemma~\ref{bool:struct}) to show this. Clearly, $\bot \sbs {\bigwedge}$. Assume that $h, g_1, \ldots,g_m \in {\bigwedge}$ and $f(\vec x) = h(g_1(\vec x),\ldots, g_m(\vec x))$ for all $\vec x$. Then
\begin{multline*}
	f(\vec \sigma \wedge \vec\tau) = h(g_1(\vec \sigma \wedge \vec\tau),\ldots, g_m(\vec \sigma \wedge \vec\tau)) =
	h(g_1(\vec\sigma) \wedge g_1(\vec\tau),\ldots,g_m(\vec\sigma) \wedge g_m(\vec\tau)) =\\
	h(g_1(\vec \sigma),\ldots, g_m(\vec \sigma)) \wedge h(g_1(\vec \tau),\ldots, g_m(\vec \tau)) = f(\vec\sigma) \wedge f(\vec\tau).
\end{multline*}
Every conjunctive function $f$ is monotonic. Indeed, let $\vec \sigma \leq \vec\tau$. Then $\sigma_i \wedge \tau_i = \sigma_i$ for each $i$ (cf. Remark~\ref{bool:ord_field}), whence $f(\vec \sigma) = f(\vec \sigma \wedge \vec\tau) = f(\vec\sigma) \wedge f(\vec\tau) \leq f(\vec{\tau})$. So, ${\bigwedge} \sbs M$.

For each Boolean function $f^{(n)}$, consider the set $N_f = \min_{\leq}\, \{ \vec{\sigma} \in \ul{2}^n \mid f(\vec{\sigma}) = 1 \}$, which is called the set of \emph{lower units} for $f$. For example, $N_{\wedge} = \{11\}$, $N_{\vee} =\{01, 10\}$, $N_{1^{(1)}} = \{0\}$, and $N_{0^{(1)}} = \void$. Clearly, $N_f$ is an antichain in $(\ul{2}^n, {\leq})$. This set is mainly interesting when $f \in M$.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.7\textwidth]{lower_units.pdf}
	\caption{For the function $\mathrm{maj}$, its \emph{units} (i.\,e., tuples $\vec{\sigma}$ with $\mathrm{maj}(\vec \sigma) = 1$) are shown in red. The \emph{lower units} are encircled.}
\end{figure}

\begin{lemma}\label{bool:mono_units}
	Let $f \in M$. Then for each $\vec \tau$, $f(\vec \tau) = 1$ iff there exists $\vec{\sigma} \in N_f$ such that $\vec\sigma \leq \vec \tau$.
\end{lemma}
\begin{proof}
	If there is such a tuple $\vec{\sigma}$, obtain $f(\vec \tau) = 1$ by monotonicity. For the other direction, it suffices to prove a general fact: every element of a finite poset (of $f$-units, in this case) is greater or equal than some \emph{minimal} element thereof (a lower unit). Assume that a finite poset $\mathcal A = (A, <)$ has just $m$ elements and $x_0 \in A$. If $x_0$ is \emph{not} greater nor equal than a minimum, then $x_0 \notin \min \mathcal A$, so there exists $x_1 < x_0$. Nor $x_1$ may be minimal, whence $x_2 < x_1 < x_0$ for some $x_2$. Iterating this argument $m$ times, we obtain $x_m < x_{m-1} < \ldots < x_1 < x_0$. By transitivity and irreflexivity, all $x_i$ are pairwise distinct, whence $|A| \geq m + 1$. A contradiction.
\end{proof}

\begin{corr}\label{bool:monot_monom}
	For every $f, g \in M$, it holds that $N_f = N_g$ iff $f = g$. That is, a monotonic function is uniquely determined by its lower units. Moreover, let $N_f = \{ \vec \sigma^1, \ldots,  \vec \sigma^k \}$, $k > 0$, and let $\mu_{\vec \sigma}$ be the monomial encoded by $\vec \sigma$ (as $101$ encodes $x_1 x_3$, etc.). Then $f(\vec x) = \mu_{\vec \sigma^1} \vee \ldots \vee \mu_{\vec \sigma^k}$ for each $\vec x$. Of course, $f(\vec x) = 0$ if $N_f = \void$.
\end{corr}
\begin{proof}
	It is clear that $\mu_{\vec \sigma}(\vec \tau) = 1$ iff $\vec \sigma \leq \vec \tau$ (cf.~the proof of Theorem~\ref{bool:zhegalkin}). Denote the expression  $\mu_{\vec \sigma^1} \vee \ldots \vee \mu_{\vec \sigma^k}$ by $\mu$. Applying Lemma~\ref{bool:mono_units}, we get
	$$
	\begin{array}{rcl}
		\mu(\vec x) = 1 &\iff& \exists j\: \mu_{\vec\sigma^j}(\vec x) = 1\\
		&\iff& \exists j\: \vec\sigma^j \leq \vec x\\
		&\iff& \exists \vec \sigma \in N_f\ \vec\sigma \leq \vec x\\
		&\iff& f(\vec x) = 1.
	\end{array}
	$$
\end{proof}

\begin{exm}
	As $N_{\mathrm{maj}} = \{011, 101, 110\}$, we have $\mathrm{maj}(x,y,z) = xy \vee xz \vee yz$.
\end{exm}

\begin{exm}
	Every expression of the form $\mu_{\vec \sigma}$ is a disjunction of monomials; hence, it is computable over $\{1^{(1)}, {\wedge}, {\vee} \}$. Allowing $N_f$ to be empty as well, we may conclude that $M = [0^{(1)}, 1^{(1)}, {\wedge},  {\vee}]$ and, of course, ${\bigwedge} \sbs [0^{(1)}, 1^{(1)}, {\wedge},  {\vee}]$.
	
	It is possible strengthen this result for the set ${\bigwedge}$. First, we claim that a monotonic function $f$ is conjunctive iff $|N_f| \leq 1$. Indeed, assume that $|N_f| > 1$. There are two distinct tuples $\vec{\sigma}, \vec{\tau}$ in $N_f$; they must be ${\leq}$-incomparable, whence $\sigma_i = 1$ and $\tau_i = 0$ for a certain $i$ (otherwise, $\vec \sigma \leq \vec \tau$). Let $\vec \rho = \vec \sigma \wedge \vec \tau$. Clearly, $\vec \rho \leq \vec \sigma$. We have $\rho_i = 0$, whence $\vec \rho \neq \vec \sigma$. By Lemma~\ref{bool:mono_units}, there should be $\vec\xi \in N_f$ with $\xi \leq \vec \rho$ if $f(\vec \rho) = 1$, that is, $\vec \xi \leq \vec \rho < \vec \sigma$ and $\vec \sigma$ is not a lower unit. This contradiction proves $f(\vec \sigma \wedge \vec \tau) = f(\vec \rho) = 0$, but $f(\vec \sigma) \wedge f(\vec \tau) = 1 \wedge 1 = 1$. Hence, $f \notin {\bigwedge}$.
	
	For the other direction, suppose that $|N_f| \leq 1$. If $|N_f| = 0$, then $N_f = \void$ and $f = 0^{(1)} \in {\bigwedge}$. Let $N_f = \{ \vec \rho \}$. Consider arbitrary tuples $\vec \sigma$ and $\vec \tau$. Applying Lemma~\ref{bool:mono_units} and the fact that conjunction returns an ${\leq}$-infimum (see Remark~\ref{bool:ord_field}), we get
	$$
	\begin{array}{rcl}
		f(\vec \sigma) \wedge f(\vec \tau) = 1 &\iff& f(\vec \sigma) = f(\vec \tau) = 1\\
		&\iff& \vec \rho \leq \vec \sigma\ \mbox{and}\ \vec \rho \leq \vec \tau\\
		&\iff& \vec \rho \leq \vec \sigma \wedge \vec \tau\\
		&\iff& f(\vec \sigma \wedge \vec \tau) = 1.
	\end{array}
	$$
	Thus, $f \in {\bigwedge}$.
	
	Finally, given $f$ is conjunctive, from $|N_f| \leq 1$ and Corollary~\ref{bool:monot_monom}, it follows that $f(\vec x)$ equals either $0$ or \emph{one} monomial $\mu_{\vec \sigma}$ (when $N_f = \{\vec \sigma\}$). So, $f \in [0^{(1)}, 1^{(1)}, {\wedge}]$ and ${\bigwedge} = [0^{(1)}, 1^{(1)}, {\wedge}]$.
\end{exm}

\begin{exm}
	Is the set $\{0^{(1)}, 1^{(1)}, {\wedge},  {\vee}\}$ a basis for $M$? In fact, it is since no lesser set suffices: we have $0^{(1)} \notin [1^{(1)}, {\wedge},  {\vee}]$ as $0^{(1)} \notin P_1$ but $[1^{(1)}, {\wedge},  {\vee}] \sbs P_1$; $1^{(1)} \notin [0^{(1)}, {\wedge},  {\vee}]$ as $1^{(1)} \notin P_0$ but $[0^{(1)}, {\wedge},  {\vee}] \sbs P_0$; ${\vee} \notin [0^{(1)}, 1^{(1)}, {\wedge}]$ as ${\vee} \notin {\bigwedge}$ but $[0^{(1)}, 1^{(1)}, {\wedge}] \sbs {\bigwedge}$.
	
	And what about ${\wedge}$? Is there any obstacle preventing it from being expressible via $\{0^{(1)}, 1^{(1)}, {\vee}\}$? As the Reader could expect, this obstacle should be the set ${\bigvee}$ of \emph{disjunctive} functions $\{ f \in \top \mid \forall \vec \sigma\, \forall \vec \tau\  f(\vec \sigma \vee \vec \tau) = f(\vec \sigma) \vee f(\vec \tau) \}$. We leave finishing this argument to the Reader.
	
	From all these considerations, it should be clear now that $\{0^{(1)}, 1^{(1)}, {\wedge}\}$ is a basis in ${\bigwedge}$. In particular, ${\wedge} \notin [0^{(1)}, 1^{(1)}]$ as the functions $0^{(1)}$ and $1^{(1)}$ are linear, whereas ${\wedge}$ is not.
\end{exm}

\begin{exc}
	Prove that ${\bigwedge}^* = {\bigvee}$ and $M^* = M$.
\end{exc}

\begin{exc}
	Prove that the set ${\bigvee}$ is closed and $\{0^{(1)}, 1^{(1)}, {\vee}\}$ is one of its bases.
\end{exc}

\begin{exc}
	For any natural $k \leq n$, there are at least $2^{C_n^k}$ monotonic functions of $n$ arguments. In order to prove this, think of how large the set $N_f$ may be.
\end{exc}

\begin{exc}
	Prove the identity $f(y, \vec x) = (y \wedge f(1, \vec x)) \vee f(0, \vec x)$ for each $f^{(n)} \in M$. Then use it to establish $M \sbs [0^{(1)}, 1^{(1)}, {\wedge},  {\vee}]$ by induction on $n$, similarly to Remark~\ref{bool:func_var}.
\end{exc}


\paragraph{Post's Criterion.} Now, we are ready to provide a nice characterization for all such sets $Q$ that $[Q] = \top$.

\begin{thm}[Post's Criterion of Functional Completeness]\label{bool:post}
	For every $Q \sbs \top$, it holds that $[Q] = \top$ iff $Q$ is \emph{not included} to any of the sets $P_0, P_1, M, S, L$.
\end{thm}
\begin{proof}
	Assume that $Q \sbs R$ for some $R \in \{P_0, P_1, M, D, L\}$. Then $[Q] \sbs [R] = R$, whereas $R  \subsetneq \top$ for each $R$ as Example~\ref{bool:in_clones} shows. Hence, $[Q] \neq \top$.
	
	For the other direction, assume that $Q \not\sbs R$ for any $R \in \{P_0, P_1, M, D, L\}$. It follows that $Q$ contains functions $f_0 \notin P_0$, $f_1 \notin P_1$, $f_M \notin M$, $f_S \notin S$, and $f_L \notin L$. (Some of these may coincide.) It suffices to prove that $\{{\neg}, {\wedge} \} \sbs [Q]$ for $[{\neg}, {\wedge} ] = \top$.
	
	Our first goal is to establish $\{{\neg}, 0^{(1)}, 1^{(1)} \} \sbs [Q]$. Consider the function $g_0$ such that $g_0(x) = f_{0}(x, \ldots, x)$. We clearly have $g_0 \in [Q]$. Furthermore, $g_0(0) = f_0(\vec 0) = 1$ as $f_0 \notin P_{0}$. If $g_0(1) = 1$, then $g_0 = 1^{(0)}$. Otherwise, $g_0(1) = 0$ and $g_0 = {\neg}$. A similar argument for the function $g_1 \in [Q]$ with $g_1(x) = f_{1}(x, \ldots, x)$ yields $g_1 = 0^{(1)}$ or $g_1 = {\neg}$.
	
	Two cases are possible now. We either have $\{1^{(1)}, 0^{(1)}\} = \{g_0, g_1\} \sbs [Q]$ or ${\neg} \in \{g_0, g_1\} \sbs [Q]$.
	
	Suppose that $\{1^{(1)}, 0^{(1)}\} \sbs [Q]$. Consider the function $f^{(m)}_M \in Q \setminus M$. There exist tuples $\vec\sigma, \vec\tau$ such that $\vec \sigma \leq \vec \tau \in \ul{2}^m$ but $f_M(\vec\sigma) > f_M(\vec\tau)$ (that is, $f_M(\vec \sigma) = 1$ and $f_M(\vec{\tau}) = 0$). As $\vec \sigma \neq \vec \tau$, obtain $\vec \sigma < \vec \tau$, that is, there exist indices $i_1 <  i_2 < \ldots < i_k$, $k > 0$, such that $\sigma_j < \tau_j$ (whence $\sigma_j = 0$ and $\tau_j = 1$) when $j = i_s$ for some $s \leq k$ and $\sigma_j = \tau_j$ otherwise. Now, let
	$$g_M(x) = f_M(\sigma_1, \ldots, \sigma_{i_1 - 1}, x, \sigma_{i_1 + 1}, \ldots, \sigma_{i_2 - 1}, x,  \sigma_{i_2 + 1}, \ldots, \sigma_{i_k - 1}, x,  \sigma_{i_2 + 1}, \ldots, \sigma_m).$$
	For example, if $\vec\sigma = (1,0,0,1,1,0,0)$ and $\vec\tau = (1,0,1,1,1,1,1)$, we shall get $g_M(x) = f_M(1, 0, x, 1, 1, x, x)$. Clearly, $g_M \in [f_M, 0^{(1)}, 1^{(1)}] \sbs [Q]$. On the other hand, $g_M(0) = f_M(\vec\sigma) = 1$ and $g_M(1) = f_M(\vec{\tau}) = 0$. Thus, $g_M = {\neg}$ and $\{{\neg}, 1^{(1)}, 0^{(1)}\} \sbs [Q]$.
	
	Now, suppose that $\neg \in [Q]$. Consider the function $f^{(s)}_S \in Q \setminus S$. We have $f(\vec{\sigma}) = f(\neg\vec{\sigma})$ for a certain $\vec\sigma \in \ul{2}^s$. Let 
	$$g_S(x) = f_S(x^{\sigma_1},\ldots,x^{\sigma_s}).$$
	Recall that $x^1$ stands for $x$ and $x^0$ for $\neg x$. E.\,g., one has $g_S(x) = f_S(x, x, \neg x, x, \neg x)$ if $\sigma = (1,1,0,1,0)$. It is obvious that $g_S \in [f_S, {\neg}] \sbs [Q]$. Moreover, $g_S(1) = f_S(\vec \sigma) = f_S(\neg\vec \sigma) = g_S(0)$. Thus, $g_S  \in \{0^{(1)}, 1^{(1)}\}$. Given one constant, we may obtain the other one by applying negation. Therefore, $\{{\neg}, 1^{(1)}, 0^{(1)}\} \sbs [Q]$ in this case as well.
	
	Now, we shall prove ${\wedge} \in [f_L, {\neg}, 1^{(1)}, 0^{(1)}] \sbs  [Q]$. Since the function $f^{(l)}_L$ is not linear, its Zhegalkin coefficient $a_{\vec\sigma} = 1$ for at least one tuple $\vec \sigma$ with $||\vec{\sigma}|| \geq 2$. W.\,l.\,o.\,g., let $\sigma_1 = 1 = \sigma_2$ and $\vec{\sigma} = 11\vec{\rho}$ for some $\vec{\rho}$. By grouping the monomials which contain $x_1 x_2$ in the equation~(\ref{bool:eq_poly}) together, we then obtain
	\begin{multline*}
		f_L(\vec x) = x_1 x_2 (a_{110\ldots0}\, 1 + a_{1110\ldots0}\, x_3 + a_{11010\ldots0}\, x_4 + \ldots + a_{111\ldots1}\, x_3 x_4 \ldots x_l) +\phantom{b}\\
		a_{00\ldots0}\, 1 + a_{10\ldots0}\, x_1 + a_{010\ldots0}\, x_2 + \ldots + a_{0\ldots01}\, x_l + a_{101\ldots0}\, x_1 x_3 + \ldots + a_{001\ldots 1}\, x_3\ldots x_l =\phantom{b}\\
		= x_1 x_2 \cdot P(x_3, \ldots, x_l) + Q(\vec x).
	\end{multline*}
	The value of the polynomial $P$ is \emph{not always} zero, for otherwise one could change every $a_{11\vec{\tau}}$ to $0$ and obtain a \emph{different} polynomial for $f_L$ (since $a_{11\vec{\rho}} = 1$), contrary to Theorem~\ref{bool:zhegalkin}. Also notice that the polynomial $Q$ has no monomial containing both $x_1$ and $x_2$. So, there exists a tuple $\vec{\tau}$ such that $P(\vec{\tau}) = 1$; by grouping together the terms with $x_1$, $x_2$, and none of those variables, respectively, in $Q$ we obtain
	$$f_L(x_1,x_2,\vec{\tau}) = x_1 x_2 + Q(x_1, x_2, \vec \tau) = x_1 x_2 + a x_1 + b x_2 + c$$
	for some $a, b, c \in \ul{2}$. Let $g_L(x_1, x_2) = f_L(x_1,x_2,\vec{\tau})$. We clearly have $g_L \in [f_L, 1^{(1)}, 0^{(1)}] \sbs [Q]$. Our last goal is to obtain conjunction from $g_L$. Let
	\begin{multline*}
		g(x_1, x_2) = g_L(x_1 + b, x_2 + a) + ab + c =\\
		(x_1 + b)\cdot(x_2 + a) + a(x_1 + b) + b(x_2 + a) + c + ab + c =\\
		x_1 x_2 + a x_1 + b x_2 + ab + a x_1 + ab + b x_2 + ab + ab =
		x_1 x_2 = x_1 \wedge x_2.
	\end{multline*}
	So, $g = {\wedge}$. On the other hand, computing $g$ requires adding a constant to a variable; this boils down to applying negation for $x + 0 = x$ and $x + 1 = \neg x$. For example, we have $g(x_1, x_2) = \neg g_L(x_1, \neg x_2)$ when $(a,b,c) = (1, 0, 1)$. Therefore, ${\wedge} = g \in [g_L, {\neg}] \sbs [Q]$. Finally, $\{{\neg}, {\wedge}\} \sbs [Q]$, as it was required.
\end{proof}

\begin{exm}
	Let $Q = \{1^{(1)},{\neg},{+},\mathrm{even}^{(3)},\mathrm{maj}\}$, where the function $\mathrm{even}$ takes $1$ iff the number of `$1$'s among its arguments is \emph{even}. In particular, $\mathrm{even}(0, 0, 0) = \mathrm{even}(1, 0, 1) = 1$. It is easy to see that $\mathrm{even}(x_1,x_2,x_3) = x_1 + x_2 + x_3 +1$. Let us check whether $Q$ is complete in $\top$ and express $\vee$ over $Q$ if it is possible.
	
	Let us make a table similar to that of Example~\ref{bool:in_clones}. By Theorem~\ref{bool:post}, the set $Q$ is complete in $\top$ iff there is a `--' in each column. So we may spare our effort and leave some cells blank.
	\begin{center}
		{\large
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				&$P_0$ & $P_1$ & $M$ & $S$ & $L$\\
				\hline
				$1^{(1)}$&--&+&+&--&+\\
				\hline
				$\neg$&&--&--&&+\\
				\hline
				$+$&&&&&+\\
				\hline
				$\mathrm{even}$&&&&&+\\
				\hline
				$\mathrm{maj}$&&&&&--\\
				\hline
			\end{tabular}
		}
	\end{center}
	It appears that $Q$ is complete in $\top$. Hence, ${\vee} \in [Q]$. How can we make this explicit? As ${\vee}$ is non-linear, we need $\mathrm{maj} \in Q \setminus L$ in order to compute it. We have $\mathrm{maj}(x_1, x_2, x_3) = x_1 x_2 + x_1 x_3 + x_2 x_3$ and $x_1 \vee x_2 = x_1 x_2 + x_1 + x_2$, whence
	$x_1 \vee x_2 = \mathrm{maj}(x_1,x_2,1(x_3))$.
\end{exm}
\begin{exc}
	It is clear that neither ${+}$ nor $\mathrm{even}$ are necessary for the set $Q$ to be complete in $\top$. Fill all the cells in and find all bases of $\top$ among the subsets of $Q$.
\end{exc}
\begin{exc}
	No basis of $\top$ has more than four elements. (Suppose a function does not preserve a constant. May it be both monotonic and self-dual?)
\end{exc}


The sets $P_0, P_1, M, S, L$ are indeed unique with respect to completeness in $\top$. We say that a set $Q$ is \emph{precomplete} (in $\top$) iff $[Q] \neq \top$ but $[Q \cup \{f\}] = \top$ for every $f \notin Q$.
\begin{exc}
	None of the sets $P_0, P_1, M, S, L$ is included into another one. Each of these sets is precomplete in $\top$.
\end{exc}
\begin{exc}
	Every precomplete set is closed. Every closed set except $\top$ itself is a subset of a precomplete set. The sets $P_0, P_1, M, S, L$ are only precomplete in $\top$.
\end{exc}

Post's Criterion is sometimes instrumental in finding bases of various closed sets. The main idea is typically as follows. Assume we want to prove $Q = [R]$. We may then add more functions to $R$ to entail $[R'] = \top$ where $R \subsetneq R'$. Finally, we look at a circuit over $R'$ for $f \in Q$ (or a related function) and try to eliminate every element of $R' \setminus R$ therefrom---in such a way that the circuit will still compute $f$.

\begin{exm}
	Let us prove that $R = \{\neg, \mathrm{maj}\}$ is a basis of $S$. It is clear that $[R] \sbs S$. Moreover, $\neg \in L$ but $\mathrm{maj} \notin L$, whence $\mathrm{maj} \notin [{\neg}]$; $\mathrm{maj} \in P_0$ but $\neg \notin P_0$, whence ${\neg} \notin [\mathrm{maj}]$. It remains to prove that $S \sbs [R]$.
	
	From Theorem~\ref{bool:post}, it follows that the set $R' = \{{\neg}, \mathrm{maj}, 0^{(1)}\}$ is complete in $\top$. Consider an arbitrary $f^{(n + 1)} \in S$. If $n = 0$, we have either $f = {\neg}$ or $f = \id_{\ul{2}} \in \bot$, whence $f \in [R]$. Assume $n > 0$. Let $g(\vec x) = f(0, \vec x)$ for each $\vec x \in \ul{2}^{n}$ and $C$ be a circuit of $(x_1, \ldots, x_n)$ over $R'$ computing $g^{(n)}$.
	
	We change $C$ in the following way: replace every assignment of the form $t_i = 0(t_j)$ with $t_i = y$, where $y$ is a new input variable. This procedure results in a circuit $D$ of $(y,x_1, \ldots, x_n)$ over $R$. The latter computes a certain function $h^{(n+1)} = g_D \in [R]$, so $h \in S$.
	
	It is easy to see that $h(0, \vec x) = g(\vec x) = f(0, \vec x)$ for each $\vec x$ since the circuits $D$ and $C$ compute identical functions at inputs $(0, x_1, \ldots, x_n)$ and $(x_1, \ldots, x_n)$, respectively. (For a formal argument, one has to define $D$ by recursion on $\sz(C)$ and prove the claim by induction on $\sz(C)$.)
	
	On the other hand, 
	$h(1, \vec x) = \neg h(\neg 1, \neg \vec x) = \neg h (0, \neg \vec x) = \neg f(0, \neg \vec x) = \neg f(\neg 1, \neg \vec x) = f(1, \vec x),$
	by the previous equation and the fact that $f, h \in S$. Hence, $f(y, \vec x) = h(y, \vec x)$ for all $y, \vec x$ and $f = h \in [R]$.
\end{exm}
\begin{exc}
	Find a basis of size $1$ for the set $S$.
\end{exc}

\begin{exc} Consider the following \emph{conditional operator} ${?}{:}$ (also known as \emph{conditioned disjunction}) such that 
	$$\begin{array}{rcl}
		x\mathrel{?}y\mathrel{:}z &=& \begin{cases}
			y\ \mbox{if}\ x = 1;\\
			z\ \mbox{if}\ x = 0
		\end{cases}
	\end{array}$$
	for all $x,y,z \in \ul{2}$.
	Prove that  $[\,{?}{:}\,] = P_0 \cap P_1$. (Try adding constants to this operator.)
\end{exc}

Let $T^\infty_1 = \{ f\in \top \mid \exists i\, \forall \vec \sigma\ f(\vec \sigma) \geq \sigma_i \}$, that is, the set $T^\infty_1$ consists of functions bounded below by an argument. For example, one has ${\vee}, {\to} \in T^\infty_1$ as $x_1 \vee x_2 \geq x_i$ for each $i = \{1, 2\}$ and $x_1 \to x_2 \geq x_2$. On the contrary, ${\wedge} \notin T^\infty_1$ since $0 \wedge x = x \wedge 0 = 0$ for any $x$. It is easy to see that $T^\infty_1 \sbs P_1$.

\begin{exc}
	Prove that the set $T^\infty_1$ is closed.
\end{exc}

\noindent We can use this closed set as an obstacle to refute expressibility: say, $\wedge \notin [{\vee}, {\to}]$.
\begin{exc}
	Nevertheless, $\vee \in [{\wedge}, {\to}]$ and, in fact, $\vee \in [{\to}]$.
\end{exc}
\begin{exc}
	Prove that $T^\infty_1 = [\to]$. (You might want to establish the identity $f(y, \vec x) = y \vee f(0, \vec x)$ for the case $f(y,\vec x) \geq y$ first and notice that $[{\neg}, {\to}] = \top$ then.)
\end{exc}

\section{Elements of Probability}

Mathematics is well-known to be useful for analyzing many aspects of reality. One typical situation here is suggested by mechanics: you have two real-world real-valued variables $y$ and $x$, which you may measure ``precisely enough'', and it appears that $y = f(x)$ (with this equality being exact for most practical purposes), where $f$ is a function from $\R$ to $\R$. For example, let $x$ be the height you drop a massive ball from and $y$ be the time it takes for the ball to reach the ground. It appears that $y$ is quite close to $\sqrt{2x/g}$, where the `constant' $g$ describes Earth's gravity at the spot. In fact, $y$ \emph{does not equal} $\sqrt{2x/g}$ due to air resistance, rotation of the Earth and, possibly, other factors. Nevertheless, the formula is `good enough' in many cases (not for a feather but likely for stones of various masses and forms). That is, knowing $x$ can spare you measuring $y$ and vice versa.

It is most striking here, however, that one can obtain the formula $y = \sqrt{2x/g}$ from a theoretical analysis of a simple model, where $x$ is the only factor. Moreover, this `good enough' formula can be made even better by introducing a few new factors (like air resistance) into the model.

But there is another kind of situation. Assume it is midsummer in Moscow, Russia. What may the air temperature you measure in your garden today be? It is notoriously hard to predict even after fixing many (apparently) contributing factors like the date, the time of the day, yesterday's temperature, whether you thermometer is shadowed or not, etc. The current technology employs measurements from many locations and time instances, sophisticated theoretical models (big brothers of the above ``function $f$'') as well as massive computing power---and it still allows for forecasts that are either short-ranged or inaccurate (or both).

On the other hand, it is extremely unlikely the temperature in question goes below $-5^\circ C$ or above $45^\circ C$. How can we know that? From the record, of course.\footnote{So, `extreme' temperature would be of no surprise during an Ice Age, yet might be absent from the record then!} We need no sophisticated model and but a modest amount of data (yearly records of monthly temperature extrema for a few decades) in order to say so, but this forecast is not very useful since it leaves too much uncertainty about the temperature. Nor is it absolutely accurate: we are not guaranteed from a new Ice Age (or a `Global Warming' if you like) coming without a warning.

Nevertheless, air temperature demonstrates noticeable persistence `on average' for time periods long enough, which makes it quite predictable---yet with a large error margin. For a short timeframe, its behavior is chaotic and still hard to predict precisely enough as it is governed by complicated dependencies and is influenced by a lot of factors.

The main idea of probabilistic methods (when applied to reality) is to take some data describing a process whose `governing law' (that is, its ``function $f$'') is not known and then extract from the data a `law' of another kind which describes the process `on average'. This `probabilistic law' should allow for predictions that come true `often enough', that is, this law should give us some measurable \emph{confidence}.

(It should be noticed that even quite regular phenomena, like the ball-dropping experiment described above, are not \emph{absolutely} regular\footnote{From the Experimenter's rather than the Philosopher's point of view.}: the measured difference between $y$ and $\sqrt{2x/g}$ (which is generally small) demonstrates a chaotic behavior (for the plethora of factors it is influenced by) but approaches a near-zero value `on average'. So, there is no clear borderline between the realms of `mechanics' and `probability'.)

For example, we may know from experiment that a `fair' coin gives `Heads' in about a half of all tosses---without any knowledge of the complicated equations that model coin tossing. We assume this statement as a probabilistic law: a fair coin gives `Heads' with \emph{probability} $1/2$---that is, we expect to get `Heads' in one out of two tosses \emph{on average}.\footnote{In fact, extracting such \emph{abstract} probability numbers from \emph{concrete} data is non-trivial and comprises a subject of \emph{mathematical statistics}.} It does not mean we expect one `Heads' in \emph{every} two tosses, but close to $n/2$ in $n$ tosses when $n$ is `large enough'.

So, our law predicts \emph{about} $500$ `Heads' if the coin is tossed $1000$ times. But how good is this prediction? We want to quantify our confidence: say, what is the chance that the coin gives `Heads' less than $450$ times? Again, this `chance' means we \emph{repeat} a series of $1000$ tosses $n$ times for a `large' $n$ and see how frequent the \emph{event} of getting less than $450$ `Heads' is.

Interestingly enough, such questions can be answered theoretically in a general mathematical \emph{probability theory} that does not depend on a particular experiment. The current theory requires advanced calculus to define and explore probability, but many key concepts can be learned from a simplified model, which we are going to see.

\paragraph{Finite models.} Let us create a formal model of some abstract experiment and define the probabilistic law it is governed by. We are not going to extract such models from the \emph{data} of real experiments at this stage, however. So, the probabilistic law will be a given for us.

Our main constraint here is that the experiment may have only \emph{finitely} many \emph{outcomes}: $\omega_1, \omega_2, \ldots,\\ \omega_n$, where $n > 0$. The set $\Omega$ of these outcomes is called a \emph{sample space}. Clearly, $\Omega$ is a finite non-empty set. Intuitively, we assume that the experiment can be repeated arbitrarily many times; each run of the experiment results in \emph{just one} outcome from $\Omega$. Which one? This is the only thing assumed `uncertain' or `random' in this model. We shall see soon how one can impose a law on this uncertainty.

In practice, we are interested in whether the outcome satisfies some property rather than what it exactly is (this is especially true when possible outcomes are quite numerous). As usual, `properties' can be replaced by subsets of $\Omega$. Any $A \sbs \Omega$ is called an \emph{event}. They say that an outcome $\omega$ is \emph{favorable} for the event $A$ iff $\omega \in A$, i.\,e., $\omega$ satisfies the property we are interested in.

\begin{rem}
	Introducing events allows us to overcome the following apparent drawback of our model: why cannot we have multiple outcomes at once? Say, assume that the experiment consists of taking a card from a standard $52$-card deck `at random'. We are interested in the rank (that is, Ace, $2$, $10$, Jack, Queen, King (the last three are known as \emph{face cards}), etc.) and the suit (hearts, diamonds, etc.) of the card taken. Why cannot we have \emph{three} simultaneous outcomes: ``it is a King'', ``it is a spades card'', and ``it is a face card''? The reason lies in unneeded complexity of such an approach as we would have non-trivial relations between outcomes then: if it is a King, it is necessarily a face card, etc. It is more convenient to identify the outcomes with cards themselves (that is, pairs of ranks and suits); this way, every card drawing results in just one outcome as each card is unique, while the properties we mentioned are the events $A_1 = \{K\clubsuit, K\spadesuit, K\diamondsuit, K\heartsuit\}$, $A_2 = \{2\spadesuit, 3\spadesuit, \ldots, 10\spadesuit, J\spadesuit, \ldots, K\spadesuit, A\spadesuit \}$, and $A_3 = \{J\clubsuit, Q\clubsuit, K\clubsuit, J\spadesuit, Q\spadesuit, K\spadesuit, J\diamondsuit, Q\diamondsuit, K\diamondsuit, J\heartsuit, Q\heartsuit, K\heartsuit\}$, respectively.
\end{rem}

By Corollary~\ref{L10:num_pow}, we have $2^n$ possible events for $n$ possible outcomes. For example, there are $2^{52}$ possible events in our $52$-card deck experiment. When tossing a coin, we naturally have two outcomes $\Omega = \{H, T\}$, where $H$ stands for `Heads' and $T$ stands for `Tails'. The \emph{event space} $\mF = \mP(\Omega) = \{\void, \{H\}, \{T\}, \{H, T\} \}$ contains $4$ elements now. The event $\void$ has no favorable outcome, so it `never happens' intuitively. We call $\void$ the \emph{impossible} event. On the contrary, the event $\Omega = \{H, T\}$ is favored by every outcome, it `always happens'. So, $\Omega$ is called the \emph{certain} event.

Now, we are ready to put probabilities in play. Assume that a sample space $\Omega$ (with the set of events $\mF = \mP(\Omega)$) is fixed. Let $p\colon \Omega \to \R$ be an arbitrary function such that $p(\omega) \geq 0$ for each $\omega \in \Omega$ and $\sum_{\omega \in \Omega} p(\omega) = 1$. We say that $p(\omega)$ is the \emph{weight} of an outcome $\omega \in \Omega$ and $$\P(A) = \sum_{\omega \in A} p(\omega)$$ is the \emph{probability} of an event $A \in \mF$. What does the symbol $\sum_{\omega \in A} p(\omega)$ stand for? As every $A$ is finite and summand order is irrelevant, we may put $\sum_{\omega \in A} p(\omega) = p(\omega'_1) + p(\omega'_2) + \ldots + p(\omega'_k)$ when $A = \{\omega'_1, \omega'_2, \ldots, \omega'_k\}$. In particular, $\sum_{\omega \in \void} p(\omega) = 0$ by definition. As a sum of finitely many reals, $\P(A)$ is a real number itself; thus, $\P$ may be treated as a function from $\mF$ to $\R$. The function $\P$ is called a \emph{probability distribution}. This is the `probabilistic law' we want to define. The function $\P$ effectively assigns a probability to every event. It determines the chances for \emph{every} property of possible outcomes.

It is easy to see that $\{ \omega \} \in \mF$ for each outcome $\omega \in \Omega$ and that $\P(\{ \omega \}) = p(\omega)$, so $p(\omega)$ is essentially the ``probability'' of the outcome $\omega$. We will however avoid calling it so because of technical considerations: in a more general model encompassing \emph{infinite} sample spaces, singletons $\{ \omega \}$ may be absent from $\mF$ and may have \emph{no} probability assigned.

The triplet $(\Omega, \mF, \P)$ is called a \emph{probability space} or a \emph{probabilistic model} (of an experiment). In our simplistic approach, we may take $(\Omega, \P)$ as a probability space since $\mF = \mP(\Omega)$.

We say that all the outcomes are \emph{equiprobable} iff $p(\omega) = p(\omega')$ for every $\omega, \omega' \in \Omega$. As $\sum_{\omega \in \Omega} p(\omega) = p(\omega_1) + p(\omega_2) + \ldots + p(\omega_n) = 1$, we have $p(\omega) = \frac{1}{n}= \frac{1}{|\Omega|}$ for each $\omega \in \Omega$ then. Moreover, for every event $A$, we get
$$\P(A) = \sum_{\omega \in A} p(\omega) = \sum_{\omega \in A} \frac{1}{|\Omega|} = \frac{1}{|\Omega|} \cdot \sum_{\omega \in A} 1 = \frac{|A|}{|\Omega|}.$$
when all the outcomes are equiprobable. We say that the probability space $(\Omega, \P)$ is \emph{classical} in this case. Thus, probability is just a ratio of two finite cardinalities (in particular, $\P(A) \in \Q$ for each event $A$) in a classical model.

\begin{rem}
	Classical models are restrictive but still important. In practice, they may well be our first guess if we know nothing about how frequent the experiment's outcomes are. Roughly speaking, you may presume every coin \emph{fair} (i.\,e., with $p(H) = p(T) = \frac{1}{2}$) until proven otherwise.
\end{rem}
\mcomm{Here goes that joke about equiprobability: \emph{What are the chances to come across a dinosaur on the street? One out of two for you either meet it or not!}}

\begin{exm}
	
\end{exm}


\begin{thm}\label{prob:prob_prop}
	Let $(\Omega, \mF, \P)$ be a probability space. Then for every $A, B \in \mF$, it holds that:
	\begin{enumerate}
		\item $0 \leq \P(A) \leq 1$;
		\item $\P(\void) = 0$, $\P(\Omega) = 1$;
		\item $\P(A \cup B) = \P(A) + \P(B)$ if $A \cap B = \void$;
		\item $\P(\bar A) = 1 - \P(A)$ (where $\bar A = \Omega \setminus A$);
		\item $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$.
	\end{enumerate}
\end{thm}
\begin{proof}\phantom{x}
	\begin{enumerate}
		\item We have $0 \leq \sum_{\omega \in A} p(\omega) \leq \sum_{\omega \in \Omega} p(\omega) = 1$ as $A \sbs \Omega$, each outcome weight $p(\omega)$ is non-negative, and the sum of those equals $1$ by definition.
		\item By definition, $\P(\void) = \sum_{\omega \in \void} p(\omega)$ and $\P(\Omega) = \sum_{\omega \in \Omega} p(\omega) = 1$.
		\item Since $A \cap B = \void$, one gets either $\omega \in A$ or $\omega \in B$ but not both for each $\omega \in A \cup B$. By grouping summands together, $\P(A \cup B) = \sum_{\omega \in A \cup B} p(\omega) = \sum_{\omega \in A} p(\omega) + \sum_{\omega \in B} p(\omega) = \P(A) + \P(B)$.
		\item As $A \cap \bar A = 1$, $A \cup \bar A = \Omega$, and $\P(\Omega) = 1$, obtain $1 = \P(\Omega) = \P(A) + \P(\bar A)$ from previous claims.
		\item Clearly, $A \cup B = (A \setminus B) \cup B$ with $(A \setminus B) \cap B = \void$, so $\P(A \cup B) = \P(A \setminus B) + P(B)$ by the previous claim. Furthermore, $A = (A \setminus B) \cup (A \cap B)$ with $(A \setminus B) \cap (A \cap B) = \void$, whence $P(A) = \P(A \setminus B) + \P(A \cap B)$ and $\P(A \setminus B) = \P(A) - \P(A \cap B)$.
	\end{enumerate}
\end{proof}
\begin{rem}
	The statements and arguments of Theorem~\ref{prob:prob_prop} are somewhat similar to that of Section~\ref{sect:comb1} (cf.~Corollary~\ref{L10:in-ex} in particular). Moreover, this theorem is a direct consequence of combinatorial statements in the case of a classical model: say, $\P(A \cup B) = \frac{|A \cup B|}{|\Omega|} = \frac{|A| + |B| - |A \cap B|}{|\Omega|} = \frac{|A|}{|\Omega|} + \frac{|B|}{|\Omega|} - \frac{|A \cap B|}{|\Omega|} = \P(A) + \P(B) + \P(A \cap B)$. Even for a general finite probability space, it is not hard to establish the following analogue of the Inclusion--Exclusion Principle (Theorem~\ref{comb2:ex-in-gen}), e.\,g., by applying the last claim of Theorem~\ref{prob:prob_prop} inductively.
\end{rem}
\begin{corr}[Poincar\'{e}'s Formula]\label{prob:poincare}
	Let $(\Omega, \mF, \P)$ be a probability space. Then for arbitrary events $A_1, A_2, \ldots, A_n \in \mF$, it holds that
	\begin{multline*}
		\P(A_1 \cup A_2 \cup \ldots \cup A_n) = \sum\limits_{k = 1}^n (-1)^{k - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_k \leqslant n} \P(A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) =\\
		\sum\limits_{1\leqslant i_1 \leqslant n} \P(A_{i_1}) - \sum\limits_{1\leqslant i_1 < i_2 \leqslant n} \P(A_{i_1} \cap A_{i_2}) + \sum\limits_{1\leqslant i_1 < i_2 < i_3 \leqslant n} \P(A_{i_1} \cap A_{i_2} \cap A_{i_3}) - \ldots\\
		+ (-1)^{n - 1} \sum\limits_{1\leqslant i_1 < i_2 < \ldots < i_n \leqslant n} \P(A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_n})= \\[3pt]
		(\P(A_1) + \P(A_2) + \ldots + \P(A_n)) - (\P(A_1 \cap A_2) + \P(A_1 \cap A_3) +  \ldots + \P(A_2 \cap A_3) + \ldots + \P(A_{n -1} \cap A_n)) +\\[3pt]
		+ (\P(A_1 \cap A_2 \cap A_3) + \P(A_1 \cap A_2 \cap A_4) +  \ldots + \P(A_2 \cap A_3 \cap A_4) + \ldots + \P(A_{n-2} \cap A_{n-1} \cap A_n)) - \ldots\\[3pt]
		+ (-1)^{n - 1} \P(A_1 \cap A_2 \cap \ldots \cap A_n).\\
	\end{multline*}
\end{corr}




%
%Evgeny Dashkov, [12.06.20 17:32]
%(1) Frequentist interpretation of probabilities: the main idea. Assume there is an experiment (say, rolling a die) whose outcome in uncertain, but surely belongs to some known set (1,2,3,4,5, or 6 point on the die). An event is just a subset of outcomes (which are then called favorable for that event). Say, X = {1} is the event 'the die shows 1' and  Y = {1,3,5} is the event 'the die shows an odd number'. So, just one outcome takes place each try of the experiment and the event happens on this trial if the outcome belongs to it.
%
%For a series of trial, one can compute the frequency of the event A—that is, the share of all trials when A happens. Say, for the series of 3 dice rollings resulting in (1,6,5) the frequency of Y is 2 out of 3, that is 2/3.
%
%Presumably, in a series long enough the frequency of every event is close to some `limit' which is called the probability of that event.
%
%Evgeny Dashkov, [12.06.20 17:47]
%(2) Classical probability model. We shall study the situation when some probabilities ARE KNOW (whatever they are—either 'frequencies' or 'confidence degrees' or anything else;  IT DOES NOT MATTER HERE), and we are to compute some other probabilities.
%
%The classical model is just a FINITE non-empty set \Omega = {\omega_1, ..., \omega_n} of some abstract 'outcomes'. Events are just all possible subsets of \Omega. WE PRESUME HERE THAT ALL OUTCOMES ARE EQUIPROBABLE (equally likely to occur). (In practice, such model may be used when we know nothing about the frequencies of the outcomes.) Under this assumption, the probability of an event A is naturally DEFINED to be just P(A) = |A| / |\Omega| (the number of favorable outcomes to the total number of outcomes.)
%
%In the classical model one has the following easy theorem implied by the properties of finite cardinalities.
%
%THEOREM 1. For every events A and B, it holds that
%0. 0 <= P(A) <= 1;
%1. P(\void) = 0; P(\Omega) = 1;
%2. P(\bar A) = 1 - P(A) (where \bar A is the complement of A to \Omega);
%3. P(A U B) = P(A) + P(B) if A and B are disjoint;
%4. P(A U B) = P(A) + P(B) - P(A ∩ B) in general.
%
%Notice that the probability (measure) P is a function from EVENTS to the segment [0; 1] of reals.
%
%Evgeny Dashkov, [12.06.20 17:59]
%(3) An easy generalization of the classical model is possible. Let's call it a FINITE model (or, finite probability space). So, here we still have just finitely many outcomes: \Omega = {\omega_1, ..., \omega_n} but these are NOT NECESSARILY EQUIPROBABLE, that is the outcomes have some specific 'probabilities'. As probability is formally a function of EVENT but NOT OUTCOME, we shall use another word. Namely, to define a finite probability space, we must specify a WEIGHT function p: \Omega -> [0, 1] (from outcomes to numbers) SUCH THAT \sum_{\omega \in \Omega} p(\omega) = p(\omega_1) + ... + p(\omega_n) = 1.
%
%Given such a function p, we DEFINE the probability of an event A as follows: P(A) = \sum_{\omega \in A} p(\omega), so we add the weights of all A-favorable outcomes together.
%
%It is easy to check that our THEOREM 1 still holds for this setting.
%
%Clearly, the classical model is a kind of a finite model where all the weights are equal:  p(\omega) = 1 / |\Omega|.
%
%Evgeny Dashkov, [12.06.20 18:24]
%(4) Conditional probability. Sometimes, we're interested in a relative frequency of one event over another one. Say, what is the frequency of the die showing a prime number AMONG the outcomes when it shows an odd number? In the limit, this leads to the notion of conditional probability. This notion is best understood in terms of another probability interpretation: when P(A) means the 'confidence degree' that A will happen.
%
%But let us put philosophy aside (despite some risks of paradoxes and misconceptions). Formally (for WHATEVER probability space) the probability of A given B (or 'under the condition B') is DEFINED for every two events A and B as P (A | B) = P (A ∩ B) / P (B) if P(B) > 0 (otherwise this number is undefined).
%
%Please notice that in probability theory, they usually write XY instead of X ∩  Y.  We shall adopt this convention.
%
%There is an easily provable analogue (and a corollary) of THEOREM 1 for conditional probabilities.
%THEOREM 2. For every events A, C and B, where P(B) > 0, it holds that
%0. 0 <= P(A | B) <= 1;
%1. P(\void | B) = 0; P(B | B) = 1;
%2. P(\bar A | B) = 1 - P(A | B) (where \bar A is the complement of A to \Omega);
%3. P(A U C | B) = P(A | B) + P(C | B) if A and C are disjoint;
%4. P(A U C | B) = P(A | B) + P(C | B) - P(A ∩ C | B) in general.
%This shows that conditional probability is a probability indeed but relative to the lesser space B \subset \Omega.
%
%Evgeny Dashkov, [12.06.20 19:12]
%(5) One may interpret P (A | B) as the confidence degree that  B implies A; as a 'power' of the implication. Remarkably, it is possible to interchange the 'reason' B and the 'consequence' A:
%THEOREM 3 (Bayes' Theorem)
%If P(A) > 0 and P(B) > 0, then P(B | A) = P(A | B) * P(B) / P(A).
%
%This is immediate from the definitions and the fact that AB = BA.
%
%Another important aspect of conditionals is INDEPENDENCE. We say that two events A and B are independent (from each other) if P(AB) = P(A) * P(B). Clearly, when both P(A) and P(B) > 0, then A and B are independent iff P(A | B) = P(A) iff P(B | A) = P(B). That is, if A does not depend on B, the 'conditional' probability P(A | B) of A given B is the same as the 'apriori' probability P(A). That is, whether B happens or not does not change the chances for A.
%
%We say that a set of events {B_1, ...., B_m} is a partition of the space \Omega if (1) B_1 U ... U B_m = \Omega and (2) B_i ∩ B_j = \void. Remarkably, we do not require any B_i to be non-empty. For example, {B, \bar B} is always a partition.
%
%Clearly, A = AB_1 U AB_2 U ... U AB_m for every event A and any partition {B_1, ...., B_m}. By THEOREM 1, this results in P(A) = P(AB_1) + P(AB_2) + ... + P(AB_m). This fact is known as THE LAW OF TOTAL PROBABILITY.
%
%Combining this law with Bayes' Theorem, we obtain the following useful formula (given all the necessary conditionals are defined):
%P(B_i | A) = P(A | B_i) * P(B) / P(A) = P(A | B_i) * P(B) / (P(AB_1) + P(AB_2) + ... + P(AB_m)) =
%= P(A | B_i) * P(B) / (P(A | B_1) * P(B_1) + P(A | B_2) * P(B_2) + ...+ P(A | B_m) * P(B_m) ).
%This formula allows us to compute the chances for one reason B_i (out many mutually exclusive ones) given the 'consequence' A happens, when we know all the conditionals P(A | B_j) and the apriori chances for the 'reasons' P(B_j).
%
%Yet another useful formula is the following 'probability multiplication law'. Givеn all the conditionals exist, one have
%P(A_1 A_2 ... A_k) = P(A_k | A_{k-1} ... A_2 A_1) * P(A_{k-1} | A_{k-2} ... A_2 A_1) * ... * P(A_2 | A_1) * P(A_1).
%
%Evgeny Dashkov, [12.06.20 19:34]
%(6) Random variables. Sometimes we want to translate the outcomes into numbers, or to get some numeric charecterization thereof. Say, you are playing a game when the die showing 1, 2, or 3 results in the loss of 2 dollars. while if the die shows k >= 4, you win k dollars. Your gain might be represented by a function that maps 1, 2, and 3 to -2 bit leaves all the other numbers unchanged. Clearly, the amount you win is uncertain, that is, RANDOM.
%
%Formally, in a FINITE model, a random variable \xi is just an arbitrary function \Omega -> \R, from outcomes to reals. In our finite world, each random variable \xi takes just finitely many values {x_1, ..., x_k} (we ASSUME these are pairwise distinct). The most interesting property of every random value \xi is its DISTRIBUTION, that is the information about the chances for \xi to take a particular value.
%
%In our finite case, this is very simple. Consider the event A_i = {\omega \in \Omega | \xi(\omega) = x_i }. Clearly, it suffices to indicate the chances for \xi to take each value x_i, that is, to specify P(A_1), ..., P(A_k). Notice that {A_1, ..., A_k} is a partition.
%
%In more complex settings, it is MUCH more convenient to consider a special numeric function F_\xi : \R -> [0,1], which is know as the CUMULATIVE DISTRIBUTION FUNCTION (CDF) of \xi. The definition is quite clear: F_\xi ( x) = P('\xi <= x') = P( {\omega \in \Omega | \xi(\omega) <= x } ). It is easy to see that x <= y implies F_\xi (x) <= F_\xi (y).
%
%Also, notice that F_\xi = F_\eta does not imply \xi = \eta in general. So that, equally distributed random values may take the same value with the same probability but at the distinct points of \Omega. Such a distinction is usually immaterial in the probabilistic context.
%
%
%Evgeny Dashkov, [12.06.20 20:38]
%7) Often, we are interested in 'average' or 'general' charateristics of a random variable \xi. For example, what is our AVERAGE (in a long series) of EXPECTED gain in some game of chance? So, the most important such charectristic is the EXPECTED VALUE E \xi of a random variable \xi.  It is defined as follows: E \xi = x_1 * P('\xi = x_1') + ... + x_k * P('\xi = x_k') = x_1 * P(A_1) + ... + x_k * P(A_k).
%
%So, E \xi is just a sum of all possible values, each weighted by the probability it is taken. Let us list the most important properties of E.
%
%First, we define two random variables \xi (with values x_1,..., x_k) and \eta (with values y_1, ..., y_l) to be INDEPENDENT if P('\xi = x_i' and '\eta = y_j') = P('\xi = x_i) * P('\eta = y_j') for every i and j . That is, the events A^{\xi}_i and A^{\eta}_j are independent.
%
%THEOREM. For every two random variable \xi and \eta on some probability space, the following hold:
%(1) E 1 = 1  (1 means the constant random variable with the only value 1);
%(2) if \xi >= 0 (everywhere), then E \xi >= 0;
%(3) for every function g: \R \to \R, E g(\xi) = \sum_{i = 1}^k  g(x_i) * P('\xi = x_i');
%(4) for every numbers a and b, E (a * \xi + b * \eta) = a * E \xi + b * E \eta;
%(5) if \xi >= \eta (everywhere), then E \xi >= E \eta;
%(6) if \xi and \eta are independent, then E (\xi * \eta) = E \xi * E \eta.

\section{Random Variables}

\end{document}
